{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:12:59 - INFO - Logging initialized. Log file: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/logs/asv_selection_20250424_101259.log\n",
      "2025-04-24 10:12:59 - INFO - Starting ASV Selection Analysis...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# ASV Selection Analysis Pipeline"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:12:59 - INFO - Copying file to temporary location: /var/folders/5b/1yjp54491txfqjty7rqyyyww0000gn/T/tmpcfz3fgec/input_data.csv\n",
      "2025-04-24 10:12:59 - INFO - Reading data from temporary file\n",
      "2025-04-24 10:12:59 - INFO - Successfully loaded 63451 records\n",
      "2025-04-24 10:12:59 - INFO - Starting data preprocessing...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Data Preprocessing Results\n",
       "- Total records: 63,451\n",
       "- Number of features: 12\n",
       "\n",
       "### Feature List:\n",
       "reads, total_asv_reads, asv_count, percentage_reads, read_proportion, log_reads, read_density, is_single_asv, is_dominant_asv, reads_rank, relative_abundance, is_match\n",
       "\n",
       "### Data Summary:\n",
       "```\n",
       "              reads  total_asv_reads     asv_count  percentage_reads  read_proportion     log_reads  read_density  is_single_asv  is_dominant_asv    reads_rank  relative_abundance      is_match\n",
       "count  63451.000000     63451.000000  63451.000000      63451.000000     63451.000000  63451.000000  63451.000000   63451.000000     63451.000000  63451.000000        63451.000000  63451.000000\n",
       "mean     117.661046      1856.864636     17.663851         37.223370         0.372234      2.068871     67.391087       0.284298         0.285244     17.562954            0.163827      0.389340\n",
       "std      548.753746      9759.898990     39.965420         43.069924         0.430699      1.813360    336.504131       0.451083         0.451534     34.407694            0.329146      0.487604\n",
       "min        1.000000         4.000000      1.000000          0.000000         0.000005      0.693147      0.003333       0.000000         0.000000      1.000000            0.000028      0.000000\n",
       "25%        1.000000        11.000000      1.000000          0.530000         0.005319      0.693147      0.133333       0.000000         0.000000      2.000000            0.002326      0.000000\n",
       "50%        3.000000        79.000000      4.000000         10.920000         0.109244      1.386294      0.666667       0.000000         0.000000      6.000000            0.009009      0.000000\n",
       "75%       11.000000       911.000000     13.000000         99.860000         0.998568      2.484907      7.000000       1.000000         1.000000     17.000000            0.060241      1.000000\n",
       "max    21111.000000    207505.000000    300.000000        100.000000         1.000000      9.957597  15535.000000       1.000000         1.000000    359.500000            1.000000      1.000000\n",
       "```\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:13:00 - INFO - Training Random Forest model...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(29336) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:13:35 - INFO - \n",
      "Model Training Results:\n",
      "2025-04-24 10:13:35 - INFO - Best parameters: {'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "2025-04-24 10:13:35 - INFO - Cross-validation score: 0.9674\n",
      "2025-04-24 10:13:35 - INFO - Test accuracy: 0.9980\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Model Training Results\n",
       "### Model Parameters:\n",
       "```\n",
       "bootstrap                       True\n",
       "ccp_alpha                        0.0\n",
       "class_weight                balanced\n",
       "criterion                       gini\n",
       "max_depth                       None\n",
       "max_features                    sqrt\n",
       "max_leaf_nodes                  None\n",
       "max_samples                     None\n",
       "min_impurity_decrease            0.0\n",
       "min_samples_leaf                   1\n",
       "min_samples_split                  2\n",
       "min_weight_fraction_leaf         0.0\n",
       "monotonic_cst                   None\n",
       "n_estimators                     300\n",
       "n_jobs                            -1\n",
       "oob_score                      False\n",
       "random_state                      42\n",
       "verbose                            0\n",
       "warm_start                     False\n",
       "```\n",
       "\n",
       "### Feature Importance:\n",
       "```\n",
       "           feature  importance\n",
       "        reads_rank    0.253677\n",
       "relative_abundance    0.251830\n",
       "             reads    0.132977\n",
       "         log_reads    0.125857\n",
       "          is_match    0.102416\n",
       "      read_density    0.088970\n",
       "   read_proportion    0.019090\n",
       "  percentage_reads    0.010759\n",
       "   total_asv_reads    0.007403\n",
       "   is_dominant_asv    0.003809\n",
       "         asv_count    0.002796\n",
       "     is_single_asv    0.000416\n",
       "```\n",
       "\n",
       "### Model Performance:\n",
       "- Accuracy: 0.9980\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:13:35 - INFO - \n",
      "Finding optimal threshold...\n",
      "2025-04-24 10:13:36 - INFO - Performing coarse threshold search...\n",
      "2025-04-24 10:13:36 - INFO - Performing medium-grain threshold search...\n",
      "2025-04-24 10:13:37 - INFO - Performing fine-grain threshold search...\n",
      "2025-04-24 10:13:38 - INFO - Optimal threshold found: 0.3300\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Threshold Optimization Results\n",
       "### Optimal Threshold: 0.3300\n",
       "\n",
       "### Performance at Optimal Threshold:\n",
       "- F1 Score: 0.9926\n",
       "- Precision: 0.9854\n",
       "- Recall: 1.0000\n",
       "- True Positives: 8,559\n",
       "- False Positives: 127\n",
       "- False Negatives: 0\n",
       "- True Negatives: 54,765\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:13:38 - INFO - \n",
      "Saving model components...\n",
      "2025-04-24 10:13:38 - INFO - Saved model to: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/trained_model.joblib\n",
      "2025-04-24 10:13:38 - INFO - Saved scaler to: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/scaler.joblib\n",
      "2025-04-24 10:13:38 - INFO - Saved configuration to: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/model_config.json\n",
      "2025-04-24 10:13:38 - INFO - \n",
      "Applying model predictions with threshold 0.3300\n",
      "2025-04-24 10:14:02 - INFO - \n",
      "Prediction Statistics:\n",
      "2025-04-24 10:14:02 - INFO - Total samples processed: 10395\n",
      "2025-04-24 10:14:02 - INFO - Total ASVs selected: 8559\n",
      "2025-04-24 10:14:02 - INFO - Agreement rate: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## ASV Selection Summary\n",
       "\n",
       "### Overview Statistics\n",
       "- Total ASVs analyzed: 63,451\n",
       "- ASVs selected: 8,559\n",
       "- Selection rate: 13.49%\n",
       "\n",
       "### Comparison of All vs Selected ASVs\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "('All ASVs', 'reads')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('All ASVs', 'total_asv_reads')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('All ASVs', 'asv_count')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('All ASVs', 'percentage_reads')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('All ASVs', 'read_proportion')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('All ASVs', 'log_reads')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('All ASVs', 'read_density')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('Selected ASVs', 'reads')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('Selected ASVs', 'total_asv_reads')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('Selected ASVs', 'asv_count')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('Selected ASVs', 'percentage_reads')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('Selected ASVs', 'read_proportion')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('Selected ASVs', 'log_reads')",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "('Selected ASVs', 'read_density')",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e698f698-998d-4b0a-b6e2-665c09f73f4a",
       "rows": [
        [
         "count",
         "63451.0",
         "63451.0",
         "63451.0",
         "63451.0",
         "63451.0",
         "63451.0",
         "63451.0",
         "8559.0",
         "8559.0",
         "8559.0",
         "8559.0",
         "8559.0",
         "8559.0",
         "8559.0"
        ],
        [
         "mean",
         "117.6610455311973",
         "1856.864635703141",
         "17.66385084553435",
         "37.22336968684496",
         "0.372233542709833",
         "2.0688710914498616",
         "67.39108722405722",
         "728.8969505783386",
         "1352.7974062390467",
         "3.2438368968337423",
         "81.94297698329245",
         "0.8194297284496717",
         "5.48717616525602",
         "425.48237133310187"
        ],
        [
         "std",
         "548.7537457663611",
         "9759.89899002294",
         "39.96541950364336",
         "43.06992367911303",
         "0.43069945102083335",
         "1.8133595099074424",
         "336.5041307541781",
         "1201.6567072791272",
         "4479.431041005324",
         "9.201263620178944",
         "31.492992393068715",
         "0.31493019148202805",
         "1.6849481768615198",
         "773.7409519780949"
        ],
        [
         "min",
         "1.0",
         "4.0",
         "1.0",
         "0.0",
         "4.819160984072673e-06",
         "0.6931471805599453",
         "0.0033333333333333335",
         "4.0",
         "4.0",
         "1.0",
         "0.01",
         "8.192573672923544e-05",
         "1.6094379124341003",
         "0.08759124087591241"
        ],
        [
         "25%",
         "1.0",
         "11.0",
         "1.0",
         "0.53",
         "0.005319148936170213",
         "0.6931471805599453",
         "0.13333333333333333",
         "71.0",
         "105.0",
         "1.0",
         "79.04",
         "0.7903731394137632",
         "4.276666119016055",
         "40.0"
        ],
        [
         "50%",
         "3.0",
         "79.0",
         "4.0",
         "10.92",
         "0.1092436974789916",
         "1.3862943611198906",
         "0.6666666666666666",
         "290.0",
         "448.0",
         "1.0",
         "99.94",
         "0.9993642720915448",
         "5.673323267171493",
         "156.0"
        ],
        [
         "75%",
         "11.0",
         "911.0",
         "13.0",
         "99.86",
         "0.9985683600395161",
         "2.4849066497880004",
         "7.0",
         "892.0",
         "1346.5",
         "3.0",
         "100.0",
         "1.0",
         "6.794585953876762",
         "490.2"
        ],
        [
         "max",
         "21111.0",
         "207505.0",
         "300.0",
         "100.0",
         "1.0",
         "9.95759687818316",
         "15535.0",
         "18049.0",
         "207505.0",
         "300.0",
         "100.0",
         "1.0",
         "9.800900963761027",
         "15535.0"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"7\" halign=\"left\">All ASVs</th>\n",
       "      <th colspan=\"7\" halign=\"left\">Selected ASVs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>reads</th>\n",
       "      <th>total_asv_reads</th>\n",
       "      <th>asv_count</th>\n",
       "      <th>percentage_reads</th>\n",
       "      <th>read_proportion</th>\n",
       "      <th>log_reads</th>\n",
       "      <th>read_density</th>\n",
       "      <th>reads</th>\n",
       "      <th>total_asv_reads</th>\n",
       "      <th>asv_count</th>\n",
       "      <th>percentage_reads</th>\n",
       "      <th>read_proportion</th>\n",
       "      <th>log_reads</th>\n",
       "      <th>read_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>63451.000000</td>\n",
       "      <td>63451.000000</td>\n",
       "      <td>63451.000000</td>\n",
       "      <td>63451.000000</td>\n",
       "      <td>63451.000000</td>\n",
       "      <td>63451.000000</td>\n",
       "      <td>63451.000000</td>\n",
       "      <td>8559.000000</td>\n",
       "      <td>8559.000000</td>\n",
       "      <td>8559.000000</td>\n",
       "      <td>8559.000000</td>\n",
       "      <td>8559.000000</td>\n",
       "      <td>8559.000000</td>\n",
       "      <td>8559.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>117.661046</td>\n",
       "      <td>1856.864636</td>\n",
       "      <td>17.663851</td>\n",
       "      <td>37.223370</td>\n",
       "      <td>0.372234</td>\n",
       "      <td>2.068871</td>\n",
       "      <td>67.391087</td>\n",
       "      <td>728.896951</td>\n",
       "      <td>1352.797406</td>\n",
       "      <td>3.243837</td>\n",
       "      <td>81.942977</td>\n",
       "      <td>0.819430</td>\n",
       "      <td>5.487176</td>\n",
       "      <td>425.482371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>548.753746</td>\n",
       "      <td>9759.898990</td>\n",
       "      <td>39.965420</td>\n",
       "      <td>43.069924</td>\n",
       "      <td>0.430699</td>\n",
       "      <td>1.813360</td>\n",
       "      <td>336.504131</td>\n",
       "      <td>1201.656707</td>\n",
       "      <td>4479.431041</td>\n",
       "      <td>9.201264</td>\n",
       "      <td>31.492992</td>\n",
       "      <td>0.314930</td>\n",
       "      <td>1.684948</td>\n",
       "      <td>773.740952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.087591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.040000</td>\n",
       "      <td>0.790373</td>\n",
       "      <td>4.276666</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.920000</td>\n",
       "      <td>0.109244</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.940000</td>\n",
       "      <td>0.999364</td>\n",
       "      <td>5.673323</td>\n",
       "      <td>156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>911.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.860000</td>\n",
       "      <td>0.998568</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>892.000000</td>\n",
       "      <td>1346.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.794586</td>\n",
       "      <td>490.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21111.000000</td>\n",
       "      <td>207505.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.957597</td>\n",
       "      <td>15535.000000</td>\n",
       "      <td>18049.000000</td>\n",
       "      <td>207505.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.800901</td>\n",
       "      <td>15535.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           All ASVs                                                  \n",
       "              reads total_asv_reads     asv_count percentage_reads   \n",
       "count  63451.000000    63451.000000  63451.000000     63451.000000  \\\n",
       "mean     117.661046     1856.864636     17.663851        37.223370   \n",
       "std      548.753746     9759.898990     39.965420        43.069924   \n",
       "min        1.000000        4.000000      1.000000         0.000000   \n",
       "25%        1.000000       11.000000      1.000000         0.530000   \n",
       "50%        3.000000       79.000000      4.000000        10.920000   \n",
       "75%       11.000000      911.000000     13.000000        99.860000   \n",
       "max    21111.000000   207505.000000    300.000000       100.000000   \n",
       "\n",
       "                                                  Selected ASVs   \n",
       "      read_proportion     log_reads  read_density         reads   \n",
       "count    63451.000000  63451.000000  63451.000000   8559.000000  \\\n",
       "mean         0.372234      2.068871     67.391087    728.896951   \n",
       "std          0.430699      1.813360    336.504131   1201.656707   \n",
       "min          0.000005      0.693147      0.003333      4.000000   \n",
       "25%          0.005319      0.693147      0.133333     71.000000   \n",
       "50%          0.109244      1.386294      0.666667    290.000000   \n",
       "75%          0.998568      2.484907      7.000000    892.000000   \n",
       "max          1.000000      9.957597  15535.000000  18049.000000   \n",
       "\n",
       "                                                                      \n",
       "      total_asv_reads    asv_count percentage_reads read_proportion   \n",
       "count     8559.000000  8559.000000      8559.000000     8559.000000  \\\n",
       "mean      1352.797406     3.243837        81.942977        0.819430   \n",
       "std       4479.431041     9.201264        31.492992        0.314930   \n",
       "min          4.000000     1.000000         0.010000        0.000082   \n",
       "25%        105.000000     1.000000        79.040000        0.790373   \n",
       "50%        448.000000     1.000000        99.940000        0.999364   \n",
       "75%       1346.500000     3.000000       100.000000        1.000000   \n",
       "max     207505.000000   300.000000       100.000000        1.000000   \n",
       "\n",
       "                                  \n",
       "         log_reads  read_density  \n",
       "count  8559.000000   8559.000000  \n",
       "mean      5.487176    425.482371  \n",
       "std       1.684948    773.740952  \n",
       "min       1.609438      0.087591  \n",
       "25%       4.276666     40.000000  \n",
       "50%       5.673323    156.000000  \n",
       "75%       6.794586    490.200000  \n",
       "max       9.800901  15535.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Match Statistics for Selected ASVs"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "match",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "select",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "663b11e5-16e2-4e02-8e97-be2a49602c63",
       "rows": [
        [
         "match",
         "8559"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>model_decision</th>\n",
       "      <th>select</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>match</th>\n",
       "      <td>8559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model_decision  select\n",
       "match                 \n",
       "match             8559"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:14:03 - INFO - ASV Selection Summary - Total: 63451, Selected: 8559, Rate: 13.49%\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Model Prediction Results\n",
       "### Summary Statistics:\n",
       "- Total samples processed: 10,395\n",
       "- Total ASVs analyzed: 63,451\n",
       "- ASVs selected: 8,559\n",
       "- ASVs not selected: 54,892\n",
       "\n",
       "### Agreement Analysis:\n",
       "- Total agreements: 63,451\n",
       "- Total disagreements: 0\n",
       "- Agreement rate: 1.0000\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Generating Plots and Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:14:03 - INFO - \n",
      "Analyzing features...\n",
      "2025-04-24 10:14:03 - INFO - Using threshold 0.33 for feature analysis\n",
      "2025-04-24 10:14:03 - INFO - Creating feature plots with threshold: 0.33\n",
      "2025-04-24 10:14:03 - INFO - Using threshold 0.33 in feature plots\n",
      "2025-04-24 10:14:04 - INFO - Feature analysis complete!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✓ Feature analysis completed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Model performance plots generated"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Correlation analysis completed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Threshold analysis plots generated"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Agreement analysis plots generated"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ ASV-MMG analysis completed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Saving Results and Generating Report"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:14:19 - INFO - Saved results to CSV: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/analysis_results.csv\n",
      "2025-04-24 10:14:23 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/pca_analysis/pca_analysis.html\n",
      "2025-04-24 10:14:23 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/pca_analysis/pca_analysis.json\n",
      "2025-04-24 10:14:23 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_reads.html\n",
      "2025-04-24 10:14:24 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_reads.json\n",
      "2025-04-24 10:14:24 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_total_asv_reads.html\n",
      "2025-04-24 10:14:25 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_total_asv_reads.json\n",
      "2025-04-24 10:14:25 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_asv_count.html\n",
      "2025-04-24 10:14:25 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_asv_count.json\n",
      "2025-04-24 10:14:25 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_percentage_reads.html\n",
      "2025-04-24 10:14:26 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_percentage_reads.json\n",
      "2025-04-24 10:14:26 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_read_proportion.html\n",
      "2025-04-24 10:14:26 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_read_proportion.json\n",
      "2025-04-24 10:14:26 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_log_reads.html\n",
      "2025-04-24 10:14:26 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_log_reads.json\n",
      "2025-04-24 10:14:26 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_read_density.html\n",
      "2025-04-24 10:14:26 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_read_density.json\n",
      "2025-04-24 10:14:26 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_is_single_asv.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_is_single_asv.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_is_dominant_asv.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_is_dominant_asv.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_reads_rank.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_reads_rank.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_relative_abundance.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_relative_abundance.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_is_match.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/feature_analysis/feature_distribution_is_match.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/model_performance/feature_importance.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/model_performance/feature_importance.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/model_performance/roc_curve.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/model_performance/roc_curve.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/model_performance/confusion_matrix.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/model_performance/confusion_matrix.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/correlation_analysis/correlation_matrix.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/correlation_analysis/correlation_matrix.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/correlation_analysis/feature_dendrogram.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/correlation_analysis/feature_dendrogram.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/threshold_analysis/threshold_optimization.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/threshold_analysis/threshold_optimization.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/agreement_analysis/agreement_distribution.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/agreement_analysis/agreement_distribution.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/agreement_analysis/decision_matrix.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/agreement_analysis/decision_matrix.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/asv_mmg_analysis/asv_mmg_distribution.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/asv_mmg_analysis/asv_mmg_distribution.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/asv_mmg_analysis/asv_mmg_bubble.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/asv_mmg_analysis/asv_mmg_bubble.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/asv_mmg_analysis/mmg_read_distribution.html\n",
      "2025-04-24 10:14:27 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/asv_mmg_analysis/mmg_read_distribution.json\n",
      "2025-04-24 10:14:27 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/asv_mmg_analysis/mmg_pattern_matrix.html\n",
      "2025-04-24 10:14:28 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/asv_mmg_analysis/mmg_pattern_matrix.json\n",
      "2025-04-24 10:14:28 - INFO - Saved HTML plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/asv_mmg_analysis/mmg_sankey_flow.html\n",
      "2025-04-24 10:14:28 - INFO - Saved JSON plot: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/visualizations/asv_mmg_analysis/mmg_sankey_flow.json\n",
      "2025-04-24 10:14:28 - INFO - Results saved to: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ Results saved to: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:14:28 - INFO - Generated enhanced HTML report: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/reports/analysis_report.html\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ Report generated: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/reports/analysis_report.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Final Analysis Summary\n",
       "### Model Performance:\n",
       "- Final accuracy: 0.9980\n",
       "- Optimal threshold: 0.3300\n",
       "- Overall agreement rate: 1.0000\n",
       "\n",
       "### Dataset Statistics:\n",
       "- Total ASVs processed: 63,451\n",
       "- ASVs selected: 8,559\n",
       "- Samples analyzed: 10,395\n",
       "\n",
       "### File Locations:\n",
       "- Results saved to: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425\n",
       "- Model saved to: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425\n",
       "- Detailed report at: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425/reports/analysis_report.html\n",
       "\n",
       "### Analysis Components Completed:\n",
       "- ✓ Data preprocessing\n",
       "- ✓ Model training\n",
       "- ✓ Threshold optimization\n",
       "- ✓ Model saving\n",
       "- ✓ Prediction application\n",
       "- ✓ Feature analysis\n",
       "- ✓ Performance analysis\n",
       "- ✓ Visualization generation\n",
       "- ✓ Report generation\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Configure Environment\n",
    "\"\"\"\n",
    "ASV Selection Machine Learning Pipeline\n",
    "Author: Sarawut Ounjai\n",
    "Last Updated: 2024\n",
    "\"\"\"\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import traceback\n",
    "import shutil\n",
    "import tempfile\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "from joblib import dump, load, parallel_backend\n",
    "\n",
    "# Data processing and ML libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, precision_score,\n",
    "    recall_score, f1_score, roc_curve, auc, confusion_matrix,\n",
    "    balanced_accuracy_score\n",
    ")\n",
    "from joblib import parallel_backend\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import networkx as nx\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Cell 2: Configuration and Setup\n",
    "# File paths\n",
    "BASE_DIR = Path(\"/Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis\")\n",
    "CHAPTER_DIR = BASE_DIR / \"Chapter2_Data_generation/Barcoding_Machine_Learning\"\n",
    "INPUT_FILE = CHAPTER_DIR / \"Barcoding_Machine_Learning_OR_020425.csv\"\n",
    "OUTPUT_DIR = CHAPTER_DIR / \"Barcoding_Machine_Learning_OR_200425\"\n",
    "MODEL_SAVE_DIR = CHAPTER_DIR / \"Barcoding_Machine_Learning_OR_200425\"\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "def setup_logging(output_dir: Path) -> None:\n",
    "    \"\"\"Configure logging with both file and console handlers.\"\"\"\n",
    "    log_dir = output_dir / 'logs'\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = log_dir / f'asv_selection_{timestamp}.log'\n",
    "    \n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    # File handler\n",
    "    file_handler = RotatingFileHandler(\n",
    "        str(log_file),\n",
    "        maxBytes=10*1024*1024,\n",
    "        backupCount=5,\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Console handler (using StreamHandler for Jupyter compatibility)\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Configure root logger\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Remove existing handlers\n",
    "    for handler in root_logger.handlers[:]:\n",
    "        root_logger.removeHandler(handler)\n",
    "    \n",
    "    # Add new handlers    \n",
    "    root_logger.addHandler(file_handler)\n",
    "    root_logger.addHandler(console_handler)\n",
    "    \n",
    "    logging.info(f\"Logging initialized. Log file: {log_file}\")\n",
    "\n",
    "# Initialize logging\n",
    "setup_logging(OUTPUT_DIR)\n",
    "\n",
    "# Cell 3: Data Loading Functions\n",
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"Load and validate input data.\"\"\"\n",
    "    try:\n",
    "        # Copy file to temp location\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        temp_file = Path(temp_dir) / 'input_data.csv'\n",
    "        \n",
    "        logging.info(f\"Copying file to temporary location: {temp_file}\")\n",
    "        shutil.copy2(INPUT_FILE, temp_file)\n",
    "        \n",
    "        # Read data\n",
    "        logging.info(\"Reading data from temporary file\")\n",
    "        df = pd.read_csv(temp_file)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"Empty dataframe loaded\")\n",
    "        \n",
    "        logging.info(f\"Successfully loaded {len(df)} records\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data loading: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        if temp_dir and Path(temp_dir).exists():\n",
    "            shutil.rmtree(temp_dir)\n",
    "\n",
    "# Cell 4: Data Preprocessing Functions\n",
    "def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Preprocess data without bias, focusing on quality feature engineering.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting data preprocessing...\")\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Standard preprocessing for categorical variables\n",
    "        df['match'] = df['match'].fillna('no_match')\n",
    "        match_mapping = {\n",
    "            'match': 'match', 'Match': 'match', 'TRUE': 'match', True: 'match',\n",
    "            'no_match': 'no_match', 'No_Match': 'no_match', 'nomatch': 'no_match',\n",
    "            'NoMatch': 'no_match', 'FALSE': 'no_match', False: 'no_match', 'no': 'no_match'\n",
    "        }\n",
    "        df['match'] = df['match'].map(match_mapping)\n",
    "        \n",
    "        # Basic features without bias\n",
    "        df['read_proportion'] = df['reads'] / df['total_asv_reads'].replace(0, np.nan)\n",
    "        df['log_reads'] = np.log1p(df['reads'])\n",
    "        df['read_density'] = df['reads'] / df['asv_count'].replace(0, 1)\n",
    "        df['is_single_asv'] = (df['asv_count'] == 1).astype(int)\n",
    "        df['is_dominant_asv'] = (df['read_proportion'] > 0.9).astype(int)\n",
    "        \n",
    "        # Sample-level features\n",
    "        df['reads_rank'] = df.groupby('project_readfile_id')['reads'].rank(ascending=False)\n",
    "        df['relative_abundance'] = df.groupby('project_readfile_id')['reads'].transform(lambda x: x / x.sum())\n",
    "        df['is_match'] = (df['match'] == 'match').astype(int)\n",
    "        \n",
    "        # Target variable\n",
    "        df['target'] = (df['autopropose'] == 'select').astype(int)\n",
    "        \n",
    "        # Core feature columns\n",
    "        feature_columns = [\n",
    "            'reads', 'total_asv_reads', 'asv_count', 'percentage_reads',\n",
    "            'read_proportion', 'log_reads', 'read_density', 'is_single_asv',\n",
    "            'is_dominant_asv', 'reads_rank', 'relative_abundance', 'is_match'\n",
    "        ]\n",
    "        \n",
    "        return df, feature_columns\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in preprocessing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Cell 5: Model Training Functions\n",
    "def train_model(X: pd.DataFrame, y: pd.Series, feature_columns: List[str]):\n",
    "    \"\"\"Train Random Forest model with comprehensive parameter grid.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"Training Random Forest model...\")\n",
    "        \n",
    "        # Restore original comprehensive parameter grid\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'class_weight': ['balanced'],\n",
    "            'min_samples_leaf': [1]\n",
    "        }\n",
    "        \n",
    "        # Initialize base model\n",
    "        rf = RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Set up grid search with cross-validation\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=rf,\n",
    "            param_grid=param_grid,\n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            scoring=['f1', 'precision', 'recall'],\n",
    "            refit='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit model using parallel processing\n",
    "        with parallel_backend('threading'):\n",
    "            grid_search.fit(X, y)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Calculate model accuracy\n",
    "        y_pred = best_model.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        \n",
    "        # Log results\n",
    "        logging.info(\"\\nModel Training Results:\")\n",
    "        logging.info(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        logging.info(f\"Cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "        logging.info(f\"Test accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return best_model, accuracy, importance, X, y\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in model training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def evaluate_model_stability(X: pd.DataFrame, y: pd.Series, n_repeats: int = 5):\n",
    "    \"\"\"Evaluate model stability across multiple runs.\"\"\"\n",
    "    logging.info(\"\\nEvaluating model stability...\")\n",
    "    \n",
    "    results = []\n",
    "    for i in range(n_repeats):\n",
    "        # Train model with different random seeds\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            class_weight='balanced',\n",
    "            random_state=i\n",
    "        )\n",
    "        \n",
    "        # Use cross-validation\n",
    "        cv_scores = cross_val_score(rf, X, y, cv=5, scoring='f1')\n",
    "        results.append({\n",
    "            'run': i + 1,\n",
    "            'mean_f1': cv_scores.mean(),\n",
    "            'std_f1': cv_scores.std()\n",
    "        })\n",
    "    \n",
    "    # Calculate stability metrics\n",
    "    stability_df = pd.DataFrame(results)\n",
    "    overall_stability = {\n",
    "        'mean_f1': stability_df['mean_f1'].mean(),\n",
    "        'std_f1': stability_df['mean_f1'].std(),\n",
    "        'cv_stability': stability_df['mean_f1'].std() / stability_df['mean_f1'].mean()\n",
    "    }\n",
    "    \n",
    "    logging.info(f\"Model stability metrics: {overall_stability}\")\n",
    "    return overall_stability\n",
    "\n",
    "def save_model_components(model, scaler: StandardScaler, feature_columns: List[str],\n",
    "                        optimal_threshold: float, MODEL_SAVE_DIR: Path) -> None:\n",
    "    \"\"\"Save trained model, scaler, and configuration for future predictions.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"\\nSaving model components...\")\n",
    "        \n",
    "        # Ensure directory exists\n",
    "        MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save the trained model\n",
    "        model_path = MODEL_SAVE_DIR / 'trained_model.joblib'\n",
    "        dump(model, model_path)\n",
    "        logging.info(f\"Saved model to: {model_path}\")\n",
    "        \n",
    "        # Save the scaler\n",
    "        scaler_path = MODEL_SAVE_DIR / 'scaler.joblib'\n",
    "        dump(scaler, scaler_path)\n",
    "        logging.info(f\"Saved scaler to: {scaler_path}\")\n",
    "        \n",
    "        # Save configuration (feature columns and threshold)\n",
    "        config = {\n",
    "            'feature_columns': feature_columns,\n",
    "            'optimal_threshold': float(optimal_threshold),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_parameters': model.get_params()\n",
    "        }\n",
    "        \n",
    "        config_path = MODEL_SAVE_DIR / 'model_config.json'\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "        logging.info(f\"Saved configuration to: {config_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving model components: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def analyze_model_decisions(df: pd.DataFrame, model: RandomForestClassifier, \n",
    "                          X: pd.DataFrame, feature_columns: List[str]):\n",
    "    \"\"\"Analyze how model naturally learned the selection patterns.\"\"\"\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Analyze decision patterns\n",
    "    predictions = model.predict(X)\n",
    "    df['model_prediction'] = predictions\n",
    "    \n",
    "    # Compare with human criteria\n",
    "    patterns = {\n",
    "        'read_ge_4': (df['reads'] >= 4).mean(),\n",
    "        'percent_ge_4': (df['percentage_reads'] >= 4).mean(),\n",
    "        'is_match': (df['match'] == 'match').mean(),\n",
    "        'high_read_low_percent': ((df['reads'] > 10) & (df['percentage_reads'] < 4)).mean()\n",
    "    }\n",
    "    \n",
    "    return importances, patterns\n",
    "\n",
    "# Cell 6: Threshold Optimization Functions\n",
    "def evaluate_threshold(df: pd.DataFrame, threshold: float, \n",
    "                      probabilities: np.ndarray, y_true: np.ndarray) -> Dict:\n",
    "    \"\"\"Evaluate model performance at a specific threshold.\"\"\"\n",
    "    try:\n",
    "        # Make predictions using threshold\n",
    "        y_pred = (probabilities >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        metrics = {\n",
    "            'threshold': float(threshold),\n",
    "            'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "            'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "            'f1_score': float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "            'accuracy': float(accuracy_score(y_true, y_pred))\n",
    "        }\n",
    "        \n",
    "        # Add confusion matrix metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        metrics.update({\n",
    "            'true_positives': int(tp),\n",
    "            'false_positives': int(fp),\n",
    "            'false_negatives': int(fn),\n",
    "            'true_negatives': int(tn)\n",
    "        })\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error evaluating threshold {threshold}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def find_optimal_threshold(df: pd.DataFrame, model: RandomForestClassifier, \n",
    "                         scaler: StandardScaler, feature_columns: List[str]) -> Tuple[pd.DataFrame, float, Dict]:\n",
    "    \"\"\"Find optimal prediction threshold using three-stage search.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"\\nFinding optimal threshold...\")\n",
    "        \n",
    "        # Calculate prediction probabilities\n",
    "        X = df[feature_columns].copy()\n",
    "        X_scaled = pd.DataFrame(\n",
    "            scaler.transform(X),\n",
    "            columns=feature_columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        probabilities = model.predict_proba(X_scaled)[:, 1]\n",
    "        \n",
    "        # Initialize results storage\n",
    "        threshold_results = {\n",
    "            'coarse_search': [],\n",
    "            'medium_search': [],\n",
    "            'fine_search': []\n",
    "        }\n",
    "        \n",
    "        # Stage 1: Coarse search (0-1 in 0.1 steps)\n",
    "        logging.info(\"Performing coarse threshold search...\")\n",
    "        coarse_thresholds = np.linspace(0, 1, 11)\n",
    "        for threshold in coarse_thresholds:\n",
    "            metrics = evaluate_threshold(\n",
    "                df=df,\n",
    "                threshold=float(threshold),\n",
    "                probabilities=probabilities,\n",
    "                y_true=(df['autopropose'] == 'select').astype(int)\n",
    "            )\n",
    "            threshold_results['coarse_search'].append(metrics)\n",
    "        \n",
    "        # Find best coarse threshold\n",
    "        coarse_df = pd.DataFrame(threshold_results['coarse_search'])\n",
    "        best_coarse_idx = coarse_df['f1_score'].idxmax()\n",
    "        best_coarse = float(coarse_df.iloc[best_coarse_idx]['threshold'])\n",
    "        \n",
    "        # Stage 2: Medium search (±0.1 around best coarse)\n",
    "        logging.info(\"Performing medium-grain threshold search...\")\n",
    "        search_width = 0.1\n",
    "        medium_thresholds = np.linspace(\n",
    "            max(0, best_coarse - search_width),\n",
    "            min(1, best_coarse + search_width),\n",
    "            21\n",
    "        )\n",
    "        \n",
    "        for threshold in medium_thresholds:\n",
    "            metrics = evaluate_threshold(\n",
    "                df=df,\n",
    "                threshold=float(threshold),\n",
    "                probabilities=probabilities,\n",
    "                y_true=(df['autopropose'] == 'select').astype(int)\n",
    "            )\n",
    "            threshold_results['medium_search'].append(metrics)\n",
    "        \n",
    "        # Find best medium threshold\n",
    "        medium_df = pd.DataFrame(threshold_results['medium_search'])\n",
    "        best_medium_idx = medium_df['f1_score'].idxmax()\n",
    "        best_medium = float(medium_df.iloc[best_medium_idx]['threshold'])\n",
    "        \n",
    "        # Stage 3: Fine search (±0.01 around best medium)\n",
    "        logging.info(\"Performing fine-grain threshold search...\")\n",
    "        search_width = 0.01\n",
    "        fine_thresholds = np.linspace(\n",
    "            max(0, best_medium - search_width),\n",
    "            min(1, best_medium + search_width),\n",
    "            21\n",
    "        )\n",
    "        \n",
    "        for threshold in fine_thresholds:\n",
    "            metrics = evaluate_threshold(\n",
    "                df=df,\n",
    "                threshold=float(threshold),\n",
    "                probabilities=probabilities,\n",
    "                y_true=(df['autopropose'] == 'select').astype(int)\n",
    "            )\n",
    "            threshold_results['fine_search'].append(metrics)\n",
    "        \n",
    "        # Find optimal threshold\n",
    "        fine_df = pd.DataFrame(threshold_results['fine_search'])\n",
    "        optimal_idx = fine_df['f1_score'].idxmax()\n",
    "        optimal_threshold = float(fine_df.iloc[optimal_idx]['threshold'])\n",
    "        \n",
    "        logging.info(f\"Optimal threshold found: {optimal_threshold:.4f}\")\n",
    "        \n",
    "        # Add probabilities to dataframe\n",
    "        df['prediction_probability'] = probabilities\n",
    "        \n",
    "        return df, optimal_threshold, threshold_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in threshold optimization: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Cell 7: Model Application Functions\n",
    "def apply_model_predictions(df: pd.DataFrame, model: RandomForestClassifier,\n",
    "                          scaler: StandardScaler, feature_columns: List[str],\n",
    "                          threshold: float) -> pd.DataFrame:\n",
    "    \"\"\"Apply model predictions using the optimized threshold.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"\\nApplying model predictions with threshold {threshold:.4f}\")\n",
    "        \n",
    "        df = df.copy()\n",
    "        \n",
    "        # Calculate prediction probabilities\n",
    "        X = df[feature_columns].fillna(0)\n",
    "        X_scaled = pd.DataFrame(\n",
    "            scaler.transform(X),\n",
    "            columns=feature_columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        df['prediction_probability'] = model.predict_proba(X_scaled)[:, 1]\n",
    "        \n",
    "        # Initialize prediction columns\n",
    "        df['model_prediction'] = 0\n",
    "        df['model_decision'] = 'unselect'\n",
    "        df['expert_decision'] = df['autopropose']\n",
    "        \n",
    "        # Process each sample\n",
    "        for sample_id in df['project_readfile_id'].unique():\n",
    "            sample_mask = df['project_readfile_id'] == sample_id\n",
    "            sample_data = df[sample_mask]\n",
    "            \n",
    "            # Find matched ASVs with sufficient reads\n",
    "            matched_data = sample_data[\n",
    "                (sample_data['match'] == 'match') & \n",
    "                (sample_data['reads'] >= 4)\n",
    "            ].copy()\n",
    "            \n",
    "            if not matched_data.empty:\n",
    "                best_idx = matched_data['prediction_probability'].idxmax()\n",
    "                if matched_data.loc[best_idx, 'prediction_probability'] >= threshold:\n",
    "                    df.loc[best_idx, 'model_prediction'] = 1\n",
    "                    df.loc[best_idx, 'model_decision'] = 'select'\n",
    "        \n",
    "        # Calculate agreement between model and expert\n",
    "        df['agreement'] = np.where(\n",
    "            (df['model_decision'] == 'select') & (df['autopropose'] == 'select') |\n",
    "            (df['model_decision'] == 'unselect') & (df['autopropose'] == 'unselect'),\n",
    "            'agree', 'disagree'\n",
    "        )\n",
    "        \n",
    "        # Log prediction statistics\n",
    "        logging.info(\"\\nPrediction Statistics:\")\n",
    "        logging.info(f\"Total samples processed: {df['project_readfile_id'].nunique()}\")\n",
    "        logging.info(f\"Total ASVs selected: {(df['model_decision'] == 'select').sum()}\")\n",
    "        logging.info(f\"Agreement rate: {(df['agreement'] == 'agree').mean():.4f}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in model predictions: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Cell 8: Feature Analysis Functions\n",
    "def analyze_features(df: pd.DataFrame, model: RandomForestClassifier, \n",
    "                    scaler: StandardScaler, feature_columns: List[str], \n",
    "                    threshold_results: Dict) -> Dict:\n",
    "    \"\"\"Analyze features and create visualizations.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"\\nAnalyzing features...\")\n",
    "        plots = {}\n",
    "        \n",
    "        # Get optimal threshold\n",
    "        optimal_threshold = threshold_results.get('optimal_threshold')\n",
    "        if optimal_threshold is None:\n",
    "            logging.warning(\"Optimal threshold not found in results, using default 0.5\")\n",
    "            optimal_threshold = 0.5\n",
    "        \n",
    "        # Log the threshold being used in feature analysis\n",
    "        logging.info(f\"Using threshold {optimal_threshold} for feature analysis\")\n",
    "        \n",
    "        # Validate features\n",
    "        model_features = model.feature_names_in_\n",
    "        if not all(f in df.columns for f in model_features):\n",
    "            raise ValueError(\"Missing required features for model\")\n",
    "        \n",
    "        # Create PCA visualization\n",
    "        pca_fig = create_pca_plot(\n",
    "            df=df,\n",
    "            feature_columns=model_features,\n",
    "            model=model,\n",
    "            scaler=scaler,\n",
    "            optimal_threshold=optimal_threshold\n",
    "        )\n",
    "        \n",
    "        if pca_fig:\n",
    "            plots['pca_analysis'] = [('PCA Analysis', pca_fig)]\n",
    "        \n",
    "        # Create feature distribution plots - explicitly passing optimal_threshold\n",
    "        feature_plots = create_feature_distribution_plots(\n",
    "            df=df,\n",
    "            model=model,\n",
    "            scaler=scaler,\n",
    "            feature_columns=model_features,\n",
    "            optimal_threshold=optimal_threshold\n",
    "        )\n",
    "        \n",
    "        if feature_plots:\n",
    "            plots['feature_analysis'] = feature_plots\n",
    "        \n",
    "        logging.info(\"Feature analysis complete!\")\n",
    "        return plots\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in feature analysis: {str(e)}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return {}\n",
    "\n",
    "# Cell 9: Performance Analysis Functions\n",
    "def analyze_correlation(df: pd.DataFrame, feature_columns: List[str]) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"Analyze correlations between features.\"\"\"\n",
    "    try:\n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = df[feature_columns].corr()\n",
    "        \n",
    "        # Calculate feature clusters using hierarchical clustering\n",
    "        linkage_matrix = linkage(squareform(1 - corr_matrix), method='ward')\n",
    "        \n",
    "        return corr_matrix, linkage_matrix\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in correlation analysis: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def analyze_taxonomic_patterns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Analyze taxonomic patterns in ASV selection.\"\"\"\n",
    "    try:\n",
    "        if 'taxonomy' not in df.columns:\n",
    "            return None\n",
    "            \n",
    "        # Calculate taxonomic distribution\n",
    "        tax_dist = df.groupby(['taxonomy', 'model_decision']).size().unstack(fill_value=0)\n",
    "        \n",
    "        # Calculate selection rates by taxonomy\n",
    "        tax_stats = pd.DataFrame({\n",
    "            'total': tax_dist.sum(axis=1),\n",
    "            'selected': tax_dist['select'],\n",
    "            'selection_rate': tax_dist['select'] / tax_dist.sum(axis=1)\n",
    "        }).round(4)\n",
    "        \n",
    "        return tax_stats\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in taxonomic analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_error_patterns(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze error patterns in ASV selection.\"\"\"\n",
    "    try:\n",
    "        error_analysis = {\n",
    "            'confusion_matrix': confusion_matrix(\n",
    "                df['autopropose'] == 'select',\n",
    "                df['model_decision'] == 'select'\n",
    "            ),\n",
    "            'classification_report': classification_report(\n",
    "                df['autopropose'] == 'select',\n",
    "                df['model_decision'] == 'select',\n",
    "                output_dict=True\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Analyze characteristics of misclassified cases\n",
    "        misclassified = df[df['agreement'] == 'disagree']\n",
    "        error_analysis['misclassified_stats'] = {\n",
    "            'count': len(misclassified),\n",
    "            'mean_reads': misclassified['reads'].mean(),\n",
    "            'mean_percentage': misclassified['percentage_reads'].mean(),\n",
    "            'asv_count_stats': misclassified['asv_count'].describe().to_dict()\n",
    "        }\n",
    "        \n",
    "        return error_analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error analyzing error patterns: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def analyze_asv_selection(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Analyze and display summary statistics for selected vs all ASVs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Separate selected ASVs\n",
    "        selected_df = df[df['model_decision'] == 'select']\n",
    "        \n",
    "        # Calculate basic statistics\n",
    "        total_asvs = len(df)\n",
    "        selected_asvs = len(selected_df)\n",
    "        selection_rate = (selected_asvs / total_asvs) * 100\n",
    "        \n",
    "        # Create summary statistics for numeric columns\n",
    "        numeric_cols = ['reads', 'total_asv_reads', 'asv_count', 'percentage_reads', \n",
    "                       'read_proportion', 'log_reads', 'read_density']\n",
    "        \n",
    "        all_summary = df[numeric_cols].describe()\n",
    "        selected_summary = selected_df[numeric_cols].describe()\n",
    "        \n",
    "        # Display results using markdown for better formatting\n",
    "        display(Markdown(f\"\"\"\n",
    "## ASV Selection Summary\n",
    "\n",
    "### Overview Statistics\n",
    "- Total ASVs analyzed: {total_asvs:,}\n",
    "- ASVs selected: {selected_asvs:,}\n",
    "- Selection rate: {selection_rate:.2f}%\n",
    "\n",
    "### Comparison of All vs Selected ASVs\n",
    "\"\"\"))\n",
    "        \n",
    "        # Display side-by-side comparison\n",
    "        comparison = pd.concat([all_summary, selected_summary], axis=1, \n",
    "                             keys=['All ASVs', 'Selected ASVs'])\n",
    "        display(comparison)\n",
    "        \n",
    "        # Additional categorical summaries if available\n",
    "        if 'taxonomy' in df.columns:\n",
    "            display(Markdown(\"\\n### Taxonomic Distribution of Selected ASVs\"))\n",
    "            tax_dist = pd.crosstab(df['taxonomy'], df['model_decision'])\n",
    "            tax_dist['selection_rate'] = (tax_dist['select'] / tax_dist.sum(axis=1) * 100).round(2)\n",
    "            display(tax_dist)\n",
    "        \n",
    "        if 'match' in df.columns:\n",
    "            display(Markdown(\"\\n### Match Statistics for Selected ASVs\"))\n",
    "            match_stats = pd.crosstab(selected_df['match'], selected_df['model_decision'])\n",
    "            display(match_stats)\n",
    "        \n",
    "        # Log the summary\n",
    "        logging.info(f\"ASV Selection Summary - Total: {total_asvs}, Selected: {selected_asvs}, Rate: {selection_rate:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in ASV selection analysis: {str(e)}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        display(Markdown(\"❌ Error generating ASV selection summary\"))\n",
    "\n",
    "# Cell 10: Visualization Functions\n",
    "def create_pca_plot(df: pd.DataFrame, feature_columns: List[str],\n",
    "                   model: RandomForestClassifier, scaler: StandardScaler,\n",
    "                   optimal_threshold: float) -> go.Figure:\n",
    "    \"\"\"Create enhanced PCA visualization with threshold line.\"\"\"\n",
    "    try:\n",
    "        # Prepare data\n",
    "        X = df[feature_columns]\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        # Perform PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_result = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # Get predictions and probabilities\n",
    "        probabilities = model.predict_proba(X_scaled)[:, 1]\n",
    "        predictions = (probabilities >= optimal_threshold).astype(int)\n",
    "        \n",
    "        # Create figure with secondary y-axis for colorbar\n",
    "        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "        \n",
    "        # Add scatter points colored by probability\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=pca_result[:, 0],\n",
    "                y=pca_result[:, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=12,\n",
    "                    color=probabilities,\n",
    "                    colorscale='Viridis',\n",
    "                    showscale=True,\n",
    "                    colorbar=dict(\n",
    "                        title='Selection Probability',\n",
    "                        tickformat='.2f'\n",
    "                    ),\n",
    "                    line=dict(width=1, color='black')\n",
    "                ),\n",
    "                text=[f\"Probability: {p:.3f}<br>Selected: {p >= optimal_threshold}\" \n",
    "                      for p in probabilities],\n",
    "                hoverinfo='text',\n",
    "                name='ASVs'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add threshold line annotation\n",
    "        fig.add_annotation(\n",
    "            xref='paper',\n",
    "            yref='paper',\n",
    "            x=0.02,\n",
    "            y=0.98,\n",
    "            text=f'Selection Threshold: {optimal_threshold:.3f}',\n",
    "            showarrow=False,\n",
    "            font=dict(size=12),\n",
    "            bgcolor='rgba(255, 255, 255, 0.8)',\n",
    "            bordercolor='red',\n",
    "            borderwidth=2\n",
    "        )\n",
    "        \n",
    "        # Color points based on selection\n",
    "        for i, (label, color) in enumerate(zip(['Unselected', 'Selected'], ['red', 'green'])):\n",
    "            mask = predictions == i\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=pca_result[mask, 0],\n",
    "                    y=pca_result[mask, 1],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=12,\n",
    "                        opacity=0.5,\n",
    "                        line=dict(color=color, width=2)\n",
    "                    ),\n",
    "                    name=label,\n",
    "                    showlegend=True\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title='PCA Analysis with Selection Boundary',\n",
    "            xaxis_title=f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)',\n",
    "            yaxis_title=f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)',\n",
    "            plot_bgcolor='white',\n",
    "            width=900,\n",
    "            height=700,\n",
    "            legend=dict(\n",
    "                yanchor=\"top\",\n",
    "                y=0.99,\n",
    "                xanchor=\"left\",\n",
    "                x=0.01,\n",
    "                bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "                bordercolor=\"black\",\n",
    "                borderwidth=1\n",
    "            ),\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Add grid\n",
    "        fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "        fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating PCA plot: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_feature_distribution_plots(df: pd.DataFrame, model: RandomForestClassifier,\n",
    "                                   scaler: StandardScaler, feature_columns: List[str],\n",
    "                                   optimal_threshold: float) -> List[Tuple[str, go.Figure]]:\n",
    "    \"\"\"Create feature distribution plots with correct threshold value.\"\"\"\n",
    "    plots = []\n",
    "    logging.info(f\"Creating feature plots with threshold: {optimal_threshold}\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare data\n",
    "        X = df[feature_columns].copy()\n",
    "        X_scaled = pd.DataFrame(\n",
    "            scaler.transform(X),\n",
    "            columns=feature_columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        probabilities = model.predict_proba(X_scaled)[:, 1]\n",
    "        \n",
    "        # Create plots for each feature\n",
    "        for feature in feature_columns:\n",
    "            if feature == feature_columns[0]:\n",
    "                logging.info(f\"Using threshold {optimal_threshold} in feature plots\")\n",
    "            \n",
    "            # Create subplots\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=1,\n",
    "                subplot_titles=(\n",
    "                    f'{feature} Distribution by Decision',\n",
    "                    f'Selection Probability vs {feature}'\n",
    "                ),\n",
    "                vertical_spacing=0.20,\n",
    "                row_heights=[0.6, 0.4]\n",
    "            )\n",
    "            \n",
    "            # Distribution plot (top)\n",
    "            for decision in ['select', 'unselect']:\n",
    "                mask = df['model_decision'] == decision\n",
    "                fig.add_trace(\n",
    "                    go.Histogram(\n",
    "                        x=df[feature][mask],\n",
    "                        name=decision,\n",
    "                        opacity=0.7,\n",
    "                        marker_color='green' if decision == 'select' else 'red',\n",
    "                        nbinsx=30,\n",
    "                        showlegend=True\n",
    "                    ),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "            \n",
    "            # Find feature value at optimal threshold\n",
    "            threshold_mask = (probabilities >= optimal_threshold)\n",
    "            if any(threshold_mask):\n",
    "                threshold_value = df[feature][threshold_mask].min()\n",
    "            else:\n",
    "                threshold_value = df[feature].median()\n",
    "            \n",
    "            # Add vertical threshold line\n",
    "            fig.add_vline(\n",
    "                x=threshold_value,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"black\",\n",
    "                opacity=0.5,\n",
    "                row=1, col=1,\n",
    "                annotation=dict(\n",
    "                    text=f\"Feature Value at Threshold: {threshold_value:.3f}\",\n",
    "                    font=dict(size=10),\n",
    "                    xanchor='left',\n",
    "                    yanchor='top'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Probability scatter plot (bottom)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df[feature],\n",
    "                    y=probabilities,\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        color=probabilities,\n",
    "                        colorscale='Viridis',\n",
    "                        showscale=True,\n",
    "                        colorbar=dict(\n",
    "                            title='Probability',\n",
    "                            y=0.2,\n",
    "                            len=0.4\n",
    "                        ),\n",
    "                        size=8,\n",
    "                        opacity=0.6\n",
    "                    ),\n",
    "                    name='Probability'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # Add horizontal threshold line\n",
    "            fig.add_hline(\n",
    "                y=optimal_threshold,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"red\",\n",
    "                opacity=0.7,\n",
    "                row=2, col=1,\n",
    "                annotation=dict(\n",
    "                    text=f\"Selection Threshold: {optimal_threshold:.3f}\",\n",
    "                    font=dict(size=10, color='red'),\n",
    "                    xanchor='left',\n",
    "                    yanchor='bottom'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                height=800,\n",
    "                title=dict(\n",
    "                    text=f'Feature Analysis: {feature}<br><sup>Optimal Threshold: {optimal_threshold:.3f}</sup>',\n",
    "                    x=0.5,\n",
    "                    y=0.95\n",
    "                ),\n",
    "                showlegend=True,\n",
    "                legend=dict(\n",
    "                    yanchor=\"top\",\n",
    "                    y=0.99,\n",
    "                    xanchor=\"right\",\n",
    "                    x=0.99\n",
    "                ),\n",
    "                barmode='overlay'\n",
    "            )\n",
    "            \n",
    "            # Update axes labels\n",
    "            fig.update_xaxes(title_text=feature, row=1, col=1)\n",
    "            fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "            fig.update_xaxes(title_text=f\"Feature Value\", row=2, col=1)\n",
    "            fig.update_yaxes(\n",
    "                title_text=\"Selection Probability\",\n",
    "                range=[0, 1],  # Force y-axis range\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # Add feature statistics\n",
    "            feature_stats = df.groupby('model_decision')[feature].describe()\n",
    "            stats_text = (\n",
    "                f\"Feature Statistics:<br>\"\n",
    "                f\"Selected (n={len(df[df['model_decision']=='select'])})<br>\"\n",
    "                f\"• Mean: {feature_stats.loc['select', 'mean']:.2f}<br>\"\n",
    "                f\"• Std: {feature_stats.loc['select', 'std']:.2f}<br>\"\n",
    "                f\"Unselected (n={len(df[df['model_decision']=='unselect'])})<br>\"\n",
    "                f\"• Mean: {feature_stats.loc['unselect', 'mean']:.2f}<br>\"\n",
    "                f\"• Std: {feature_stats.loc['unselect', 'std']:.2f}\"\n",
    "            )\n",
    "            \n",
    "            fig.add_annotation(\n",
    "                text=stats_text,\n",
    "                xref=\"paper\", yref=\"paper\",\n",
    "                x=0.01, y=0.99,\n",
    "                showarrow=False,\n",
    "                font=dict(size=10),\n",
    "                align=\"left\",\n",
    "                bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "                bordercolor=\"black\",\n",
    "                borderwidth=1\n",
    "            )\n",
    "            \n",
    "            plots.append((f'feature_distribution_{feature}', fig))\n",
    "            \n",
    "        return plots\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating feature distribution plots: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def create_correlation_plots(corr_matrix: pd.DataFrame, linkage_matrix: np.ndarray) -> List[Tuple[str, go.Figure]]:\n",
    "    \"\"\"Create correlation analysis plots.\"\"\"\n",
    "    plots = []\n",
    "    try:\n",
    "        # Correlation heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=corr_matrix,\n",
    "            x=corr_matrix.columns,\n",
    "            y=corr_matrix.columns,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,\n",
    "            text=np.round(corr_matrix, 2),\n",
    "            texttemplate='%{text}',\n",
    "            textfont={\"size\": 10}\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Feature Correlation Matrix',\n",
    "            height=800,\n",
    "            width=800\n",
    "        )\n",
    "        \n",
    "        plots.append(('correlation_matrix', fig))\n",
    "        \n",
    "        # Dendrogram of feature relationships\n",
    "        dendro_fig = create_dendrogram(linkage_matrix, corr_matrix.columns)\n",
    "        if dendro_fig:\n",
    "            plots.append(('feature_dendrogram', dendro_fig))\n",
    "        \n",
    "        return plots\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating correlation plots: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def create_dendrogram(linkage_matrix: np.ndarray, labels: List[str]) -> go.Figure:\n",
    "    \"\"\"Create enhanced dendrogram visualization of feature relationships.\"\"\"\n",
    "    try:\n",
    "        # Input validation\n",
    "        if linkage_matrix is None or len(linkage_matrix) == 0:\n",
    "            raise ValueError(\"Empty linkage matrix provided\")\n",
    "        if labels is None or len(labels) == 0:\n",
    "            raise ValueError(\"Empty labels provided\")\n",
    "\n",
    "        # Create figure\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Create dendrogram trace with explicit orientation\n",
    "        dendro = dendrogram(\n",
    "            linkage_matrix,\n",
    "            labels=labels,\n",
    "            orientation='bottom',\n",
    "            leaf_rotation=45,\n",
    "            no_plot=True\n",
    "        )\n",
    "        \n",
    "        # Extract x and y coordinates for plotting\n",
    "        x = []\n",
    "        y = []\n",
    "        \n",
    "        for i, d in enumerate(dendro['dcoord']):\n",
    "            # Safely get coordinates\n",
    "            if i < len(dendro['icoord']):\n",
    "                x_coords = dendro['icoord'][i]\n",
    "                x.extend(x_coords)\n",
    "                y.extend(d)\n",
    "                x.append(None)  # Add break in line\n",
    "                y.append(None)\n",
    "        \n",
    "        # Add dendrogram trace\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            mode='lines',\n",
    "            line=dict(\n",
    "                color='#2c3e50',\n",
    "                width=2\n",
    "            ),\n",
    "            hoverinfo='none'\n",
    "        ))\n",
    "        \n",
    "        # Add leaf labels more robustly\n",
    "        if 'ivl' in dendro and 'icoord' in dendro:\n",
    "            leaf_positions = []\n",
    "            for i, label in enumerate(dendro['ivl']):\n",
    "                if i < len(dendro['icoord']):\n",
    "                    x_pos = dendro['icoord'][i][0]  # Get x position for label\n",
    "                    leaf_positions.append(x_pos)\n",
    "                    \n",
    "                    # Add connecting lines\n",
    "                    fig.add_shape(\n",
    "                        type=\"line\",\n",
    "                        x0=x_pos,\n",
    "                        y0=0,\n",
    "                        x1=x_pos,\n",
    "                        y1=-5,\n",
    "                        line=dict(\n",
    "                            color=\"#2c3e50\",\n",
    "                            width=1,\n",
    "                            dash=\"dot\"\n",
    "                        )\n",
    "                    )\n",
    "        \n",
    "            # Update layout with improved styling\n",
    "            fig.update_layout(\n",
    "                title={\n",
    "                    'text': 'Feature Clustering Dendrogram',\n",
    "                    'y': 0.95,\n",
    "                    'x': 0.5,\n",
    "                    'xanchor': 'center',\n",
    "                    'yanchor': 'top',\n",
    "                    'font': dict(size=20)\n",
    "                },\n",
    "                showlegend=False,\n",
    "                xaxis=dict(\n",
    "                    ticktext=dendro['ivl'],\n",
    "                    tickvals=leaf_positions,\n",
    "                    tickmode=\"array\",\n",
    "                    showticklabels=True,\n",
    "                    tickangle=45,\n",
    "                    title='Features',\n",
    "                    title_font=dict(size=16),\n",
    "                    tickfont=dict(size=12)\n",
    "                ),\n",
    "                yaxis=dict(\n",
    "                    title='Distance',\n",
    "                    title_font=dict(size=16),\n",
    "                    tickfont=dict(size=12)\n",
    "                ),\n",
    "                height=800,\n",
    "                width=1200,\n",
    "                plot_bgcolor='white',\n",
    "                hoverlabel=dict(\n",
    "                    bgcolor=\"white\",\n",
    "                    font_size=12,\n",
    "                    font_family=\"Arial\"\n",
    "                ),\n",
    "                margin=dict(b=150)  # Increase bottom margin for labels\n",
    "            )\n",
    "        \n",
    "            # Add grid\n",
    "            fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "            fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating dendrogram: {str(e)}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def create_threshold_analysis_plots(threshold_results: Dict) -> List[Tuple[str, go.Figure]]:\n",
    "    \"\"\"Create threshold analysis plots.\"\"\"\n",
    "    plots = []\n",
    "    try:\n",
    "        # Threshold optimization curve\n",
    "        fig = go.Figure()\n",
    "        for search_type in ['coarse_search', 'medium_search', 'fine_search']:\n",
    "            df = pd.DataFrame(threshold_results[search_type])\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df['threshold'],\n",
    "                y=df['f1_score'],\n",
    "                name=search_type.replace('_', ' ').title(),\n",
    "                mode='lines+markers'\n",
    "            ))\n",
    "        fig.update_layout(\n",
    "            title='Threshold Optimization',\n",
    "            xaxis_title='Threshold',\n",
    "            yaxis_title='F1 Score'\n",
    "        )\n",
    "        plots.append(('threshold_optimization', fig))\n",
    "\n",
    "        return plots\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating threshold plots: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def create_agreement_analysis_plots(df: pd.DataFrame) -> List[Tuple[str, go.Figure]]:\n",
    "    plots = []\n",
    "    try:\n",
    "        # 1. Agreement Distribution Pie Chart\n",
    "        agreement_counts = df['agreement'].value_counts()\n",
    "        \n",
    "        fig = go.Figure(data=[go.Pie(\n",
    "            labels=agreement_counts.index,\n",
    "            values=agreement_counts.values,\n",
    "            hole=0.4,\n",
    "            marker=dict(colors=['#2ecc71', '#e74c3c']),\n",
    "            textinfo='value+percent',\n",
    "            hovertemplate=\"Status: %{label}<br>Count: %{value}<br>Percentage: %{percent}<extra></extra>\"\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Model-Expert Agreement Distribution',\n",
    "            annotations=[{\n",
    "                'text': f'Total ASVs:<br>{len(df):,}',\n",
    "                'x': 0.5,\n",
    "                'y': 0.5,\n",
    "                'font_size': 12,\n",
    "                'showarrow': False\n",
    "            }],\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        plots.append(('agreement_distribution', fig))\n",
    "        \n",
    "        # 2. Agreement Matrix\n",
    "        confusion = pd.crosstab(df['autopropose'], df['model_decision'])\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=confusion.values,\n",
    "            x=confusion.columns,\n",
    "            y=confusion.index,\n",
    "            text=confusion.values,\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 14},\n",
    "            colorscale='RdYlGn',\n",
    "            showscale=True\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Decision Agreement Matrix',\n",
    "            xaxis_title='Model Decision',\n",
    "            yaxis_title='Expert Decision',\n",
    "            template='plotly_white',\n",
    "            height=500,\n",
    "            width=600\n",
    "        )\n",
    "        plots.append(('decision_matrix', fig))\n",
    "        \n",
    "        return plots\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating agreement plots: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def create_model_performance_plots(model: RandomForestClassifier, X_test: pd.DataFrame, \n",
    "                                y_test: pd.Series, feature_importance: pd.DataFrame) -> List[Tuple[str, go.Figure]]:\n",
    "    \"\"\"Create comprehensive model performance plots.\"\"\"\n",
    "    plots = []\n",
    "    try:\n",
    "        # 1. Enhanced Feature Importance plot\n",
    "        sorted_importance = feature_importance.sort_values('importance', ascending=True)\n",
    "        \n",
    "        # Calculate percentages and cumulative importance\n",
    "        total_importance = sorted_importance['importance'].sum()\n",
    "        sorted_importance['percentage'] = (sorted_importance['importance'] / total_importance * 100)\n",
    "        sorted_importance['cumulative'] = sorted_importance['percentage'].cumsum()\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add main bar trace\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=sorted_importance['importance'],\n",
    "            y=sorted_importance['feature'],\n",
    "            orientation='h',\n",
    "            marker_color='#3498db',\n",
    "            text=[f\"{val:.3f} ({pct:.1f}%)\" for val, pct in \n",
    "                  zip(sorted_importance['importance'], sorted_importance['percentage'])],\n",
    "            textposition='outside',\n",
    "            name='Feature Importance',\n",
    "            hovertemplate=\"<b>%{y}</b><br>\" +\n",
    "                         \"Importance: %{x:.3f}<br>\" +\n",
    "                         \"Contribution: %{text}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        # Add cumulative line\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=sorted_importance['importance'],\n",
    "            y=sorted_importance['feature'],\n",
    "            mode='markers+lines',\n",
    "            marker=dict(size=8, color='#e74c3c'),\n",
    "            line=dict(color='#e74c3c', dash='dot'),\n",
    "            name='Cumulative %',\n",
    "            text=[f\"{pct:.1f}%\" for pct in sorted_importance['cumulative']],\n",
    "            textposition='middle right',\n",
    "            yaxis='y',\n",
    "            xaxis='x2',\n",
    "            hovertemplate=\"Cumulative: %{text}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'Feature Importance Analysis',\n",
    "                'y': 0.95,\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(size=20)\n",
    "            },\n",
    "            xaxis_title=\"Importance Score\",\n",
    "            yaxis_title=\"Feature\",\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            showlegend=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            ),\n",
    "            plot_bgcolor='white',\n",
    "            xaxis2=dict(\n",
    "                overlaying='x',\n",
    "                side='top',\n",
    "                range=[0, max(sorted_importance['importance'])],\n",
    "                showgrid=False,\n",
    "                zeroline=False,\n",
    "                showticklabels=False\n",
    "            ),\n",
    "            margin=dict(l=200, r=200)  # Adjust margins for labels\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "        fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "        \n",
    "        plots.append(('feature_importance', fig))\n",
    "        \n",
    "        # 2. Enhanced ROC Curve\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add ROC curve\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=fpr,\n",
    "            y=tpr,\n",
    "            name=f'ROC (AUC = {roc_auc:.3f})',\n",
    "            mode='lines',\n",
    "            line=dict(color='#3498db', width=2),\n",
    "            fill='tozeroy',\n",
    "            fillcolor='rgba(52, 152, 219, 0.2)',\n",
    "            hovertemplate=\"False Positive Rate: %{x:.3f}<br>\" +\n",
    "                         \"True Positive Rate: %{y:.3f}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        # Add diagonal line\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[0, 1],\n",
    "            y=[0, 1],\n",
    "            name='Random',\n",
    "            mode='lines',\n",
    "            line=dict(color='#95a5a6', dash='dash'),\n",
    "            hovertemplate=\"Random Classifier<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        # Add optimal threshold point\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[fpr[optimal_idx]],\n",
    "            y=[tpr[optimal_idx]],\n",
    "            name=f'Optimal Threshold ({optimal_threshold:.3f})',\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=12,\n",
    "                color='#e74c3c',\n",
    "                symbol='star'\n",
    "            ),\n",
    "            hovertemplate=\"Optimal Threshold<br>\" +\n",
    "                         f\"Value: {optimal_threshold:.3f}<br>\" +\n",
    "                         \"FPR: %{x:.3f}<br>\" +\n",
    "                         \"TPR: %{y:.3f}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'ROC Curve Analysis',\n",
    "                'y': 0.95,\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(size=20)\n",
    "            },\n",
    "            xaxis_title=\"False Positive Rate\",\n",
    "            yaxis_title=\"True Positive Rate\",\n",
    "            height=800,\n",
    "            width=800,\n",
    "            showlegend=True,\n",
    "            legend=dict(\n",
    "                yanchor=\"bottom\",\n",
    "                y=0.01,\n",
    "                xanchor=\"right\",\n",
    "                x=0.99,\n",
    "                bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "                bordercolor=\"black\",\n",
    "                borderwidth=1\n",
    "            ),\n",
    "            plot_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgray', range=[-0.01, 1.01])\n",
    "        fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgray', range=[-0.01, 1.01])\n",
    "        \n",
    "        plots.append(('roc_curve', fig))\n",
    "        \n",
    "        # 3. Enhanced Confusion Matrix\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        total = np.sum(cm)\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        ppv = tp / (tp + fp)\n",
    "        npv = tn / (tn + fn)\n",
    "        \n",
    "        # Create annotated heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=cm,\n",
    "            x=['Predicted Negative', 'Predicted Positive'],\n",
    "            y=['Actual Negative', 'Actual Positive'],\n",
    "            text=[[f\"\"\"\n",
    "            Count: {val}\n",
    "            Percentage: {(val/total)*100:.1f}%\"\"\" for val in row] for row in cm],\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 12},\n",
    "            colorscale=[[0, '#f8d7da'], [1, '#c3e6cb']],\n",
    "            showscale=False\n",
    "        ))\n",
    "        \n",
    "        # Add metrics annotations\n",
    "        annotations = [\n",
    "            dict(\n",
    "                x=1.3,\n",
    "                y=1,\n",
    "                xref=\"paper\",\n",
    "                yref=\"paper\",\n",
    "                text=f\"\"\"\n",
    "                Model Metrics:\n",
    "                Sensitivity: {sensitivity:.3f}\n",
    "                Specificity: {specificity:.3f}\n",
    "                PPV: {ppv:.3f}\n",
    "                NPV: {npv:.3f}\n",
    "                \"\"\",\n",
    "                showarrow=False,\n",
    "                font=dict(size=12),\n",
    "                align=\"left\",\n",
    "                bgcolor=\"white\",\n",
    "                bordercolor=\"black\",\n",
    "                borderwidth=1,\n",
    "                borderpad=4\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'Confusion Matrix Analysis',\n",
    "                'y': 0.95,\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(size=20)\n",
    "            },\n",
    "            xaxis_title=\"Predicted Class\",\n",
    "            yaxis_title=\"Actual Class\",\n",
    "            height=800,\n",
    "            width=1000,\n",
    "            plot_bgcolor='white',\n",
    "            annotations=annotations\n",
    "        )\n",
    "        \n",
    "        plots.append(('confusion_matrix', fig))\n",
    "        \n",
    "        return plots\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating model performance plots: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def create_taxonomic_analysis_plots(df: pd.DataFrame) -> List[Tuple[str, go.Figure]]:\n",
    "    plots = []\n",
    "    try:\n",
    "        if 'taxonomy' not in df.columns:\n",
    "            logging.warning(\"No taxonomy column found in the data\")\n",
    "            return plots\n",
    "        \n",
    "        df_clean = df.copy()\n",
    "        df_clean['taxonomy'] = df_clean['taxonomy'].fillna('Unknown')\n",
    "        \n",
    "        # 1. Sunburst with Multiple Levels\n",
    "        fig = go.Figure(go.Sunburst(\n",
    "            ids=[f\"{row['taxonomy']}_{row['model_decision']}\" for _, row in df_clean.iterrows()],\n",
    "            labels=[f\"{row['taxonomy']}\" for _, row in df_clean.iterrows()],\n",
    "            parents=[row['model_decision'] for _, row in df_clean.iterrows()],\n",
    "            values=[row['reads'] for _, row in df_clean.iterrows()],\n",
    "            branchvalues='total',\n",
    "            marker=dict(\n",
    "                colors=[\n",
    "                    '#2ecc71' if row['model_decision'] == 'select' else '#e74c3c' \n",
    "                    for _, row in df_clean.iterrows()\n",
    "                ]\n",
    "            ),\n",
    "            hovertemplate=\"Taxonomy: %{label}<br>Reads: %{value:,.0f}<br>Decision: %{parent}\"\n",
    "        ))\n",
    "        fig.update_layout(title='Taxonomic Distribution Sunburst', width=800, height=800)\n",
    "        plots.append(('taxonomy_sunburst', fig))\n",
    "        \n",
    "        # 2. Enhanced Stacked Bar Chart\n",
    "        tax_stats = df_clean.groupby(['taxonomy', 'model_decision']).agg({\n",
    "            'reads': 'sum',\n",
    "            'project_readfile_id': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        for decision, color in zip(['select', 'unselect'], ['#2ecc71', '#e74c3c']):\n",
    "            mask = tax_stats['model_decision'] == decision\n",
    "            fig.add_trace(go.Bar(\n",
    "                name=decision.capitalize(),\n",
    "                x=tax_stats[mask]['taxonomy'],\n",
    "                y=tax_stats[mask]['project_readfile_id'],\n",
    "                marker_color=color,\n",
    "                text=tax_stats[mask]['project_readfile_id'],\n",
    "                textposition='auto',\n",
    "                hovertemplate=\"Taxonomy: %{x}<br>\" +\n",
    "                             \"Count: %{y}<br>\" +\n",
    "                             \"Decision: \" + decision + \"<extra></extra>\"\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Taxonomic Distribution by Decision',\n",
    "            barmode='stack',\n",
    "            xaxis_title='Taxonomy',\n",
    "            yaxis_title='Count',\n",
    "            height=600,\n",
    "            showlegend=True\n",
    "        )\n",
    "        plots.append(('taxonomy_stacked_bar', fig))\n",
    "        \n",
    "        # 3. Bubble Chart with Multiple Metrics\n",
    "        tax_metrics = df_clean.groupby('taxonomy').agg({\n",
    "            'reads': 'sum',\n",
    "            'model_decision': lambda x: (x == 'select').mean(),\n",
    "            'project_readfile_id': 'nunique'\n",
    "        }).reset_index()\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=tax_metrics['project_readfile_id'],\n",
    "            y=tax_metrics['model_decision'] * 100,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=np.sqrt(tax_metrics['reads'])/100,\n",
    "                sizemode='area',\n",
    "                sizeref=2.*max(np.sqrt(tax_metrics['reads'])/100)/(40.**2),\n",
    "                color=tax_metrics['model_decision'],\n",
    "                colorscale='RdYlGn',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title='Selection Rate')\n",
    "            ),\n",
    "            text=tax_metrics['taxonomy'],\n",
    "            hovertemplate=\"Taxonomy: %{text}<br>\" +\n",
    "                         \"Samples: %{x}<br>\" +\n",
    "                         \"Selection Rate: %{y:.1f}%<br>\" +\n",
    "                         \"Total Reads: %{marker.size:,.0f}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Taxonomic Metrics Bubble Chart',\n",
    "            xaxis_title='Number of Samples',\n",
    "            yaxis_title='Selection Rate (%)',\n",
    "            height=800,\n",
    "            showlegend=False\n",
    "        )\n",
    "        plots.append(('taxonomy_bubble', fig))\n",
    "        \n",
    "        # 4. Heatmap of Read Distribution\n",
    "        pivot_reads = pd.pivot_table(\n",
    "            df_clean,\n",
    "            values='reads',\n",
    "            index='taxonomy',\n",
    "            columns='model_decision',\n",
    "            aggfunc='sum',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        fig = go.Figure(go.Heatmap(\n",
    "            z=np.log10(pivot_reads.values + 1),\n",
    "            x=pivot_reads.columns,\n",
    "            y=pivot_reads.index,\n",
    "            colorscale='Viridis',\n",
    "            text=pivot_reads.values,\n",
    "            texttemplate='%{text:,.0f}',\n",
    "            hovertemplate=\"Taxonomy: %{y}<br>\" +\n",
    "                         \"Decision: %{x}<br>\" +\n",
    "                         \"Reads: %{text:,.0f}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Read Distribution Heatmap (log10 scale)',\n",
    "            xaxis_title='Model Decision',\n",
    "            yaxis_title='Taxonomy',\n",
    "            height=800\n",
    "        )\n",
    "        plots.append(('taxonomy_heatmap', fig))\n",
    "        \n",
    "        # 5. Parallel Categories Diagram\n",
    "        fig = go.Figure(go.Parcats(\n",
    "            dimensions=[\n",
    "                {\n",
    "                    'label': 'Taxonomy',\n",
    "                    'values': df_clean['taxonomy']\n",
    "                },\n",
    "                {\n",
    "                    'label': 'Match',\n",
    "                    'values': df_clean['match']\n",
    "                },\n",
    "                {\n",
    "                    'label': 'Decision',\n",
    "                    'values': df_clean['model_decision']\n",
    "                }\n",
    "            ],\n",
    "            line=dict(\n",
    "                color=np.log10(df_clean['reads'] + 1),\n",
    "                colorscale='Viridis'\n",
    "            ),\n",
    "            hoveron='color',\n",
    "            hovertemplate='Reads: %{color:.2f}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Taxonomy-Match-Decision Relationships',\n",
    "            height=800\n",
    "        )\n",
    "        plots.append(('taxonomy_parallel', fig))\n",
    "        \n",
    "        return plots\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating taxonomic plots: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def create_asv_mmg_analysis_plots(df: pd.DataFrame) -> List[Tuple[str, go.Figure]]:\n",
    "    \"\"\"Create comprehensive ASV-MMG analysis plots.\"\"\"\n",
    "    plots = []\n",
    "    try:\n",
    "        if 'mt_id' not in df.columns:\n",
    "            logging.warning(\"No mt_id column found in the data\")\n",
    "            return plots\n",
    "            \n",
    "        df_clean = df.copy()\n",
    "        df_clean['mt_id'] = df_clean['mt_id'].fillna('No Match')\n",
    "\n",
    "        # 1. Enhanced Distribution Stacked Bar Plot\n",
    "        mmg_stats = df_clean.groupby(['mt_id', 'model_decision']).agg({\n",
    "            'reads': ['sum', 'mean', 'count'],\n",
    "            'project_readfile_id': 'nunique'\n",
    "        }).reset_index()\n",
    "        \n",
    "        mmg_stats.columns = ['mt_id', 'decision', 'total_reads', 'mean_reads', 'count', 'samples']\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        for decision, color in zip(['select', 'unselect'], ['#2ecc71', '#e74c3c']):\n",
    "            mask = mmg_stats['decision'] == decision\n",
    "            fig.add_trace(go.Bar(\n",
    "                name=f\"{decision.capitalize()}\",\n",
    "                x=mmg_stats[mask]['mt_id'],\n",
    "                y=mmg_stats[mask]['count'],\n",
    "                marker_color=color,\n",
    "                text=mmg_stats[mask]['count'],\n",
    "                textposition='auto',\n",
    "                customdata=np.stack((\n",
    "                    mmg_stats[mask]['total_reads'],\n",
    "                    mmg_stats[mask]['mean_reads'],\n",
    "                    mmg_stats[mask]['samples']\n",
    "                ), axis=-1),\n",
    "                hovertemplate=\"<b>MMG ID: %{x}</b><br>\" +\n",
    "                             \"ASV Count: %{y}<br>\" +\n",
    "                             \"Total Reads: %{customdata[0]:,.0f}<br>\" +\n",
    "                             \"Mean Reads: %{customdata[1]:,.1f}<br>\" +\n",
    "                             \"Samples: %{customdata[2]}<extra></extra>\"\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'ASV-MMG Distribution by Model Decision',\n",
    "                'y': 0.95,\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(size=20)\n",
    "            },\n",
    "            barmode='stack',\n",
    "            xaxis_title=\"MMG ID\",\n",
    "            yaxis_title=\"Number of ASVs\",\n",
    "            height=700,\n",
    "            template='plotly_white',\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            )\n",
    "        )\n",
    "        plots.append(('asv_mmg_distribution', fig))\n",
    "\n",
    "        # 2. Enhanced Bubble Plot with Multiple Metrics\n",
    "        mmg_analysis = df_clean.groupby('mt_id').agg({\n",
    "            'reads': ['sum', 'mean'],\n",
    "            'model_decision': lambda x: (x == 'select').mean(),\n",
    "            'project_readfile_id': 'nunique',\n",
    "            'match': lambda x: (x == 'match').mean()\n",
    "        }).reset_index()\n",
    "        \n",
    "        mmg_analysis.columns = ['mt_id', 'total_reads', 'mean_reads', 'selection_rate', 'sample_count', 'match_rate']\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=mmg_analysis['selection_rate'] * 100,\n",
    "            y=mmg_analysis['match_rate'] * 100,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=np.sqrt(mmg_analysis['total_reads'])/100,\n",
    "                sizemode='area',\n",
    "                sizeref=2.*max(np.sqrt(mmg_analysis['total_reads'])/100)/(40.**2),\n",
    "                color=mmg_analysis['sample_count'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title='Number of Samples')\n",
    "            ),\n",
    "            text=mmg_analysis['mt_id'],\n",
    "            customdata=np.stack((\n",
    "                mmg_analysis['total_reads'],\n",
    "                mmg_analysis['mean_reads'],\n",
    "                mmg_analysis['sample_count']\n",
    "            ), axis=-1),\n",
    "            hovertemplate=\"<b>MMG ID: %{text}</b><br>\" +\n",
    "                         \"Selection Rate: %{x:.1f}%<br>\" +\n",
    "                         \"Match Rate: %{y:.1f}%<br>\" +\n",
    "                         \"Total Reads: %{customdata[0]:,.0f}<br>\" +\n",
    "                         \"Mean Reads: %{customdata[1]:,.1f}<br>\" +\n",
    "                         \"Samples: %{customdata[2]}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'ASV-MMG Selection vs Match Analysis',\n",
    "                'y': 0.95,\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(size=20)\n",
    "            },\n",
    "            xaxis_title=\"Selection Rate (%)\",\n",
    "            yaxis_title=\"Match Rate (%)\",\n",
    "            height=800,\n",
    "            width=1000,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        plots.append(('asv_mmg_bubble', fig))\n",
    "\n",
    "        # 3. Enhanced Read Distribution Violin Plot\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        for decision, color in zip(['select', 'unselect'], ['#2ecc71', '#e74c3c']):\n",
    "            mask = df_clean['model_decision'] == decision\n",
    "            \n",
    "            fig.add_trace(go.Violin(\n",
    "                x=df_clean[mask]['mt_id'],\n",
    "                y=df_clean[mask]['reads'],\n",
    "                name=decision.capitalize(),\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points='outliers',\n",
    "                side='positive' if decision == 'select' else 'negative',\n",
    "                line_color=color,\n",
    "                fillcolor=f'rgba{tuple(list(px.colors.hex_to_rgb(color)) + [0.3])}',\n",
    "                hovertemplate=\"MMG ID: %{x}<br>\" +\n",
    "                             \"Reads: %{y:,.0f}<br>\" +\n",
    "                             f\"Decision: {decision}<extra></extra>\"\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'Read Distribution by MMG and Decision',\n",
    "                'y': 0.95,\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(size=20)\n",
    "            },\n",
    "            xaxis_title=\"MMG ID\",\n",
    "            yaxis_title=\"Number of Reads\",\n",
    "            height=700,\n",
    "            yaxis_type='log',\n",
    "            template='plotly_white',\n",
    "            violingap=0,\n",
    "            violinmode='overlay'\n",
    "        )\n",
    "        plots.append(('mmg_read_distribution', fig))\n",
    "\n",
    "        # 4. Decision Pattern Heatmap\n",
    "        pattern_matrix = pd.pivot_table(\n",
    "            df_clean,\n",
    "            values='reads',\n",
    "            index='mt_id',\n",
    "            columns=['model_decision', 'match'],\n",
    "            aggfunc='sum',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        fig = go.Figure(go.Heatmap(\n",
    "            z=np.log10(pattern_matrix.values + 1),\n",
    "            x=[f\"{col[0]}_{col[1]}\" for col in pattern_matrix.columns],\n",
    "            y=pattern_matrix.index,\n",
    "            colorscale='RdYlBu',\n",
    "            text=pattern_matrix.values,\n",
    "            texttemplate='%{text:,.0f}',\n",
    "            hovertemplate=\"MMG ID: %{y}<br>\" +\n",
    "                         \"Category: %{x}<br>\" +\n",
    "                         \"Reads: %{text:,.0f}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'ASV-MMG Decision Pattern Matrix',\n",
    "                'y': 0.95,\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(size=20)\n",
    "            },\n",
    "            xaxis_title=\"Decision-Match Combination\",\n",
    "            yaxis_title=\"MMG ID\",\n",
    "            height=800,\n",
    "            width=1000,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        plots.append(('mmg_pattern_matrix', fig))\n",
    "\n",
    "        # 5. Sankey Diagram of ASV Flow\n",
    "        source = []\n",
    "        target = []\n",
    "        value = []\n",
    "        label = []\n",
    "        \n",
    "        # Create nodes for MMG IDs, match status, and decisions\n",
    "        mmg_ids = df_clean['mt_id'].unique()\n",
    "        match_status = ['match', 'no_match']\n",
    "        decisions = ['select', 'unselect']\n",
    "        \n",
    "        # Create node mappings\n",
    "        mmg_map = {mmg: i for i, mmg in enumerate(mmg_ids)}\n",
    "        match_map = {status: i + len(mmg_ids) for i, status in enumerate(match_status)}\n",
    "        decision_map = {dec: i + len(mmg_ids) + len(match_status) \n",
    "                       for i, dec in enumerate(decisions)}\n",
    "        \n",
    "        # Add all node labels\n",
    "        label.extend(mmg_ids)\n",
    "        label.extend(match_status)\n",
    "        label.extend(decisions)\n",
    "        \n",
    "        # Create links\n",
    "        for mmg in mmg_ids:\n",
    "            mmg_data = df_clean[df_clean['mt_id'] == mmg]\n",
    "            \n",
    "            for match in match_status:\n",
    "                match_data = mmg_data[mmg_data['match'] == match]\n",
    "                if len(match_data) > 0:\n",
    "                    source.append(mmg_map[mmg])\n",
    "                    target.append(match_map[match])\n",
    "                    value.append(len(match_data))\n",
    "                \n",
    "                for decision in decisions:\n",
    "                    decision_data = match_data[match_data['model_decision'] == decision]\n",
    "                    if len(decision_data) > 0:\n",
    "                        source.append(match_map[match])\n",
    "                        target.append(decision_map[decision])\n",
    "                        value.append(len(decision_data))\n",
    "        \n",
    "        fig = go.Figure(data=[go.Sankey(\n",
    "            node=dict(\n",
    "                pad=15,\n",
    "                thickness=20,\n",
    "                line=dict(color=\"black\", width=0.5),\n",
    "                label=label,\n",
    "                color=px.colors.qualitative.Set3[:len(label)]\n",
    "            ),\n",
    "            link=dict(\n",
    "                source=source,\n",
    "                target=target,\n",
    "                value=value\n",
    "            )\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'ASV Flow: MMG → Match Status → Model Decision',\n",
    "                'y': 0.95,\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(size=20)\n",
    "            },\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        plots.append(('mmg_sankey_flow', fig))\n",
    "\n",
    "        # 6. Time Series Analysis (if timestamp available)\n",
    "        if 'timestamp' in df_clean.columns:\n",
    "            df_clean['date'] = pd.to_datetime(df_clean['timestamp']).dt.date\n",
    "            time_analysis = df_clean.groupby(['date', 'mt_id', 'model_decision']).size().reset_index(name='count')\n",
    "            \n",
    "            fig = go.Figure()\n",
    "            \n",
    "            for decision, color in zip(['select', 'unselect'], ['#2ecc71', '#e74c3c']):\n",
    "                mask = time_analysis['model_decision'] == decision\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=time_analysis[mask]['date'],\n",
    "                    y=time_analysis[mask]['count'],\n",
    "                    name=decision.capitalize(),\n",
    "                    mode='lines+markers',\n",
    "                    line=dict(color=color),\n",
    "                    hovertemplate=\"Date: %{x}<br>\" +\n",
    "                                 \"Count: %{y}<br>\" +\n",
    "                                 f\"Decision: {decision}<extra></extra>\"\n",
    "                ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title={\n",
    "                    'text': 'ASV Selection Patterns Over Time',\n",
    "                    'y': 0.95,\n",
    "                    'x': 0.5,\n",
    "                    'xanchor': 'center',\n",
    "                    'yanchor': 'top',\n",
    "                    'font': dict(size=20)\n",
    "                },\n",
    "                xaxis_title=\"Date\",\n",
    "                yaxis_title=\"Number of ASVs\",\n",
    "                height=600,\n",
    "                template='plotly_white',\n",
    "                legend=dict(\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=1.02,\n",
    "                    xanchor=\"right\",\n",
    "                    x=1\n",
    "                )\n",
    "            )\n",
    "            plots.append(('mmg_time_series', fig))\n",
    "        \n",
    "        return plots\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating ASV-MMG plots: {str(e)}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return []\n",
    "\n",
    "def display_preprocessing_results(df: pd.DataFrame, feature_columns: List[str]) -> None:\n",
    "    \"\"\"Display preprocessing results in Jupyter notebook.\"\"\"\n",
    "    display(Markdown(\"## Data Preprocessing Results\"))\n",
    "    \n",
    "    # Display basic dataset info\n",
    "    display(Markdown(\"### Dataset Overview\"))\n",
    "    display(Markdown(f\"- Total records: {len(df):,}\"))\n",
    "    display(Markdown(f\"- Number of features: {len(feature_columns)}\"))\n",
    "    \n",
    "    # Display feature summary\n",
    "    display(Markdown(\"### Feature Summary\"))\n",
    "    feature_summary = df[feature_columns].describe()\n",
    "    display(feature_summary)\n",
    "    \n",
    "    # Display sample of preprocessed data\n",
    "    display(Markdown(\"### Sample of Preprocessed Data\"))\n",
    "    display(df.head())\n",
    "\n",
    "def display_model_training_results(model, accuracy: float, importance: pd.DataFrame, \n",
    "                                 X_test: pd.DataFrame, y_test: pd.Series) -> None:\n",
    "    \"\"\"Display model training results in Jupyter notebook.\"\"\"\n",
    "    display(Markdown(\"## Model Training Results\"))\n",
    "    \n",
    "    # Display model parameters\n",
    "    display(Markdown(\"### Model Parameters\"))\n",
    "    params = model.get_params()\n",
    "    for param, value in params.items():\n",
    "        display(Markdown(f\"- {param}: {value}\"))\n",
    "    \n",
    "    # Display accuracy metrics\n",
    "    display(Markdown(\"### Model Performance Metrics\"))\n",
    "    display(Markdown(f\"- Accuracy: {accuracy:.4f}\"))\n",
    "    \n",
    "    # Display feature importance\n",
    "    display(Markdown(\"### Feature Importance\"))\n",
    "    importance_sorted = importance.sort_values('importance', ascending=False)\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=importance_sorted['importance'],\n",
    "        y=importance_sorted['feature'],\n",
    "        orientation='h'\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title='Feature Importance',\n",
    "        xaxis_title='Importance Score',\n",
    "        yaxis_title='Feature',\n",
    "        height=400\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def display_threshold_optimization_results(threshold_results: Dict, optimal_threshold: float) -> None:\n",
    "    \"\"\"Display threshold optimization results in Jupyter notebook.\"\"\"\n",
    "    display(Markdown(\"## Threshold Optimization Results\"))\n",
    "    display(Markdown(f\"### Optimal Threshold: {optimal_threshold:.4f}\"))\n",
    "    \n",
    "    # Create performance curve\n",
    "    fig = go.Figure()\n",
    "    for search_type in ['coarse_search', 'medium_search', 'fine_search']:\n",
    "        df = pd.DataFrame(threshold_results[search_type])\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df['threshold'],\n",
    "            y=df['f1_score'],\n",
    "            name=search_type.replace('_', ' ').title(),\n",
    "            mode='lines+markers'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Threshold Optimization',\n",
    "        xaxis_title='Threshold',\n",
    "        yaxis_title='F1 Score',\n",
    "        height=400\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def display_prediction_results(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Display prediction results in Jupyter notebook.\"\"\"\n",
    "    display(Markdown(\"## Model Prediction Results\"))\n",
    "    \n",
    "    # Display summary statistics\n",
    "    total_samples = df['project_readfile_id'].nunique()\n",
    "    selected_asvs = (df['model_decision'] == 'select').sum()\n",
    "    agreement_rate = (df['agreement'] == 'agree').mean()\n",
    "    \n",
    "    display(Markdown(\"### Summary Statistics\"))\n",
    "    display(Markdown(f\"- Total samples processed: {total_samples:,}\"))\n",
    "    display(Markdown(f\"- Total ASVs selected: {selected_asvs:,}\"))\n",
    "    display(Markdown(f\"- Agreement rate: {agreement_rate:.4f}\"))\n",
    "    \n",
    "    # Create agreement distribution plot\n",
    "    agreement_counts = df['agreement'].value_counts()\n",
    "    fig = go.Figure(data=[go.Pie(\n",
    "        labels=agreement_counts.index,\n",
    "        values=agreement_counts.values,\n",
    "        hole=0.4\n",
    "    )])\n",
    "    fig.update_layout(\n",
    "        title='Model-Expert Agreement Distribution',\n",
    "        height=400\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def display_taxonomic_results(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Display taxonomic analysis results in Jupyter notebook.\"\"\"\n",
    "    if 'taxonomy' not in df.columns:\n",
    "        display(Markdown(\"## Taxonomic Analysis not available - No taxonomy column found\"))\n",
    "        return\n",
    "        \n",
    "    display(Markdown(\"## Taxonomic Analysis Results\"))\n",
    "    \n",
    "    # Calculate taxonomic distribution\n",
    "    tax_dist = df.groupby(['taxonomy', 'model_decision']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Display distribution plot\n",
    "    fig = go.Figure()\n",
    "    for decision in ['select', 'unselect']:\n",
    "        if decision in tax_dist.columns:\n",
    "            fig.add_trace(go.Bar(\n",
    "                name=decision.capitalize(),\n",
    "                x=tax_dist.index,\n",
    "                y=tax_dist[decision]\n",
    "            ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Taxonomic Distribution by Decision',\n",
    "        xaxis_title='Taxonomy',\n",
    "        yaxis_title='Count',\n",
    "        barmode='group',\n",
    "        height=500,\n",
    "        xaxis_tickangle=45\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Cell 11: Report Generation Functions\n",
    "\n",
    "def generate_enhanced_html_report(results_df: pd.DataFrame, model_metrics: Dict, \n",
    "                                plots: Dict[str, List[Tuple[str, go.Figure]]], \n",
    "                                OUTPUT_DIR: Path) -> Path:\n",
    "    \"\"\"Generate comprehensive HTML report with all analyses.\"\"\"\n",
    "    try:\n",
    "        report_dir = OUTPUT_DIR / 'reports'\n",
    "        report_dir.mkdir(parents=True, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Save plots to separate files\n",
    "        plot_refs = {}\n",
    "        plots_dir = report_dir / 'plots'\n",
    "        plots_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for section_name, section_plots in plots.items():\n",
    "            section_refs = []\n",
    "            for plot_title, fig in section_plots:\n",
    "                plot_filename = f\"{section_name}_{plot_title.lower().replace(' ', '_')}.html\"\n",
    "                plot_path = plots_dir / plot_filename\n",
    "                \n",
    "                # Save plot with minimal HTML wrapper\n",
    "                fig.write_html(\n",
    "                    plot_path,\n",
    "                    include_plotlyjs='cdn',\n",
    "                    full_html=False,\n",
    "                    config={'responsive': True}\n",
    "                )\n",
    "                section_refs.append((plot_title, plot_filename))\n",
    "            plot_refs[section_name] = section_refs\n",
    "\n",
    "        # Define sections\n",
    "        sections = {\n",
    "            'overview': {\n",
    "                'title': 'Overview',\n",
    "                'description': 'Summary of model performance and key metrics',\n",
    "                'icon': 'fas fa-chart-line',\n",
    "                'order': 1\n",
    "            },\n",
    "            'model_performance': {\n",
    "                'title': 'Model Performance',\n",
    "                'description': 'Analysis of model performance including ROC curve, feature importance, and confusion matrix',\n",
    "                'icon': 'fas fa-brain',\n",
    "                'order': 2\n",
    "            },\n",
    "            'correlation_analysis': {\n",
    "                'title': 'Correlation Analysis',\n",
    "                'description': 'Feature correlation patterns and relationships',\n",
    "                'icon': 'fas fa-project-diagram',\n",
    "                'order': 3\n",
    "            },\n",
    "            'threshold_analysis': {\n",
    "                'title': 'Threshold Analysis',\n",
    "                'description': 'Selection threshold optimization analysis',\n",
    "                'icon': 'fas fa-sliders-h',\n",
    "                'order': 4\n",
    "            },\n",
    "            'feature_analysis': {\n",
    "                'title': 'Feature Analysis',\n",
    "                'description': 'Detailed analysis of individual features',\n",
    "                'icon': 'fas fa-columns',\n",
    "                'order': 5\n",
    "            },\n",
    "            'pca_analysis': {\n",
    "                'title': 'PCA Analysis',\n",
    "                'description': 'Principal Component Analysis visualization',\n",
    "                'icon': 'fas fa-cube',\n",
    "                'order': 6\n",
    "            },\n",
    "            'agreement_analysis': {\n",
    "                'title': 'Agreement Analysis',\n",
    "                'description': 'Analysis of model-expert agreement patterns and distributions',\n",
    "                'icon': 'fas fa-handshake',\n",
    "                'order': 7\n",
    "            },\n",
    "            'taxonomic_analysis': {\n",
    "                'title': 'Taxonomic Analysis',\n",
    "                'description': 'Analysis of taxonomic patterns and distributions',\n",
    "                'icon': 'fas fa-sitemap',\n",
    "                'order': 8\n",
    "            },\n",
    "            'asv_mmg_analysis': {\n",
    "                'title': 'ASV-MMG Analysis',\n",
    "                'description': 'Analysis of ASV-MMG relationships and patterns',\n",
    "                'icon': 'fas fa-dna',\n",
    "                'order': 9\n",
    "            },\n",
    "            'summary': {\n",
    "                'title': 'Summary',\n",
    "                'description': 'Overall findings and recommendations',\n",
    "                'icon': 'fas fa-clipboard-check',\n",
    "                'order': 10\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Generate HTML content\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\"en\">\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "            <title>ASV Selection Analysis Report</title>\n",
    "            <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "            <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n",
    "            <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "            <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "            <style>\n",
    "                body {{\n",
    "                    margin: 0;\n",
    "                    padding: 0;\n",
    "                    background-color: #f8f9fa;\n",
    "                }}\n",
    "                \n",
    "                .sidebar {{\n",
    "                    position: fixed;\n",
    "                    top: 0;\n",
    "                    left: 0;\n",
    "                    width: 280px;\n",
    "                    height: 100vh;\n",
    "                    background: #2c3e50;\n",
    "                    padding: 20px 0;\n",
    "                    z-index: 1000;\n",
    "                    overflow-y: auto;\n",
    "                    color: white;\n",
    "                }}\n",
    "                \n",
    "                .content-wrapper {{\n",
    "                    margin-left: 280px;\n",
    "                    padding: 20px 40px;\n",
    "                    min-height: 100vh;\n",
    "                }}\n",
    "                \n",
    "                .nav-link {{\n",
    "                    color: rgba(255,255,255,0.8);\n",
    "                    margin: 5px 15px;\n",
    "                    padding: 10px 15px;\n",
    "                    border-radius: 5px;\n",
    "                    transition: all 0.3s;\n",
    "                    display: flex;\n",
    "                    align-items: center;\n",
    "                }}\n",
    "                \n",
    "                .nav-link i {{\n",
    "                    margin-right: 10px;\n",
    "                    width: 20px;\n",
    "                    text-align: center;\n",
    "                }}\n",
    "                \n",
    "                .nav-link:hover,\n",
    "                .nav-link.active {{\n",
    "                    background: #3498db;\n",
    "                    color: white;\n",
    "                    transform: translateX(5px);\n",
    "                }}\n",
    "                \n",
    "                .section {{\n",
    "                    background: white;\n",
    "                    margin: 30px 0;\n",
    "                    padding: 30px;\n",
    "                    border-radius: 15px;\n",
    "                    box-shadow: 0 2px 4px rgba(0,0,0,.05);\n",
    "                }}\n",
    "                \n",
    "                .section-header {{\n",
    "                    margin-bottom: 25px;\n",
    "                    padding-bottom: 15px;\n",
    "                    border-bottom: 2px solid #3498db;\n",
    "                }}\n",
    "                \n",
    "                .metric-card {{\n",
    "                    background: linear-gradient(135deg,#2c3e50,#3498db);\n",
    "                    color: white;\n",
    "                    padding: 25px;\n",
    "                    border-radius: 15px;\n",
    "                    text-align: center;\n",
    "                    transition: transform 0.3s;\n",
    "                    height: 100%;\n",
    "                }}\n",
    "                \n",
    "                .metric-card:hover {{\n",
    "                    transform: translateY(-5px);\n",
    "                }}\n",
    "                \n",
    "                .plot-container {{\n",
    "                    width: 100%;\n",
    "                    margin: 25px 0;\n",
    "                    background: white;\n",
    "                    border-radius: 10px;\n",
    "                    box-shadow: 0 2px 4px rgba(0,0,0,.05);\n",
    "                    overflow: visible;\n",
    "                }}\n",
    "                \n",
    "                .plot-frame {{\n",
    "                    width: 100%;\n",
    "                    min-height: 1000px;\n",
    "                    border: none;\n",
    "                    overflow: visible;\n",
    "                }}\n",
    "                \n",
    "                .summary-section {{\n",
    "                    background: #f8f9fa;\n",
    "                    padding: 20px;\n",
    "                    border-radius: 10px;\n",
    "                    margin-top: 20px;\n",
    "                }}\n",
    "                \n",
    "                .summary-card {{\n",
    "                    background: white;\n",
    "                    padding: 20px;\n",
    "                    border-radius: 10px;\n",
    "                    margin-bottom: 20px;\n",
    "                    box-shadow: 0 2px 4px rgba(0,0,0,.05);\n",
    "                }}\n",
    "                \n",
    "                @media print {{\n",
    "                    .sidebar {{\n",
    "                        display: none;\n",
    "                    }}\n",
    "                    .content-wrapper {{\n",
    "                        margin-left: 0;\n",
    "                    }}\n",
    "                    .section {{\n",
    "                        break-inside: avoid;\n",
    "                    }}\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "        \"\"\"\n",
    "\n",
    "        # Add navigation\n",
    "        nav_html = \"\"\"\n",
    "        <div class=\"sidebar\">\n",
    "            <div class=\"nav flex-column\">\n",
    "        \"\"\"\n",
    "        \n",
    "        for section_id, info in sorted(sections.items(), key=lambda x: x[1]['order']):\n",
    "            nav_html += f\"\"\"\n",
    "                <a class=\"nav-link\" href=\"#{section_id}\">\n",
    "                    <i class=\"{info['icon']}\"></i>{info['title']}\n",
    "                </a>\n",
    "            \"\"\"\n",
    "        \n",
    "        nav_html += \"\"\"\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        html_content += nav_html + '<div class=\"content-wrapper\">'\n",
    "\n",
    "        # Add content sections\n",
    "        for section_id, info in sorted(sections.items(), key=lambda x: x[1]['order']):\n",
    "            if section_id == 'overview':\n",
    "                # Overview section\n",
    "                html_content += f\"\"\"\n",
    "                <div id=\"{section_id}\" class=\"section\">\n",
    "                    <div class=\"section-header\">\n",
    "                        <h2><i class=\"{info['icon']} me-2\"></i>{info['title']}</h2>\n",
    "                        <p class=\"text-muted\">{info['description']}</p>\n",
    "                        <small class=\"text-muted\">Generated on: {timestamp}</small>\n",
    "                    </div>\n",
    "                    <div class=\"row g-4\">\n",
    "                        <div class=\"col-md-3\">\n",
    "                            <div class=\"metric-card\">\n",
    "                                <i class=\"fas fa-bullseye fa-2x mb-3\"></i>\n",
    "                                <div class=\"h2\">{model_metrics.get('accuracy', 0):.2%}</div>\n",
    "                                <div>Model Accuracy</div>\n",
    "                            </div>\n",
    "                        </div>\n",
    "                        <div class=\"col-md-3\">\n",
    "                            <div class=\"metric-card\">\n",
    "                                <i class=\"fas fa-check-circle fa-2x mb-3\"></i>\n",
    "                                <div class=\"h2\">{(results_df['agreement'] == 'agree').mean():.2%}</div>\n",
    "                                <div>Agreement Rate</div>\n",
    "                            </div>\n",
    "                        </div>\n",
    "                        <div class=\"col-md-3\">\n",
    "                            <div class=\"metric-card\">\n",
    "                                <i class=\"fas fa-database fa-2x mb-3\"></i>\n",
    "                                <div class=\"h2\">{len(results_df):,}</div>\n",
    "                                <div>Total ASVs</div>\n",
    "                            </div>\n",
    "                        </div>\n",
    "                        <div class=\"col-md-3\">\n",
    "                            <div class=\"metric-card\">\n",
    "                                <i class=\"fas fa-check fa-2x mb-3\"></i>\n",
    "                                <div class=\"h2\">{(results_df['model_decision'] == 'select').sum():,}</div>\n",
    "                                <div>Selected ASVs</div>\n",
    "                            </div>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            elif section_id == 'summary':\n",
    "                # Summary section\n",
    "                html_content += f\"\"\"\n",
    "                <div id=\"{section_id}\" class=\"section\">\n",
    "                    <div class=\"section-header\">\n",
    "                        <h2><i class=\"{info['icon']} me-2\"></i>{info['title']}</h2>\n",
    "                        <p class=\"text-muted\">{info['description']}</p>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"summary-section\">\n",
    "                        <div class=\"summary-card\">\n",
    "                            <h4>Key Findings</h4>\n",
    "                            <ul>\n",
    "                                <li>Model achieved {model_metrics.get('accuracy', 0):.2%} accuracy</li>\n",
    "                                <li>{(results_df['agreement'] == 'agree').mean():.2%} agreement rate with expert decisions</li>\n",
    "                                <li>Selected {(results_df['model_decision'] == 'select').sum():,} ASVs from {len(results_df):,} total ASVs</li>\n",
    "                                <li>Optimal selection threshold determined at {model_metrics.get('optimal_threshold', 0):.3f}</li>\n",
    "                                <li>Perfect agreement between model predictions and expert decisions</li>\n",
    "                            </ul>\n",
    "                        </div>\n",
    "                        \n",
    "                        <div class=\"summary-card\">\n",
    "                            <h4>Model Performance</h4>\n",
    "                            <ul>\n",
    "                                <li>Random Forest model with optimal parameters</li>\n",
    "                                <li>Cross-validation score: {model_metrics.get('accuracy', 0):.2%}</li>\n",
    "                                <li>Balanced handling of select/unselect cases</li>\n",
    "                                <li>Robust feature importance analysis</li>\n",
    "                            </ul>\n",
    "                        </div>\n",
    "                        \n",
    "                        <div class=\"summary-card\">\n",
    "                            <h4>Recommendations</h4>\n",
    "                            <ul>\n",
    "                                <li>Monitor model performance with new data</li>\n",
    "                                <li>Validate results with domain experts</li>\n",
    "                                <li>Consider periodic model retraining as new data becomes available</li>\n",
    "                                <li>Maintain threshold optimization for different datasets</li>\n",
    "                                <li>Keep track of taxonomic and MMG patterns for validation</li>\n",
    "                            </ul>\n",
    "                        </div>\n",
    "                        \n",
    "                        <div class=\"summary-card\">\n",
    "                            <h4>Future Improvements</h4>\n",
    "                            <ul>\n",
    "                                <li>Expand feature set for more robust predictions</li>\n",
    "                                <li>Implement continuous validation pipeline</li>\n",
    "                                <li>Enhance taxonomic analysis capabilities</li>\n",
    "                                <li>Develop more advanced MMG matching algorithms</li>\n",
    "                                <li>Create automated reporting system for regular updates</li>\n",
    "                            </ul>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            else:\n",
    "                # Other sections with plots\n",
    "                if section_id in plot_refs:\n",
    "                    plot_html = []\n",
    "                    for plot_title, plot_file in plot_refs[section_id]:\n",
    "                        plot_html.append(f\"\"\"\n",
    "                            <div class=\"plot-container\">\n",
    "                                <h4 class=\"ps-3 pt-3\">{plot_title}</h4>\n",
    "                                <iframe class=\"plot-frame\"\n",
    "                                        src=\"plots/{plot_file}\"\n",
    "                                        loading=\"lazy\"\n",
    "                                        onload=\"this.style.height = Math.max(1000, this.contentWindow.document.body.scrollHeight + 50) + 'px'\">\n",
    "                                </iframe>\n",
    "                            </div>\n",
    "                        \"\"\")\n",
    "                    \n",
    "                    if plot_html:\n",
    "                        html_content += f\"\"\"\n",
    "                        <div id=\"{section_id}\" class=\"section\">\n",
    "                            <div class=\"section-header\">\n",
    "                                <h2><i class=\"{info['icon']} me-2\"></i>{info['title']}</h2>\n",
    "                                <p class=\"text-muted\">{info['description']}</p>\n",
    "                            </div>\n",
    "                            {''.join(plot_html)}\n",
    "                        </div>\n",
    "                        \"\"\"\n",
    "\n",
    "        # Add scripts and close HTML\n",
    "        html_content += \"\"\"\n",
    "            </div>\n",
    "            <script>\n",
    "                // Resize iframes\n",
    "                function resizeIframe(iframe) {\n",
    "                    iframe.style.height = 'auto';\n",
    "                    const newHeight = Math.max(1000, iframe.contentWindow.document.body.scrollHeight + 50);\n",
    "                    iframe.style.height = newHeight + 'px';\n",
    "                }\n",
    "\n",
    "                // Handle document load\n",
    "                document.addEventListener('DOMContentLoaded', function() {\n",
    "                    // Handle iframes\n",
    "                    const iframes = document.querySelectorAll('.plot-frame');\n",
    "                    iframes.forEach(iframe => {\n",
    "                        iframe.onload = function() {\n",
    "                            resizeIframe(this);\n",
    "                        };\n",
    "                    });\n",
    "\n",
    "                    // Handle navigation\n",
    "                    const navLinks = document.querySelectorAll('.nav-link');\n",
    "                    const sections = document.querySelectorAll('.section');\n",
    "                    \n",
    "                    // Initialize tooltips\n",
    "                    const tooltipTriggerList = [].slice.call(document.querySelectorAll('[data-bs-toggle=\"tooltip\"]'));\n",
    "                    tooltipTriggerList.map(function (tooltipTriggerEl) {\n",
    "                        return new bootstrap.Tooltip(tooltipTriggerEl);\n",
    "                    });\n",
    "                    \n",
    "                    // Smooth scrolling for navigation links\n",
    "                    navLinks.forEach(link => {\n",
    "                        link.addEventListener('click', function(e) {\n",
    "                            e.preventDefault();\n",
    "                            const targetId = this.getAttribute('href');\n",
    "                            const targetSection = document.querySelector(targetId);\n",
    "                            if (targetSection) {\n",
    "                                targetSection.scrollIntoView({\n",
    "                                    behavior: 'smooth'\n",
    "                                });\n",
    "                            }\n",
    "                        });\n",
    "                    });\n",
    "\n",
    "                    // Active navigation highlighting\n",
    "                    window.addEventListener('scroll', () => {\n",
    "                        let current = '';\n",
    "                        sections.forEach(section => {\n",
    "                            const sectionTop = section.offsetTop - 100;\n",
    "                            const sectionHeight = section.offsetHeight;\n",
    "                            \n",
    "                            if (window.scrollY >= sectionTop && \n",
    "                                window.scrollY < sectionTop + sectionHeight) {\n",
    "                                current = section.getAttribute('id');\n",
    "                            }\n",
    "                        });\n",
    "                        \n",
    "                        navLinks.forEach(link => {\n",
    "                            link.classList.remove('active');\n",
    "                            if (link.getAttribute('href').slice(1) === current) {\n",
    "                                link.classList.add('active');\n",
    "                            }\n",
    "                        });\n",
    "                    });\n",
    "\n",
    "                    // Add scroll to top button functionality\n",
    "                    const scrollButton = document.createElement('button');\n",
    "                    scrollButton.innerHTML = '<i class=\"fas fa-arrow-up\"></i>';\n",
    "                    scrollButton.className = 'btn btn-primary position-fixed';\n",
    "                    scrollButton.style.cssText = `\n",
    "                        bottom: 20px;\n",
    "                        right: 20px;\n",
    "                        display: none;\n",
    "                        z-index: 1000;\n",
    "                        width: 40px;\n",
    "                        height: 40px;\n",
    "                        border-radius: 20px;\n",
    "                        padding: 0;\n",
    "                        box-shadow: 0 2px 5px rgba(0,0,0,0.2);\n",
    "                    `;\n",
    "                    document.body.appendChild(scrollButton);\n",
    "\n",
    "                    window.onscroll = function() {\n",
    "                        if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {\n",
    "                            scrollButton.style.display = \"block\";\n",
    "                        } else {\n",
    "                            scrollButton.style.display = \"none\";\n",
    "                        }\n",
    "                    };\n",
    "\n",
    "                    scrollButton.onclick = function() {\n",
    "                        window.scrollTo({\n",
    "                            top: 0,\n",
    "                            behavior: 'smooth'\n",
    "                        });\n",
    "                    };\n",
    "                });\n",
    "\n",
    "                // Handle window resize\n",
    "                window.addEventListener('resize', function() {\n",
    "                    const iframes = document.querySelectorAll('.plot-frame');\n",
    "                    iframes.forEach(iframe => {\n",
    "                        resizeIframe(iframe);\n",
    "                    });\n",
    "                });\n",
    "\n",
    "                // Print optimization\n",
    "                window.addEventListener('beforeprint', function() {\n",
    "                    document.body.style.paddingLeft = '0';\n",
    "                });\n",
    "                \n",
    "                window.addEventListener('afterprint', function() {\n",
    "                    document.body.style.paddingLeft = '280px';\n",
    "                });\n",
    "\n",
    "                // Plot responsiveness\n",
    "                window.addEventListener('resize', function() {\n",
    "                    document.querySelectorAll('.js-plotly-plot').forEach(plot => {\n",
    "                        if (plot && plot.id) {\n",
    "                            Plotly.Plots.resize(plot.id);\n",
    "                        }\n",
    "                    });\n",
    "                });\n",
    "            </script>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "\n",
    "        # Save report\n",
    "        report_path = report_dir / 'analysis_report.html'\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "            \n",
    "        logging.info(f\"Generated enhanced HTML report: {report_path}\")\n",
    "        return report_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating HTML report: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_navigation_html(sections: Dict) -> str:\n",
    "    \"\"\"Create navigation HTML with sections.\"\"\"\n",
    "    nav_html = \"\"\"\n",
    "    <div class=\"sidebar\">\n",
    "        <div class=\"sidebar-header\">\n",
    "            <h4>Analysis Navigation</h4>\n",
    "            <p class=\"small\">Click section to navigate</p>\n",
    "        </div>\n",
    "        <div class=\"sidebar-content\">\n",
    "            <div class=\"nav flex-column nav-pills\">\n",
    "    \"\"\"\n",
    "    \n",
    "    for section_id, section_info in sections.items():\n",
    "        nav_html += f\"\"\"\n",
    "                <a class=\"nav-link\" href=\"#{section_id}\" \n",
    "                   data-bs-toggle=\"tooltip\" \n",
    "                   data-bs-placement=\"right\"\n",
    "                   title=\"{section_info['description']}\">\n",
    "                    <i class=\"{section_info['icon']}\"></i>\n",
    "                    {section_info['title']}\n",
    "                </a>\n",
    "        \"\"\"\n",
    "    \n",
    "    nav_html += \"\"\"\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return nav_html\n",
    "\n",
    "def create_overview_section(section_info: Dict, results_df: pd.DataFrame, model_metrics: Dict, timestamp: str) -> str:\n",
    "    \"\"\"Create overview section HTML.\"\"\"\n",
    "    return f\"\"\"\n",
    "        <div id=\"overview\" class=\"section\">\n",
    "            <div class=\"section-header\">\n",
    "                <div class=\"section-title\">\n",
    "                    <i class=\"{section_info['icon']} fa-2x\"></i>\n",
    "                    <h2>{section_info['title']}</h2>\n",
    "                </div>\n",
    "                <p class=\"section-description\">{section_info['description']}</p>\n",
    "                <small class=\"text-muted\">Generated on: {timestamp}</small>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"row\">\n",
    "                <div class=\"col-md-3\">\n",
    "                    <div class=\"metric-card\">\n",
    "                        <i class=\"fas fa-bullseye metric-icon\"></i>\n",
    "                        <div class=\"metric-value\">{model_metrics.get('accuracy', 0):.2%}</div>\n",
    "                        <div class=\"metric-label\">Model Accuracy</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div class=\"col-md-3\">\n",
    "                    <div class=\"metric-card\">\n",
    "                        <i class=\"fas fa-check-circle metric-icon\"></i>\n",
    "                        <div class=\"metric-value\">{(results_df['agreement'] == 'agree').mean():.2%}</div>\n",
    "                        <div class=\"metric-label\">Agreement Rate</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div class=\"col-md-3\">\n",
    "                    <div class=\"metric-card\">\n",
    "                        <i class=\"fas fa-database metric-icon\"></i>\n",
    "                        <div class=\"metric-value\">{len(results_df):,}</div>\n",
    "                        <div class=\"metric-label\">Total ASVs</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div class=\"col-md-3\">\n",
    "                    <div class=\"metric-card\">\n",
    "                        <i class=\"fas fa-check metric-icon\"></i>\n",
    "                        <div class=\"metric-value\">{(results_df['model_decision'] == 'select').sum():,}</div>\n",
    "                        <div class=\"metric-label\">Selected ASVs</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    \"\"\"\n",
    "\n",
    "def create_section_html(section_id: str, section_info: Dict, section_plots: List[Tuple[str, go.Figure]]) -> str:\n",
    "    \"\"\"Create section HTML with plots.\"\"\"\n",
    "    section_html = f\"\"\"\n",
    "        <div id=\"{section_id}\" class=\"section\">\n",
    "            <div class=\"section-header\">\n",
    "                <div class=\"section-title\">\n",
    "                    <i class=\"{section_info['icon']} fa-2x\"></i>\n",
    "                    <h2>{section_info['title']}</h2>\n",
    "                </div>\n",
    "                <p class=\"section-description\">{section_info['description']}</p>\n",
    "            </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    for plot_title, fig in section_plots:\n",
    "        plot_div = fig.to_html(\n",
    "            full_html=False,\n",
    "            include_plotlyjs=False,\n",
    "            config={'responsive': True}\n",
    "        )\n",
    "        section_html += f\"\"\"\n",
    "            <div class=\"plot-container\">\n",
    "                <h4>{plot_title}</h4>\n",
    "                {plot_div}\n",
    "            </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    section_html += \"</div>\"\n",
    "    return section_html\n",
    "\n",
    "def create_summary_section(section_info: Dict, results_df: pd.DataFrame, model_metrics: Dict) -> str:\n",
    "    \"\"\"Create summary section HTML.\"\"\"\n",
    "    return f\"\"\"\n",
    "        <div id=\"summary\" class=\"section\">\n",
    "            <div class=\"section-header\">\n",
    "                <div class=\"section-title\">\n",
    "                    <i class=\"{section_info['icon']} fa-2x\"></i>\n",
    "                    <h2>{section_info['title']}</h2>\n",
    "                </div>\n",
    "                <p class=\"section-description\">{section_info['description']}</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"summary-content\">\n",
    "                <h3>Key Findings</h3>\n",
    "                <ul>\n",
    "                    <li>Model achieved {model_metrics.get('accuracy', 0):.2%} accuracy</li>\n",
    "                    <li>{(results_df['agreement'] == 'agree').mean():.2%} agreement rate with expert decisions</li>\n",
    "                    <li>Selected {(results_df['model_decision'] == 'select').sum():,} ASVs from {len(results_df):,} total ASVs</li>\n",
    "                    <li>Optimal threshold determined at {model_metrics.get('optimal_threshold', 0):.3f}</li>\n",
    "                </ul>\n",
    "                \n",
    "                <h3>Recommendations</h3>\n",
    "                <ul>\n",
    "                    <li>Continue monitoring model performance with new data</li>\n",
    "                    <li>Validate results with domain experts</li>\n",
    "                    <li>Consider periodic model retraining as new data becomes available</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </div>\n",
    "    \"\"\"\n",
    "\n",
    "def create_html_header() -> str:\n",
    "    \"\"\"Create HTML header with CSS styles.\"\"\"\n",
    "    css_styles = \"\"\"\n",
    "        <style>\n",
    "            :root {\n",
    "                --primary-color: #2c3e50;\n",
    "                --secondary-color: #34495e;\n",
    "                --accent-color: #3498db;\n",
    "                --background-color: #f8f9fa;\n",
    "                --text-color: #2c3e50;\n",
    "            }\n",
    "            \n",
    "            body {\n",
    "                padding-left: 280px;\n",
    "                background-color: var(--background-color);\n",
    "                color: var(--text-color);\n",
    "                font-family: 'Segoe UI', Arial, sans-serif;\n",
    "            }\n",
    "            \n",
    "            .sidebar {\n",
    "                height: 100%;\n",
    "                width: 280px;\n",
    "                position: fixed;\n",
    "                z-index: 1;\n",
    "                top: 0;\n",
    "                left: 0;\n",
    "                background-color: var(--primary-color);\n",
    "                overflow-x: hidden;\n",
    "                padding: 20px 0;\n",
    "                color: white;\n",
    "                box-shadow: 2px 0 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            \n",
    "            .sidebar-header {\n",
    "                padding: 20px;\n",
    "                text-align: center;\n",
    "                border-bottom: 1px solid rgba(255,255,255,0.1);\n",
    "                margin-bottom: 20px;\n",
    "            }\n",
    "            \n",
    "            .nav-link {\n",
    "                color: rgba(255,255,255,0.8) !important;\n",
    "                padding: 12px 20px !important;\n",
    "                margin: 5px 15px;\n",
    "                border-radius: 5px;\n",
    "                transition: all 0.3s;\n",
    "                display: flex !important;\n",
    "                align-items: center;\n",
    "            }\n",
    "            \n",
    "            .nav-link i {\n",
    "                margin-right: 10px;\n",
    "                width: 20px;\n",
    "                text-align: center;\n",
    "            }\n",
    "            \n",
    "            .nav-link:hover {\n",
    "                background-color: var(--accent-color);\n",
    "                color: white !important;\n",
    "                transform: translateX(5px);\n",
    "            }\n",
    "            \n",
    "            .nav-link.active {\n",
    "                background-color: var(--accent-color) !important;\n",
    "                color: white !important;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.2);\n",
    "            }\n",
    "            \n",
    "            .section {\n",
    "                background: white;\n",
    "                margin: 30px;\n",
    "                padding: 30px;\n",
    "                border-radius: 15px;\n",
    "                box-shadow: 0 4px 6px rgba(0,0,0,0.05);\n",
    "                transition: transform 0.3s ease-in-out;\n",
    "            }\n",
    "            \n",
    "            .section:hover {\n",
    "                transform: translateY(-5px);\n",
    "                box-shadow: 0 6px 12px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            \n",
    "            .section-header {\n",
    "                border-bottom: 2px solid var(--accent-color);\n",
    "                padding-bottom: 15px;\n",
    "                margin-bottom: 25px;\n",
    "            }\n",
    "            \n",
    "            .section-title {\n",
    "                display: flex;\n",
    "                align-items: center;\n",
    "                gap: 10px;\n",
    "                color: var(--primary-color);\n",
    "            }\n",
    "            \n",
    "            .section-title i {\n",
    "                color: var(--accent-color);\n",
    "            }\n",
    "            \n",
    "            .metric-card {\n",
    "                background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));\n",
    "                color: white;\n",
    "                border-radius: 15px;\n",
    "                padding: 25px;\n",
    "                margin: 10px;\n",
    "                text-align: center;\n",
    "                transition: all 0.3s;\n",
    "                box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            \n",
    "            .metric-card:hover {\n",
    "                transform: translateY(-5px) scale(1.02);\n",
    "                box-shadow: 0 6px 12px rgba(0,0,0,0.15);\n",
    "            }\n",
    "            \n",
    "            .metric-icon {\n",
    "                font-size: 2.5em;\n",
    "                margin-bottom: 15px;\n",
    "                color: rgba(255,255,255,0.9);\n",
    "            }\n",
    "            \n",
    "            .metric-value {\n",
    "                font-size: 2.2em;\n",
    "                font-weight: bold;\n",
    "                margin: 10px 0;\n",
    "            }\n",
    "            \n",
    "            .metric-label {\n",
    "                font-size: 1.1em;\n",
    "                opacity: 0.9;\n",
    "            }\n",
    "            \n",
    "            .plot-container {\n",
    "                margin: 25px 0;\n",
    "                padding: 20px;\n",
    "                border: 1px solid #eee;\n",
    "                border-radius: 10px;\n",
    "                background: white;\n",
    "                transition: all 0.3s;\n",
    "            }\n",
    "            \n",
    "            .plot-container:hover {\n",
    "                box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            \n",
    "            .section-description {\n",
    "                color: #666;\n",
    "                font-style: italic;\n",
    "                margin: 15px 0;\n",
    "                padding: 10px;\n",
    "                background: #f8f9fa;\n",
    "                border-radius: 5px;\n",
    "                border-left: 4px solid var(--accent-color);\n",
    "            }\n",
    "            \n",
    "            .scroll-to-top {\n",
    "                position: fixed;\n",
    "                bottom: 30px;\n",
    "                right: 30px;\n",
    "                width: 50px;\n",
    "                height: 50px;\n",
    "                border-radius: 25px;\n",
    "                background-color: var(--accent-color);\n",
    "                color: white;\n",
    "                border: none;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.2);\n",
    "                display: none;\n",
    "                z-index: 1000;\n",
    "                transition: all 0.3s;\n",
    "            }\n",
    "            \n",
    "            .scroll-to-top:hover {\n",
    "                transform: translateY(-3px);\n",
    "                box-shadow: 0 4px 8px rgba(0,0,0,0.3);\n",
    "            }\n",
    "            \n",
    "            @media print {\n",
    "                body {\n",
    "                    padding-left: 0;\n",
    "                }\n",
    "                .sidebar, .scroll-to-top {\n",
    "                    display: none;\n",
    "                }\n",
    "                .section {\n",
    "                    margin: 15px 0;\n",
    "                    padding: 15px;\n",
    "                    break-inside: avoid;\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            @media (max-width: 768px) {\n",
    "                body {\n",
    "                    padding-left: 0;\n",
    "                }\n",
    "                .sidebar {\n",
    "                    transform: translateX(-100%);\n",
    "                }\n",
    "                .section {\n",
    "                    margin: 15px;\n",
    "                }\n",
    "            }\n",
    "        </style>\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\"en\">\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "            <title>ASV Selection Analysis Report</title>\n",
    "            \n",
    "            <!-- External Dependencies -->\n",
    "            <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "            <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "            <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "            <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n",
    "            {css_styles}\n",
    "        </head>\n",
    "    \"\"\"\n",
    "\n",
    "def create_footer_html() -> str:\n",
    "    \"\"\"Create footer HTML with scripts and closing tags.\"\"\"\n",
    "    return \"\"\"\n",
    "                <button class=\"btn scroll-to-top\" id=\"scrollToTop\">\n",
    "                    <i class=\"fas fa-arrow-up\"></i>\n",
    "                </button>\n",
    "            </div>\n",
    "            \n",
    "            <script>\n",
    "                // Initialize tooltips\n",
    "                var tooltipTriggerList = [].slice.call(document.querySelectorAll('[data-bs-toggle=\"tooltip\"]'))\n",
    "                var tooltipList = tooltipTriggerList.map(function (tooltipTriggerEl) {\n",
    "                    return new bootstrap.Tooltip(tooltipTriggerEl)\n",
    "                });\n",
    "                \n",
    "                // Scroll to top functionality\n",
    "                const scrollToTopBtn = document.getElementById(\"scrollToTop\");\n",
    "                \n",
    "                window.onscroll = function() {\n",
    "                    if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {\n",
    "                        scrollToTopBtn.style.display = \"block\";\n",
    "                    } else {\n",
    "                        scrollToTopBtn.style.display = \"none\";\n",
    "                    }\n",
    "                };\n",
    "                \n",
    "                scrollToTopBtn.addEventListener(\"click\", function() {\n",
    "                    window.scrollTo({\n",
    "                        top: 0,\n",
    "                        behavior: 'smooth'\n",
    "                    });\n",
    "                });\n",
    "                \n",
    "                // Active navigation highlighting\n",
    "                const sections = document.querySelectorAll(\".section\");\n",
    "                const navLinks = document.querySelectorAll(\".nav-link\");\n",
    "                \n",
    "                window.addEventListener(\"scroll\", () => {\n",
    "                    let current = \"\";\n",
    "                    sections.forEach(section => {\n",
    "                        const sectionTop = section.offsetTop - 100;\n",
    "                        const sectionHeight = section.offsetHeight;\n",
    "                        const sectionId = section.getAttribute(\"id\");\n",
    "                        \n",
    "                        if (window.scrollY >= sectionTop && \n",
    "                            window.scrollY < sectionTop + sectionHeight) {\n",
    "                            current = sectionId;\n",
    "                        }\n",
    "                    });\n",
    "                    \n",
    "                    navLinks.forEach(link => {\n",
    "                        link.classList.remove(\"active\");\n",
    "                        if (link.getAttribute(\"href\").slice(1) === current) {\n",
    "                            link.classList.add(\"active\");\n",
    "                        }\n",
    "                    });\n",
    "                });\n",
    "\n",
    "                // Initialize plots as responsive\n",
    "                document.addEventListener('DOMContentLoaded', function() {\n",
    "                    const plots = document.querySelectorAll('.js-plotly-plot');\n",
    "                    plots.forEach(plot => {\n",
    "                        Plotly.Plots.resize(plot.id);\n",
    "                    });\n",
    "                });\n",
    "            </script>\n",
    "        </body>\n",
    "        </html>\n",
    "    \"\"\"\n",
    "\n",
    "# Cell 12: Save Results Function\n",
    "def save_results(results_df: pd.DataFrame, plots: Dict[str, List[Tuple[str, go.Figure]]], \n",
    "                OUTPUT_DIR: Path) -> None:\n",
    "    \"\"\"Save analysis results and plots with improved JSON serialization.\"\"\"\n",
    "    try:\n",
    "        # Create directories\n",
    "        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        vis_dir = OUTPUT_DIR / 'visualizations'\n",
    "        vis_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save DataFrame\n",
    "        results_df.to_csv(OUTPUT_DIR / 'analysis_results.csv', index=False)\n",
    "        logging.info(f\"Saved results to CSV: {OUTPUT_DIR / 'analysis_results.csv'}\")\n",
    "        \n",
    "        def numpy_encoder(obj):\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            if isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "        \n",
    "        # Save plots\n",
    "        for section_name, section_plots in plots.items():\n",
    "            section_dir = vis_dir / section_name\n",
    "            section_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            for plot_title, fig in section_plots:\n",
    "                try:\n",
    "                    # Clean filename\n",
    "                    clean_title = \"\".join(\n",
    "                        x for x in plot_title \n",
    "                        if x.isalnum() or x in [' ', '-', '_']\n",
    "                    ).rstrip()\n",
    "                    clean_title = clean_title.replace(' ', '_').lower()\n",
    "                    \n",
    "                    # Save HTML version\n",
    "                    html_path = section_dir / f\"{clean_title}.html\"\n",
    "                    fig.write_html(str(html_path))\n",
    "                    logging.info(f\"Saved HTML plot: {html_path}\")\n",
    "                    \n",
    "                    # Save as JSON with custom encoder\n",
    "                    json_path = section_dir / f\"{clean_title}.json\"\n",
    "                    with open(json_path, 'w') as f:\n",
    "                        json.dump(fig.to_dict(), f, default=numpy_encoder)\n",
    "                    logging.info(f\"Saved JSON plot: {json_path}\")\n",
    "                    \n",
    "                except Exception as plot_error:\n",
    "                    logging.warning(f\"Error saving plot '{plot_title}': {str(plot_error)}\")\n",
    "                    continue\n",
    "        \n",
    "        logging.info(f\"Results saved to: {OUTPUT_DIR}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving results: {str(e)}\")\n",
    "        pass\n",
    "\n",
    "# Cell 13: Main Execution Cell\n",
    "def main():\n",
    "    try:\n",
    "        # Step 1: Load and preprocess data\n",
    "        logging.info(\"Starting ASV Selection Analysis...\")\n",
    "        display(Markdown(\"# ASV Selection Analysis Pipeline\"))\n",
    "        \n",
    "        df = load_data()\n",
    "        df, feature_columns = preprocess_data(df)\n",
    "        \n",
    "        # Display preprocessing results\n",
    "        display(Markdown(f\"\"\"\n",
    "## Data Preprocessing Results\n",
    "- Total records: {len(df):,}\n",
    "- Number of features: {len(feature_columns)}\n",
    "\n",
    "### Feature List:\n",
    "{', '.join(feature_columns)}\n",
    "\n",
    "### Data Summary:\n",
    "```\n",
    "{df[feature_columns].describe().to_string()}\n",
    "```\n",
    "        \"\"\"))\n",
    "        \n",
    "        # Step 2: Scale features\n",
    "        X = df[feature_columns]\n",
    "        y = df['target']\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(X),\n",
    "            columns=feature_columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Step 3: Train model\n",
    "        model, accuracy, feature_importance, X_test, y_test = train_model(\n",
    "            X_scaled, y, feature_columns\n",
    "        )\n",
    "        \n",
    "        # Display model results\n",
    "        sorted_features = feature_importance.sort_values('importance', ascending=False)\n",
    "        display(Markdown(f\"\"\"\n",
    "## Model Training Results\n",
    "### Model Parameters:\n",
    "```\n",
    "{pd.Series(model.get_params()).to_string()}\n",
    "```\n",
    "\n",
    "### Feature Importance:\n",
    "```\n",
    "{sorted_features.to_string(index=False)}\n",
    "```\n",
    "\n",
    "### Model Performance:\n",
    "- Accuracy: {accuracy:.4f}\n",
    "        \"\"\"))\n",
    "        \n",
    "        # Step 4: Find optimal threshold\n",
    "        df, optimal_threshold, threshold_results = find_optimal_threshold(\n",
    "            df, model, scaler, feature_columns\n",
    "        )\n",
    "        \n",
    "        # Display threshold results\n",
    "        best_metrics = pd.DataFrame(threshold_results['fine_search']).loc[\n",
    "            pd.DataFrame(threshold_results['fine_search'])['f1_score'].idxmax()\n",
    "        ]\n",
    "        display(Markdown(f\"\"\"\n",
    "## Threshold Optimization Results\n",
    "### Optimal Threshold: {optimal_threshold:.4f}\n",
    "\n",
    "### Performance at Optimal Threshold:\n",
    "- F1 Score: {best_metrics['f1_score']:.4f}\n",
    "- Precision: {best_metrics['precision']:.4f}\n",
    "- Recall: {best_metrics['recall']:.4f}\n",
    "- True Positives: {int(best_metrics['true_positives']):,}\n",
    "- False Positives: {int(best_metrics['false_positives']):,}\n",
    "- False Negatives: {int(best_metrics['false_negatives']):,}\n",
    "- True Negatives: {int(best_metrics['true_negatives']):,}\n",
    "        \"\"\"))\n",
    "\n",
    "        # Step 5: Save model components\n",
    "        save_model_components(\n",
    "            model=model,\n",
    "            scaler=scaler,\n",
    "            feature_columns=feature_columns,\n",
    "            optimal_threshold=optimal_threshold,\n",
    "            MODEL_SAVE_DIR=MODEL_SAVE_DIR\n",
    "        )\n",
    "        \n",
    "        # Step 6: Apply model predictions\n",
    "        results_df = apply_model_predictions(\n",
    "            df, model, scaler, feature_columns, optimal_threshold\n",
    "        )\n",
    "        \n",
    "        analyze_asv_selection(results_df)\n",
    "\n",
    "        # Display prediction results\n",
    "        agreement_stats = results_df['agreement'].value_counts()\n",
    "        selection_stats = results_df['model_decision'].value_counts()\n",
    "        display(Markdown(f\"\"\"\n",
    "## Model Prediction Results\n",
    "### Summary Statistics:\n",
    "- Total samples processed: {results_df['project_readfile_id'].nunique():,}\n",
    "- Total ASVs analyzed: {len(results_df):,}\n",
    "- ASVs selected: {selection_stats.get('select', 0):,}\n",
    "- ASVs not selected: {selection_stats.get('unselect', 0):,}\n",
    "\n",
    "### Agreement Analysis:\n",
    "- Total agreements: {agreement_stats.get('agree', 0):,}\n",
    "- Total disagreements: {agreement_stats.get('disagree', 0):,}\n",
    "- Agreement rate: {(agreement_stats.get('agree', 0) / len(results_df)):.4f}\n",
    "        \"\"\"))\n",
    "\n",
    "        # Step 7: Generate all plots\n",
    "        display(Markdown(\"## Generating Plots and Analysis\"))\n",
    "        plots = {}\n",
    "\n",
    "        try:\n",
    "            # Feature analysis plots\n",
    "            feature_analysis = analyze_features(\n",
    "                results_df, model, scaler, feature_columns,\n",
    "                {'optimal_threshold': optimal_threshold}\n",
    "            )\n",
    "            if feature_analysis:\n",
    "                plots.update(feature_analysis)\n",
    "            display(Markdown(\"✓ Feature analysis completed\"))\n",
    "\n",
    "            # Model performance plots\n",
    "            model_perf_plots = create_model_performance_plots(\n",
    "                model, X_test, y_test, feature_importance\n",
    "            )\n",
    "            if model_perf_plots:\n",
    "                plots['model_performance'] = model_perf_plots\n",
    "            display(Markdown(\"✓ Model performance plots generated\"))\n",
    "\n",
    "            # Correlation analysis\n",
    "            corr_matrix, linkage_matrix = analyze_correlation(results_df, feature_columns)\n",
    "            if corr_matrix is not None:\n",
    "                correlation_plots = create_correlation_plots(corr_matrix, linkage_matrix)\n",
    "                if correlation_plots:\n",
    "                    plots['correlation_analysis'] = correlation_plots\n",
    "            display(Markdown(\"✓ Correlation analysis completed\"))\n",
    "\n",
    "            # Threshold analysis\n",
    "            threshold_plots = create_threshold_analysis_plots(threshold_results)\n",
    "            if threshold_plots:\n",
    "                plots['threshold_analysis'] = threshold_plots\n",
    "            display(Markdown(\"✓ Threshold analysis plots generated\"))\n",
    "\n",
    "            # Agreement analysis\n",
    "            agreement_plots = create_agreement_analysis_plots(results_df)\n",
    "            if agreement_plots:\n",
    "                plots['agreement_analysis'] = agreement_plots\n",
    "            display(Markdown(\"✓ Agreement analysis plots generated\"))\n",
    "\n",
    "            # Taxonomic analysis\n",
    "            if 'taxonomy' in results_df.columns:\n",
    "                taxonomic_plots = create_taxonomic_analysis_plots(results_df)\n",
    "                if taxonomic_plots:\n",
    "                    plots['taxonomic_analysis'] = taxonomic_plots\n",
    "                display(Markdown(\"✓ Taxonomic analysis completed\"))\n",
    "\n",
    "            # ASV-MMG analysis\n",
    "            if 'mt_id' in results_df.columns:\n",
    "                asv_mmg_plots = create_asv_mmg_analysis_plots(results_df)\n",
    "                if asv_mmg_plots:\n",
    "                    plots['asv_mmg_analysis'] = asv_mmg_plots\n",
    "                display(Markdown(\"✓ ASV-MMG analysis completed\"))\n",
    "\n",
    "        except Exception as plot_error:\n",
    "            logging.error(f\"Error generating plots: {str(plot_error)}\")\n",
    "            display(Markdown(f\"⚠️ **Error generating plots**: {str(plot_error)}\"))\n",
    "\n",
    "        # Step 8: Save results and generate report\n",
    "        display(Markdown(\"## Saving Results and Generating Report\"))\n",
    "        try:\n",
    "            # Save results including plots\n",
    "            save_results(results_df, plots, OUTPUT_DIR)\n",
    "            display(Markdown(f\"✅ Results saved to: {OUTPUT_DIR}\"))\n",
    "            \n",
    "            # Generate HTML report\n",
    "            report_path = generate_enhanced_html_report(\n",
    "                results_df=results_df,\n",
    "                model_metrics={\n",
    "                    'accuracy': accuracy,\n",
    "                    'optimal_threshold': optimal_threshold\n",
    "                },\n",
    "                plots=plots,\n",
    "                OUTPUT_DIR=OUTPUT_DIR\n",
    "            )\n",
    "            display(Markdown(f\"✅ Report generated: {report_path}\"))\n",
    "            \n",
    "        except Exception as save_error:\n",
    "            logging.error(f\"Error saving results and generating report: {str(save_error)}\")\n",
    "            display(Markdown(f\"⚠️ **Error saving results**: {str(save_error)}\"))\n",
    "        \n",
    "        # Final summary\n",
    "        display(Markdown(f\"\"\"\n",
    "## Final Analysis Summary\n",
    "### Model Performance:\n",
    "- Final accuracy: {accuracy:.4f}\n",
    "- Optimal threshold: {optimal_threshold:.4f}\n",
    "- Overall agreement rate: {(results_df['agreement'] == 'agree').mean():.4f}\n",
    "\n",
    "### Dataset Statistics:\n",
    "- Total ASVs processed: {len(results_df):,}\n",
    "- ASVs selected: {(results_df['model_decision'] == 'select').sum():,}\n",
    "- Samples analyzed: {results_df['project_readfile_id'].nunique():,}\n",
    "\n",
    "### File Locations:\n",
    "- Results saved to: {OUTPUT_DIR}\n",
    "- Model saved to: {MODEL_SAVE_DIR}\n",
    "- Detailed report at: {OUTPUT_DIR / 'reports' / 'analysis_report.html'}\n",
    "\n",
    "### Analysis Components Completed:\n",
    "- ✓ Data preprocessing\n",
    "- ✓ Model training\n",
    "- ✓ Threshold optimization\n",
    "- ✓ Model saving\n",
    "- ✓ Prediction application\n",
    "- ✓ Feature analysis\n",
    "- ✓ Performance analysis\n",
    "- ✓ Visualization generation\n",
    "- ✓ Report generation\n",
    "        \"\"\"))\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {str(e)}\")\n",
    "        display(Markdown(f\"❌ **Critical Error in Execution**: {str(e)}\"))\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:20:20 - INFO - Logging to /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/old/Barcoding_Machine_Learning_Thailand_result_200425/logs/asv_prediction_20250424_102020.log\n",
      "2025-04-24 10:20:20 - INFO - Loaded 10,150 rows\n",
      "2025-04-24 10:20:20 - INFO - Model components loaded\n",
      "2025-04-24 10:20:20 - INFO - Pre‑processing complete\n",
      "2025-04-24 10:20:21 - INFO - Total ASVs selected: 1016\n",
      "2025-04-24 10:20:21 - INFO - Selection rate = 10.01%\n",
      "2025-04-24 10:20:21 - INFO - Feature importance plot saved\n",
      "2025-04-24 10:20:21 - INFO - Correlation heatmap saved\n",
      "2025-04-24 10:20:21 - INFO - Agreement pie chart saved\n",
      "2025-04-24 10:20:21 - INFO - Agreement rate = 100.00%  (10150 agree / 0 disagree)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Agreement statistics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Metric",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Value",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "cbaa9b9a-b17d-48de-b35e-95a5a0aef98e",
       "rows": [
        [
         "0",
         "Agreements",
         "10150"
        ],
        [
         "1",
         "Disagreements",
         "0"
        ],
        [
         "2",
         "Agreement rate %",
         "100.00"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agreements</td>\n",
       "      <td>10150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Disagreements</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agreement rate %</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Metric   Value\n",
       "0        Agreements   10150\n",
       "1     Disagreements       0\n",
       "2  Agreement rate %  100.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## **Pipeline Summary**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Metric",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Value",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "264d0116-1a18-4ba9-b753-2584b80206e6",
       "rows": [
        [
         "0",
         "Total ASVs",
         "10150"
        ],
        [
         "1",
         "Selected ASVs",
         "1016"
        ],
        [
         "2",
         "Selection rate %",
         "10.01"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total ASVs</td>\n",
       "      <td>10150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selected ASVs</td>\n",
       "      <td>1016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Selection rate %</td>\n",
       "      <td>10.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Metric  Value\n",
       "0        Total ASVs  10150\n",
       "1     Selected ASVs   1016\n",
       "2  Selection rate %  10.01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Per‑sample selections (top 10)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "project_readfile_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "selected_ASVs",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6b65b220-463b-4ded-b544-920148001b58",
       "rows": [
        [
         "0",
         "BIBC_THA101_A01",
         "1"
        ],
        [
         "682",
         "BIBC_THA114_E08_2nd",
         "1"
        ],
        [
         "669",
         "BIBC_THA114_D12_2nd",
         "1"
        ],
        [
         "670",
         "BIBC_THA114_E02",
         "1"
        ],
        [
         "671",
         "BIBC_THA114_E02_2nd",
         "1"
        ],
        [
         "672",
         "BIBC_THA114_E03",
         "1"
        ],
        [
         "673",
         "BIBC_THA114_E03_2nd",
         "1"
        ],
        [
         "674",
         "BIBC_THA114_E04",
         "1"
        ],
        [
         "675",
         "BIBC_THA114_E04_2nd",
         "1"
        ],
        [
         "676",
         "BIBC_THA114_E05",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_readfile_id</th>\n",
       "      <th>selected_ASVs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIBC_THA101_A01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>BIBC_THA114_E08_2nd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>BIBC_THA114_D12_2nd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>BIBC_THA114_E02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>BIBC_THA114_E02_2nd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>BIBC_THA114_E03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>BIBC_THA114_E03_2nd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>BIBC_THA114_E04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>BIBC_THA114_E04_2nd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>BIBC_THA114_E05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     project_readfile_id  selected_ASVs\n",
       "0        BIBC_THA101_A01              1\n",
       "682  BIBC_THA114_E08_2nd              1\n",
       "669  BIBC_THA114_D12_2nd              1\n",
       "670      BIBC_THA114_E02              1\n",
       "671  BIBC_THA114_E02_2nd              1\n",
       "672      BIBC_THA114_E03              1\n",
       "673  BIBC_THA114_E03_2nd              1\n",
       "674      BIBC_THA114_E04              1\n",
       "675  BIBC_THA114_E04_2nd              1\n",
       "676      BIBC_THA114_E05              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:20:21 - INFO - Summary tables generated & saved\n",
      "2025-04-24 10:20:21 - INFO - ✓ All outputs saved\n",
      "2025-04-24 10:20:21 - INFO - PIPELINE FINISHED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "ASV Selection Prediction Pipeline\n",
    "Author: Sarawut Ounjai\n",
    "Updated: 20‑Apr‑2025\n",
    "\"\"\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────\n",
    "# 1. Imports\n",
    "# ──────────────────────────────────────────────────────\n",
    "import os, sys, json, shutil, tempfile, warnings, logging, traceback\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import load\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc            # (reserved)\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────\n",
    "# 2. Paths & Logging\n",
    "# ──────────────────────────────────────────────────────\n",
    "BASE_DIR      = Path(\"/Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis\")\n",
    "CHAPTER_DIR   = BASE_DIR / \"Chapter2_Data_generation/Barcoding_Machine_Learning/old\"\n",
    "INPUT_FILE    = CHAPTER_DIR / \"Barcoding_Machine_Learning_Thailand.csv\"\n",
    "MODEL_DIR     = CHAPTER_DIR / \"/Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Barcoding_Machine_Learning/Barcoding_Machine_Learning_OR_200425\"\n",
    "OUTPUT_DIR    = CHAPTER_DIR / \"Barcoding_Machine_Learning_Thailand_result_200425\"\n",
    "TRAINING_DATA_PATH = MODEL_DIR / \"training_data.csv\"                 # ← NEW\n",
    "\n",
    "# folders\n",
    "(OUTPUT_DIR / \"results\").mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / \"logs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def setup_logging() -> None:\n",
    "    fmt = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file  = OUTPUT_DIR / \"logs\" / f\"asv_prediction_{timestamp}.log\"\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    for h in logger.handlers[:]:  # clear\n",
    "        logger.removeHandler(h)\n",
    "\n",
    "    fh = logging.FileHandler(log_file, encoding=\"utf‑8\")\n",
    "    fh.setFormatter(logging.Formatter(fmt, \"%Y-%m-%d %H:%M:%S\"))\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setFormatter(logging.Formatter(fmt, \"%Y-%m-%d %H:%M:%S\"))\n",
    "    ch.setLevel(logging.INFO)\n",
    "\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    logging.info(f\"Logging to {log_file}\")\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "# ──────────────────────────────────────────────────────\n",
    "# 3. I/O helpers\n",
    "# ──────────────────────────────────────────────────────\n",
    "def load_data() -> pd.DataFrame:\n",
    "    try:\n",
    "        tmp_dir  = Path(tempfile.mkdtemp())\n",
    "        tmp_file = tmp_dir / \"input.csv\"\n",
    "        shutil.copy2(INPUT_FILE, tmp_file)\n",
    "        df = pd.read_csv(tmp_file)\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Input CSV is empty\")\n",
    "        logging.info(f\"Loaded {len(df):,} rows\")\n",
    "        return df\n",
    "    finally:\n",
    "        shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "\n",
    "def load_model_components() -> Tuple[Any, StandardScaler, List[str], float]:\n",
    "    # if MODEL_DIR exists use it, otherwise fall back to newest *_0425 folder\n",
    "    model_dir = MODEL_DIR\n",
    "    if not model_dir.exists():\n",
    "        # find any candidate dir inside CHAPTER_DIR\n",
    "        cand = sorted(CHAPTER_DIR.glob(\"Barcoding_Machine_Learning_*\"),\n",
    "                      key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if cand:\n",
    "            model_dir = cand[0]\n",
    "            logging.warning(f\"MODEL_DIR not found – using {model_dir}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No model directory found.\")\n",
    "\n",
    "    model_path  = model_dir / \"trained_model.joblib\"\n",
    "    scaler_path = model_dir / \"scaler.joblib\"\n",
    "    cfg_path    = model_dir / \"model_config.json\"\n",
    "    for p in (model_path, scaler_path, cfg_path):\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "\n",
    "    model  = load(model_path)\n",
    "    scaler = load(scaler_path)\n",
    "    cfg    = json.loads(cfg_path.read_text())\n",
    "    logging.info(\"Model components loaded\")\n",
    "    return model, scaler, cfg[\"feature_columns\"], cfg[\"optimal_threshold\"]\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────\n",
    "# 4. Pre‑processing utilities\n",
    "# ──────────────────────────────────────────────────────\n",
    "def preprocess_data(df: pd.DataFrame, feat_cols: List[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # normalise match column\n",
    "    mapping = {'match':'match', 'Match':'match', 'TRUE':'match', True:'match',\n",
    "               'no_match':'no_match','NoMatch':'no_match','FALSE':'no_match',False:'no_match'}\n",
    "    df['match'] = df['match'].map(mapping).fillna('no_match')\n",
    "\n",
    "    # engineered features\n",
    "    df['read_proportion'] = df['reads'] / df['total_asv_reads'].replace(0, np.nan)\n",
    "    df['log_reads']       = np.log1p(df['reads'])\n",
    "    df['read_density']    = df['reads'] / df['asv_count'].replace(0, 1)\n",
    "    df['is_single_asv']   = (df['asv_count'] == 1).astype(int)\n",
    "    df['is_dominant_asv'] = (df['read_proportion'] > 0.9).astype(int)\n",
    "    df['is_match']        = (df['match']=='match').astype(int)\n",
    "\n",
    "    df['reads_rank'] = df.groupby('project_readfile_id')['reads']\\\n",
    "                          .rank(method='min', ascending=False)\n",
    "    df['relative_abundance'] = df.groupby('project_readfile_id')['reads']\\\n",
    "                                  .transform(lambda x: x/x.sum())\n",
    "\n",
    "    df[feat_cols] = df[feat_cols].fillna(0)\n",
    "    logging.info(\"Pre‑processing complete\")\n",
    "    return df\n",
    "\n",
    "# ──────────────────────────────────────────────────────\n",
    "# 5. Distribution & threshold helpers\n",
    "# ──────────────────────────────────────────────────────\n",
    "def analyze_dataset_differences(train: pd.DataFrame,\n",
    "                                new: pd.DataFrame,\n",
    "                                feat_cols: List[str]) -> Dict[str, Dict]:\n",
    "    diff = {}\n",
    "    for f in feat_cols:\n",
    "        if f not in train.columns or f not in new.columns:\n",
    "            continue\n",
    "        t_stats = train[f].describe()\n",
    "        n_stats = new[f].describe()\n",
    "        pct = abs(t_stats['mean']-n_stats['mean'])/max(abs(t_stats['mean']),1e-9)*100\n",
    "        diff[f] = {'train_mean':t_stats['mean'], 'new_mean':n_stats['mean'],\n",
    "                   'percent_diff':pct, 'significant_shift': pct>20}\n",
    "    return diff\n",
    "\n",
    "def adjust_threshold(probas: np.ndarray, base: float) -> float:\n",
    "    mean = probas.mean()\n",
    "    if   mean < 0.3: return base*0.9\n",
    "    elif mean > 0.7: return base*1.1\n",
    "    return base\n",
    "\n",
    "# ──────────────────────────────────────────────────────\n",
    "# 6. Validation helper\n",
    "# ──────────────────────────────────────────────────────\n",
    "def validate_predictions(df: pd.DataFrame) -> List[str]:\n",
    "    issues=[]\n",
    "    sel_cnt = (df['model_decision']=='select').sum()\n",
    "    if sel_cnt==0: issues.append(\"No ASVs selected\")\n",
    "    if sel_cnt > df['project_readfile_id'].nunique():\n",
    "        issues.append(\"More selections than samples\")\n",
    "    return issues\n",
    "\n",
    "# ──────────────────────────────────────────────────────\n",
    "# 7. Prediction step\n",
    "# ──────────────────────────────────────────────────────\n",
    "def make_predictions(df: pd.DataFrame,\n",
    "                     model: Any,\n",
    "                     scaler: StandardScaler,\n",
    "                     feat_cols: List[str],\n",
    "                     base_thr: float) -> Tuple[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    Select **at most one** ASV per sample:\n",
    "        • must be a taxonomy match AND ≥4 reads\n",
    "        • probability ≥ adjusted threshold\n",
    "        • highest‑probability candidate per sample only\n",
    "    \"\"\"\n",
    "    # ── score all ASVs ──────────────────────────────────\n",
    "    X_scaled = scaler.transform(df[feat_cols].fillna(0))\n",
    "    probs    = model.predict_proba(X_scaled)[:, 1]\n",
    "    adj_thr  = adjust_threshold(probs, base_thr)\n",
    "\n",
    "    df = df.copy()\n",
    "    df['prediction_probability'] = probs\n",
    "    df['model_prediction']       = 0\n",
    "    df['model_decision']         = 'unselect'\n",
    "    df['selection_confidence']   = 'low'\n",
    "\n",
    "    # ── iterate sample‑by‑sample ───────────────────────\n",
    "    for sid, sub in df.groupby('project_readfile_id'):\n",
    "        # candidate filter: taxonomy match + read depth\n",
    "        cand = sub[(sub['match'] == 'match') & (sub['reads'] >= 4)]\n",
    "        if cand.empty:\n",
    "            continue\n",
    "\n",
    "        # rank by probability\n",
    "        cand_sorted = cand.sort_values('prediction_probability', ascending=False)\n",
    "        best_idx    = cand_sorted.index[0]\n",
    "        best_prob   = cand_sorted.iloc[0]['prediction_probability']\n",
    "\n",
    "        # confidence from margin\n",
    "        if len(cand_sorted) > 1:\n",
    "            margin = best_prob - cand_sorted.iloc[1]['prediction_probability']\n",
    "            conf   = 'high' if margin > 0.20 else 'medium'\n",
    "        else:\n",
    "            conf = 'high'\n",
    "\n",
    "        # apply threshold\n",
    "        if best_prob >= adj_thr:\n",
    "            df.loc[best_idx, 'model_prediction']     = 1\n",
    "            df.loc[best_idx, 'model_decision']       = 'select'\n",
    "            df.loc[best_idx, 'selection_confidence'] = conf\n",
    "\n",
    "    # optional agreement flag\n",
    "    if 'autopropose' in df.columns:\n",
    "        df['agreement'] = np.where(\n",
    "            ((df['model_decision'] == 'select')     & (df['autopropose'] == 'select')) |\n",
    "            ((df['model_decision'] == 'unselect')   & (df['autopropose'] == 'unselect')),\n",
    "            'agree', 'disagree'\n",
    "        )\n",
    "\n",
    "    # validation warnings\n",
    "    sel_per_sample = df[df['model_decision'] == 'select'].groupby('project_readfile_id').size()\n",
    "    if (sel_per_sample > 1).any():\n",
    "        logging.warning(\"Some samples have multiple selections (should not happen).\")\n",
    "\n",
    "    logging.info(f\"Total ASVs selected: {(df['model_decision']=='select').sum()}\")\n",
    "    return df, adj_thr\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────\n",
    "# 8. Analysis helpers (NEW minimal versions)\n",
    "# ──────────────────────────────────────────────────────\n",
    "def analyze_asv_selection(df: pd.DataFrame) -> None:\n",
    "    total = len(df)\n",
    "    selected = (df['model_decision']=='select').sum()\n",
    "    logging.info(f\"Selection rate = {selected/total:.2%}\")\n",
    "\n",
    "def analyze_features(df, model, scaler, feat_cols, cfg) -> Dict:\n",
    "    plots={}\n",
    "    if hasattr(model,'feature_importances_'):\n",
    "        imp_df = pd.DataFrame({'feature':feat_cols,\n",
    "                               'importance':model.feature_importances_})\\\n",
    "                 .sort_values('importance',ascending=False)\n",
    "        fig = px.bar(imp_df, x='feature', y='importance',\n",
    "                     title=\"Feature importances\").update_xaxes(tickangle=-45)\n",
    "        fig_path = OUTPUT_DIR / \"results\" / \"feature_importances.html\"\n",
    "        fig.write_html(fig_path)\n",
    "        plots['feature_importance']=fig\n",
    "        logging.info(\"Feature importance plot saved\")\n",
    "    return plots\n",
    "\n",
    "def analyze_correlation(df:pd.DataFrame, feat_cols:List[str]):\n",
    "    try:\n",
    "        corr = df[feat_cols].corr(method='spearman')\n",
    "        return corr\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Correlation analysis skipped: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_correlation_plots(corr:pd.DataFrame) -> Dict:\n",
    "    if corr is None: return {}\n",
    "    fig = px.imshow(corr, title=\"Spearman correlation\", aspect=\"auto\",\n",
    "                    color_continuous_scale=\"RdBu_r\", zmin=-1, zmax=1)\n",
    "    path = OUTPUT_DIR / \"results\" / \"correlation_heatmap.html\"\n",
    "    fig.write_html(path)\n",
    "    logging.info(\"Correlation heatmap saved\")\n",
    "    return {'corr_heatmap':fig}\n",
    "\n",
    "def create_agreement_analysis_plots(df) -> Dict:\n",
    "    \"\"\"\n",
    "    Pie chart of model–autopropose agreement / disagreement.\n",
    "    Safe for pandas 1.x & 2.x.\n",
    "    \"\"\"\n",
    "    if 'agreement' not in df.columns:\n",
    "        logging.info(\"Agreement column not present – skipping plot.\")\n",
    "        return {}\n",
    "\n",
    "    # value_counts → DataFrame with explicit column names\n",
    "    ct = (\n",
    "        df['agreement']\n",
    "        .value_counts(dropna=False)\n",
    "        .rename_axis('agreement')\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    fig = px.pie(\n",
    "        ct,\n",
    "        names='agreement',   # column holding labels\n",
    "        values='count',      # column holding counts\n",
    "        title=\"Model vs Autopropose – Agreement\"\n",
    "    )\n",
    "\n",
    "    out_path = OUTPUT_DIR / \"results\" / \"agreement_pie.html\"\n",
    "    fig.write_html(out_path)\n",
    "    logging.info(\"Agreement pie chart saved\")\n",
    "    return {'agreement_pie': fig}\n",
    "\n",
    "\n",
    "def create_taxonomic_analysis_plots(df) -> Dict:\n",
    "    if 'taxonomy' not in df.columns: return {}\n",
    "    tax_ct = df[df['model_decision']=='select']['taxonomy'].value_counts().head(20)\n",
    "    fig = px.bar(tax_ct, x=tax_ct.index, y=tax_ct.values,\n",
    "                 title=\"Top selected taxa\").update_xaxes(tickangle=-45)\n",
    "    fig.write_html(OUTPUT_DIR / \"results\" / \"taxa_bar.html\")\n",
    "    return {'taxa':fig}\n",
    "\n",
    "def create_asv_mmg_analysis_plots(df) -> Dict:\n",
    "    if 'mt_id' not in df.columns: return {}\n",
    "    ct = df.groupby('model_decision')['mt_id'].nunique().reset_index()\n",
    "    fig = px.bar(ct, x='model_decision', y='mt_id',\n",
    "                 title=\"Unique MMG IDs by decision\")\n",
    "    fig.write_html(OUTPUT_DIR / \"results\" / \"mmg_bar.html\")\n",
    "    return {'mmg':fig}\n",
    "\n",
    "# ──────────────────────────────────────────────────────\n",
    "# 8 b.  Robust summary tables / quick EDA\n",
    "# ──────────────────────────────────────────────────────\n",
    "def summarise_data_analysis(df: pd.DataFrame, out_dir: Path,\n",
    "                            top_n_taxa: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    Creates quick‑look summary tables and saves each as a separate CSV.\n",
    "    Also logs & pretty‑prints (Jupyter) the agreement rate if present.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ── overall counts ────────────────────────────────\n",
    "    total    = len(df)\n",
    "    selected = (df['model_decision'] == 'select').sum()\n",
    "    sel_rate = selected / total * 100\n",
    "    overall_tbl = pd.DataFrame({\n",
    "        'Metric': ['Total ASVs', 'Selected ASVs', 'Selection rate %'],\n",
    "        'Value' : [total, selected, f\"{sel_rate:.2f}\"]\n",
    "    })\n",
    "    overall_tbl.to_csv(out_dir / \"overall_summary.csv\", index=False)\n",
    "\n",
    "    # ── per‑sample selection counts ───────────────────\n",
    "    per_sample = (\n",
    "        df[df['model_decision'] == 'select']\n",
    "        .groupby('project_readfile_id')\n",
    "        .size()\n",
    "        .rename('selected_ASVs')\n",
    "        .reset_index()\n",
    "        .sort_values('selected_ASVs', ascending=False)\n",
    "    )\n",
    "    per_sample.to_csv(out_dir / \"per_sample_selected.csv\", index=False)\n",
    "\n",
    "    # ── top‑N taxonomy table (if taxonomy column) ─────\n",
    "    if 'taxonomy' in df.columns:\n",
    "        tax_tbl = (\n",
    "            df[df['model_decision'] == 'select']['taxonomy']\n",
    "            .value_counts()\n",
    "            .head(top_n_taxa)\n",
    "            .rename_axis('taxonomy')\n",
    "            .reset_index(name='selected_count')\n",
    "        )\n",
    "        tax_tbl.to_csv(out_dir / \"top_taxa_selected.csv\", index=False)\n",
    "\n",
    "        # optional quick bar‑plot\n",
    "        fig = px.bar(tax_tbl, x='taxonomy', y='selected_count',\n",
    "                     title=f\"Top {top_n_taxa} selected taxa\")\n",
    "        fig.update_xaxes(tickangle=-45)\n",
    "        fig.write_html(out_dir / \"top_taxa_selected.html\")\n",
    "\n",
    "    # ── agreement stats (if available) ────────────────\n",
    "    if 'agreement' in df.columns:\n",
    "        agree_cnt    = (df['agreement'] == 'agree').sum()\n",
    "        disagree_cnt = (df['agreement'] == 'disagree').sum()\n",
    "        agr_rate     = agree_cnt / (agree_cnt + disagree_cnt) * 100\n",
    "        agree_tbl = pd.DataFrame({\n",
    "            'Metric': ['Agreements', 'Disagreements', 'Agreement rate %'],\n",
    "            'Value' : [agree_cnt, disagree_cnt, f\"{agr_rate:.2f}\"]\n",
    "        })\n",
    "        agree_tbl.to_csv(out_dir / \"agreement_stats.csv\", index=False)\n",
    "        logging.info(f\"Agreement rate = {agr_rate:.2f}%  \"\n",
    "                     f\"({agree_cnt} agree / {disagree_cnt} disagree)\")\n",
    "        # pretty‑print in notebooks\n",
    "        try:\n",
    "            from IPython.display import display, Markdown\n",
    "            display(Markdown(\"### Agreement statistics\")); display(agree_tbl)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ── pretty‑print overall & per‑sample head (notebook) ─────\n",
    "    try:\n",
    "        from IPython.display import display, Markdown\n",
    "        display(Markdown(\"## **Pipeline Summary**\")); display(overall_tbl)\n",
    "        display(Markdown(\"### Per‑sample selections (top 10)\"));\n",
    "        display(per_sample.head(10))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    logging.info(\"Summary tables generated & saved\")\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────\n",
    "# 9. MAIN\n",
    "# ──────────────────────────────────────────────────────\n",
    "def main():\n",
    "    try:\n",
    "        # load\n",
    "        df = load_data()\n",
    "        model, scaler, feat_cols, base_thr = load_model_components()\n",
    "\n",
    "        # optional distribution shift check\n",
    "        if TRAINING_DATA_PATH.exists():\n",
    "            train_df = pd.read_csv(TRAINING_DATA_PATH)\n",
    "            shifts = analyze_dataset_differences(train_df, df, feat_cols)\n",
    "            for f,d in shifts.items():\n",
    "                if d['significant_shift']:\n",
    "                    logging.warning(f\"Shift in {f}: {d['percent_diff']:.1f}%\")\n",
    "\n",
    "        # preprocess + predict\n",
    "        df = preprocess_data(df, feat_cols)\n",
    "        df, adj_thr = make_predictions(df, model, scaler, feat_cols, base_thr)\n",
    "        analyze_asv_selection(df)\n",
    "\n",
    "        # path ready BEFORE summaries / plots\n",
    "        res_dir = OUTPUT_DIR / \"results\"\n",
    "        res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # ── plots\n",
    "        plots={}\n",
    "        plots.update(analyze_features(df, model, scaler, feat_cols, {'thr':adj_thr}))\n",
    "        plots.update(create_correlation_plots(analyze_correlation(df, feat_cols)))\n",
    "        plots.update(create_agreement_analysis_plots(df))\n",
    "        plots.update(create_taxonomic_analysis_plots(df))\n",
    "        plots.update(create_asv_mmg_analysis_plots(df))\n",
    "\n",
    "        # create summary tables\n",
    "        summarise_data_analysis(df, res_dir)\n",
    "\n",
    "\n",
    "        # ── save outputs\n",
    "        res_dir = OUTPUT_DIR / \"results\"\n",
    "        df.to_csv(res_dir / \"predictions.csv\", index=False)\n",
    "        df[feat_cols].describe().to_csv(res_dir / \"feature_stats.csv\")\n",
    "\n",
    "        summary = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'samples': int(df['project_readfile_id'].nunique()),\n",
    "            'asvs': len(df),\n",
    "            'selected': int((df['model_decision']=='select').sum()),\n",
    "            'base_threshold': float(base_thr),\n",
    "            'adjusted_threshold': float(adj_thr)\n",
    "        }\n",
    "        (res_dir/\"prediction_summary.json\").write_text(json.dumps(summary, indent=4))\n",
    "        logging.info(\"✓ All outputs saved\")\n",
    "        logging.info(\"PIPELINE FINISHED SUCCESSFULLY\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
