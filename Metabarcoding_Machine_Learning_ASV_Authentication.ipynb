{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metabarcoding ASV Analysis Pipeline\n",
      "==================================================\n",
      "\n",
      "Initializing ASV analyzer...\n",
      "\n",
      "=== STARTING COMPLETE ASV ANALYSIS ===\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysis Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOADING AND PREPROCESSING DATA ===\n",
      "\n",
      "=== LOADING AND PREPROCESSING DATA ===\n",
      "--------------------------------------------------\n",
      "\n",
      "Loading metadata...\n",
      "\n",
      "Available columns in your data:\n",
      "['well', 'project_readfile_id', 'image_id', 'sample_count', 'asv_count', 'asv_id', 'read_count', 'total_read', 'count_read', 'percentage', 'nearest_main_ASV', 'nearest_main_dist', 'nearest_cand_ASV', 'nearest_cand_dist', 'family_tree', 'subfamily_tree', 'step1', 'step2']\n",
      "\n",
      "Filtering outgroups...\n",
      "Removed 1286 outgroup entries\n",
      "\n",
      "Loading sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading FASTA:  33%|███▎      | 781k/2.37M [00:00<00:00, 25.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating basic features...\n",
      "\n",
      "Creating advanced features...\n",
      "\n",
      "Creating and validating features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating basic features...\n",
      "Creating basic features...\n",
      "\n",
      "Creating taxonomic features...\n",
      "Creating taxonomic features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating abundance features...\n",
      "Creating abundance features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abundance features created successfully\n",
      "\n",
      "Creating distance features...\n",
      "Creating distance features...\n",
      "\n",
      "Creating quality metrics...\n",
      "Creating quality metrics...\n",
      "\n",
      "Cleaning features...\n",
      "Cleaning features...\n",
      "Cleaning missing values in nearest_main_ASV\n",
      "Cleaning missing values in nearest_cand_ASV\n",
      "Cleaning missing values in subfamily_tree\n",
      "Cleaning missing values in nearest_main_family\n",
      "Cleaning missing values in nearest_main_subfamily\n",
      "Cleaning missing values in relative_cand_dist\n",
      "Applied stricter winsorization to log_read (capped 291 outliers)\n",
      "Applied winsorization to group_size_factor (capped 0 outliers)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning features: 100%|██████████| 10/10 [00:00<00:00, 1208.42it/s]\n",
      "Feature Creation: 100%|██████████| 7/7 [00:00<00:00,  8.20it/s]\n",
      "Analysis Progress:  25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 outliers in log_group_size\n",
      "Found 89 outliers in read_count\n",
      "Found 147 outliers in total_read\n",
      "Found 16 outliers in nearest_main_dist\n",
      "Found 69 outliers in nearest_cand_dist\n",
      "Found 106 outliers in dist_ratio\n",
      "\n",
      "Validating features...\n",
      "\n",
      "Feature Summary:\n",
      "--------------------------------------------------\n",
      "\n",
      "Basic Statistics:\n",
      "       sample_count  asv_count  read_count  total_read  count_read   \n",
      "count      7656.000   7656.000    7656.000    7656.000    7656.000  \\\n",
      "mean         20.990     82.090      17.541     170.915       3.653   \n",
      "std          18.205     54.396      22.208     254.706       3.725   \n",
      "min           1.000      1.000       1.000       4.000       1.000   \n",
      "25%           8.000     40.000       2.000       7.000       1.000   \n",
      "50%          15.000     64.000       5.000      18.000       2.000   \n",
      "75%          28.000    126.000      25.000     260.000       5.000   \n",
      "max          69.000    231.000      59.500     639.500      25.000   \n",
      "\n",
      "       percentage  nearest_main_dist  nearest_cand_dist  log_read   \n",
      "count    7656.000           7656.000           7656.000  7656.000  \\\n",
      "mean       53.723              0.114              0.016     2.539   \n",
      "std        41.407              0.148              0.021     1.923   \n",
      "min         0.000              0.000              0.000     0.693   \n",
      "25%        11.760              0.012              0.000     1.099   \n",
      "50%        50.000              0.036              0.006     1.792   \n",
      "75%       100.000              0.157              0.023     3.258   \n",
      "max       100.000              0.375              0.058     7.577   \n",
      "\n",
      "       read_proportion  ...  distance_product  relative_main_dist   \n",
      "count         7656.000  ...          7656.000            7656.000  \\\n",
      "mean             0.537  ...             0.106               1.000   \n",
      "std              0.414  ...             0.424               2.049   \n",
      "min              0.000  ...             0.000               0.000   \n",
      "25%              0.118  ...             0.000               0.067   \n",
      "50%              0.500  ...             0.000               0.231   \n",
      "75%              1.000  ...             0.001               0.914   \n",
      "max              1.000  ...             9.434              29.676   \n",
      "\n",
      "       relative_cand_dist  log_distance_ratio  normalized_main_dist   \n",
      "count            7656.000            7656.000              7656.000  \\\n",
      "mean                0.998               1.124                 0.000   \n",
      "std                 3.169               1.216                 1.000   \n",
      "min                 0.000               0.000                -0.585   \n",
      "25%                 0.000               0.000                -0.559   \n",
      "50%                 0.084               0.725                -0.502   \n",
      "75%                 0.339               1.799                -0.224   \n",
      "max                52.407               6.457                 6.550   \n",
      "\n",
      "       normalized_cand_dist  quality_score  reliability_score   \n",
      "count              7656.000       7656.000           7656.000  \\\n",
      "mean                  0.000          0.227              0.051   \n",
      "std                   1.000          0.052              0.037   \n",
      "min                  -0.383          0.000              0.000   \n",
      "25%                  -0.383          0.204              0.032   \n",
      "50%                  -0.365          0.226              0.050   \n",
      "75%                  -0.309          0.244              0.060   \n",
      "max                   9.399          1.000              1.000   \n",
      "\n",
      "       classification_confidence  sequence_complexity  \n",
      "count                   7656.000             7656.000  \n",
      "mean                      -0.000                0.408  \n",
      "std                        0.559                0.011  \n",
      "min                       -1.247                0.376  \n",
      "25%                       -0.148                0.400  \n",
      "50%                       -0.069                0.408  \n",
      "75%                        0.130                0.417  \n",
      "max                        8.094                0.455  \n",
      "\n",
      "[8 rows x 43 columns]\n",
      "\n",
      "Feature Correlations with Targets:\n",
      "sample_count:\n",
      "  Main/Candidate correlation: 0.006\n",
      "  Authentication correlation: 0.011\n",
      "asv_count:\n",
      "  Main/Candidate correlation: -0.121\n",
      "  Authentication correlation: -0.125\n",
      "read_count:\n",
      "  Main/Candidate correlation: 0.949\n",
      "  Authentication correlation: 0.874\n",
      "total_read:\n",
      "  Main/Candidate correlation: 0.497\n",
      "  Authentication correlation: 0.461\n",
      "count_read:\n",
      "  Main/Candidate correlation: 0.078\n",
      "  Authentication correlation: 0.058\n",
      "percentage:\n",
      "  Main/Candidate correlation: 0.191\n",
      "  Authentication correlation: 0.195\n",
      "nearest_main_dist:\n",
      "  Main/Candidate correlation: 0.132\n",
      "  Authentication correlation: 0.290\n",
      "nearest_cand_dist:\n",
      "  Main/Candidate correlation: -0.443\n",
      "  Authentication correlation: -0.321\n",
      "log_read:\n",
      "  Main/Candidate correlation: 0.864\n",
      "  Authentication correlation: 0.815\n",
      "read_proportion:\n",
      "  Main/Candidate correlation: 0.191\n",
      "  Authentication correlation: 0.195\n",
      "sample_proportion:\n",
      "  Main/Candidate correlation: 0.010\n",
      "  Authentication correlation: 0.014\n",
      "normalized_read_count:\n",
      "  Main/Candidate correlation: 0.363\n",
      "  Authentication correlation: 0.326\n",
      "dist_ratio:\n",
      "  Main/Candidate correlation: -0.447\n",
      "  Authentication correlation: -0.377\n",
      "rank_read:\n",
      "  Main/Candidate correlation: 0.759\n",
      "  Authentication correlation: 0.759\n",
      "group_size:\n",
      "  Main/Candidate correlation: -0.172\n",
      "  Authentication correlation: -0.181\n",
      "log_group_size:\n",
      "  Main/Candidate correlation: -0.168\n",
      "  Authentication correlation: -0.182\n",
      "group_size_quantile:\n",
      "  Main/Candidate correlation: -0.130\n",
      "  Authentication correlation: -0.142\n",
      "group_size_factor:\n",
      "  Main/Candidate correlation: -0.174\n",
      "  Authentication correlation: -0.186\n",
      "taxonomic_depth:\n",
      "  Main/Candidate correlation: nan\n",
      "  Authentication correlation: nan\n",
      "family_abundance_mean:\n",
      "  Main/Candidate correlation: 0.058\n",
      "  Authentication correlation: 0.045\n",
      "family_abundance_std:\n",
      "  Main/Candidate correlation: 0.031\n",
      "  Authentication correlation: 0.012\n",
      "read_count_count:\n",
      "  Main/Candidate correlation: -0.103\n",
      "  Authentication correlation: -0.147\n",
      "family_prevalence:\n",
      "  Main/Candidate correlation: -0.016\n",
      "  Authentication correlation: -0.033\n",
      "count_read_std:\n",
      "  Main/Candidate correlation: -0.016\n",
      "  Authentication correlation: -0.041\n",
      "taxonomic_similarity:\n",
      "  Main/Candidate correlation: -0.087\n",
      "  Authentication correlation: -0.210\n",
      "relative_abundance:\n",
      "  Main/Candidate correlation: 0.440\n",
      "  Authentication correlation: 0.399\n",
      "abundance_zscore:\n",
      "  Main/Candidate correlation: 0.185\n",
      "  Authentication correlation: 0.166\n",
      "normalized_by_group_size:\n",
      "  Main/Candidate correlation: 0.440\n",
      "  Authentication correlation: 0.399\n",
      "within_group_rank:\n",
      "  Main/Candidate correlation: 0.694\n",
      "  Authentication correlation: 0.693\n",
      "robust_group_norm:\n",
      "  Main/Candidate correlation: 0.185\n",
      "  Authentication correlation: 0.166\n",
      "prevalence_ratio:\n",
      "  Main/Candidate correlation: 0.078\n",
      "  Authentication correlation: 0.058\n",
      "group_presence_ratio:\n",
      "  Main/Candidate correlation: 0.069\n",
      "  Authentication correlation: 0.070\n",
      "distance_sum:\n",
      "  Main/Candidate correlation: -0.037\n",
      "  Authentication correlation: 0.119\n",
      "distance_product:\n",
      "  Main/Candidate correlation: -0.146\n",
      "  Authentication correlation: -0.036\n",
      "relative_main_dist:\n",
      "  Main/Candidate correlation: 0.107\n",
      "  Authentication correlation: 0.198\n",
      "relative_cand_dist:\n",
      "  Main/Candidate correlation: -0.184\n",
      "  Authentication correlation: -0.100\n",
      "log_distance_ratio:\n",
      "  Main/Candidate correlation: -0.541\n",
      "  Authentication correlation: -0.429\n",
      "normalized_main_dist:\n",
      "  Main/Candidate correlation: 0.106\n",
      "  Authentication correlation: 0.257\n",
      "normalized_cand_dist:\n",
      "  Main/Candidate correlation: -0.224\n",
      "  Authentication correlation: -0.103\n",
      "quality_score:\n",
      "  Main/Candidate correlation: 0.320\n",
      "  Authentication correlation: 0.237\n",
      "reliability_score:\n",
      "  Main/Candidate correlation: 0.094\n",
      "  Authentication correlation: 0.073\n",
      "classification_confidence:\n",
      "  Main/Candidate correlation: 0.238\n",
      "  Authentication correlation: 0.116\n",
      "sequence_complexity:\n",
      "  Main/Candidate correlation: -0.018\n",
      "  Authentication correlation: -0.035\n",
      "\n",
      "Data Summary:\n",
      "Initial ASVs: 8942\n",
      "ASVs after filtering: 7656\n",
      "\n",
      "Step1 Distribution:\n",
      "step1\n",
      "candidate    5701\n",
      "main         1955\n",
      "\n",
      "Step2 Distribution:\n",
      "step2\n",
      "unauthenticated    5369\n",
      "authenticated      2287\n",
      "\n",
      "Feature Summary:\n",
      "          log_read  rank_read  log_group_size  group_size_quantile  read_count  total_read  nearest_main_dist  nearest_cand_dist  dist_ratio  log_group_size\n",
      "count      7656.00    7656.00         7656.00              7656.00     7656.00     7656.00            7656.00            7656.00     7656.00         7656.00\n",
      "mean          2.54       0.50            3.82                 2.22       17.54      170.91               0.11               0.02        3.39            3.82\n",
      "std           1.92       0.29            0.69                 0.95       22.21      254.71               0.15               0.02        4.44            0.69\n",
      "min           0.69       0.10            2.05                 0.00        1.00        4.00               0.00               0.00        0.00            2.05\n",
      "25%           1.10       0.24            3.40                 2.00        2.00        7.00               0.01               0.00        0.00            3.40\n",
      "50%           1.79       0.47            3.76                 3.00        5.00       18.00               0.04               0.01        1.06            3.76\n",
      "75%           3.26       0.75            4.30                 3.00       25.00      260.00               0.16               0.02        5.04            4.30\n",
      "max           7.58       1.00            5.20                 3.00       59.50      639.50               0.38               0.06       12.61            5.20\n",
      "skew          1.28       0.04           -0.15                -0.95        1.17        1.18               1.11               1.22        1.22           -0.15\n",
      "kurtosis      0.68      -1.26           -0.33                -0.25       -0.38       -0.46              -0.64              -0.15       -0.06           -0.33\n",
      "missing       0.00       0.00            0.00                 0.00        0.00        0.00               0.00               0.00        0.00            0.00\n",
      "\n",
      "=== MAIN/CANDIDATE CLASSIFICATION ===\n",
      "\n",
      "=== MAIN/CANDIDATE CLASSIFICATION ===\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing: Preparing data\n",
      "\n",
      "Preparing classification data...\n",
      "\n",
      "Class Distribution:\n",
      "Candidate: 5701 (74.46%)\n",
      "Main: 1955 (25.54%)\n",
      "\n",
      "Executing: Selecting best model\n",
      "\n",
      "Performing model selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForest Results:\n",
      "Best F1 score: 0.9997\n",
      "Best parameters: {'class_weight': 'balanced', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models: 100%|██████████| 2/2 [00:09<00:00,  4.75s/it]\n",
      "Classification Progress: 100%|██████████| 4/4 [00:09<00:00,  2.40s/it]\n",
      "Analysis Progress:  50%|█████     | 2/4 [00:10<00:12,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GradientBoosting Results:\n",
      "Best F1 score: 0.9994\n",
      "Best parameters: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 100, 'subsample': 0.8}\n",
      "\n",
      "Selected model: RandomForestClassifier\n",
      "Best cross-validation F1 score: 0.9997\n",
      "\n",
      "Executing: Evaluating model\n",
      "\n",
      "Evaluating model performance...\n",
      "\n",
      "Optimal threshold: 0.9900\n",
      "ROC AUC: 1.0000\n",
      "PR AUC: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Candidate       1.00      1.00      1.00      1141\n",
      "        Main       1.00      1.00      1.00       391\n",
      "\n",
      "    accuracy                           1.00      1532\n",
      "   macro avg       1.00      1.00      1.00      1532\n",
      "weighted avg       1.00      1.00      1.00      1532\n",
      "\n",
      "\n",
      "Executing: Applying classification\n",
      "\n",
      "Applying classification to all data...\n",
      "\n",
      "Classification Results:\n",
      "------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "step1             candidate  main   All\n",
      "predicted_status                       \n",
      "candidate              5701     1  5702\n",
      "main                      0  1954  1954\n",
      "All                    5701  1955  7656\n",
      "\n",
      "Overall Agreement: 99.99%\n",
      "\n",
      "Per-Class Metrics:\n",
      "\n",
      "Main:\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.9995\n",
      "  F1: 0.9997\n",
      "\n",
      "Candidate:\n",
      "  Precision: 0.9998\n",
      "  Recall: 1.0000\n",
      "  F1: 0.9999\n",
      "\n",
      "Additional Insights:\n",
      "High confidence predictions: 7650 (99.9% of total)\n",
      "Agreement for high confidence predictions: 100.00%\n",
      "\n",
      "Disagreement Analysis:\n",
      "predicted_status  candidate\n",
      "step1                      \n",
      "main                      1\n",
      "\n",
      "=== AUTHENTICATION ANALYSIS ===\n",
      "\n",
      "=== AUTHENTICATION ANALYSIS ===\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing: Preparing authentication data\n",
      "\n",
      "Preparing authentication data...\n",
      "\n",
      "Authentication Class Distribution:\n",
      "Unauthenticated: 5369 (70.13%)\n",
      "Authenticated: 2287 (29.87%)\n",
      "\n",
      "Executing: Training authentication model\n",
      "\n",
      "Training authentication model...\n",
      "\n",
      "Cross-validation scores:\n",
      "Mean F1: 0.9992 (±0.0022)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Authentication Analysis: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]\n",
      "Analysis Progress:  75%|███████▌  | 3/4 [00:14<00:05,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "             feature  importance\n",
      "0         read_count    0.411708\n",
      "4         dist_ratio    0.227022\n",
      "3  nearest_cand_dist    0.175075\n",
      "2  nearest_main_dist    0.109529\n",
      "1         total_read    0.071033\n",
      "5     log_group_size    0.005633\n",
      "\n",
      "Executing: Evaluating authentication model\n",
      "\n",
      "Evaluating authentication model performance...\n",
      "\n",
      "Authentication Model Performance:\n",
      "Accuracy: 0.9980\n",
      "Precision: 1.0000\n",
      "Recall: 0.9934\n",
      "F1: 0.9967\n",
      "Roc_Auc: 0.9998\n",
      "Matthews_Corrcoef: 0.9953\n",
      "\n",
      "Executing: Applying authentication\n",
      "\n",
      "Applying authentication rules...\n",
      "\n",
      "Authentication Results:\n",
      "------------------------------\n",
      "\n",
      "Overall Results:\n",
      "Total ASVs authenticated: 2120\n",
      "Overall authentication rate: 27.69%\n",
      "Agreement with original labels: 97.82%\n",
      "\n",
      "Results by ASV Type:\n",
      "\n",
      "Main ASVs:\n",
      "Total: 1954\n",
      "Authenticated: 1954\n",
      "Authentication rate: 25.52%\n",
      "Agreement: 25.52%\n",
      "\n",
      "Candidate ASVs:\n",
      "Total: 5702\n",
      "Authenticated: 166\n",
      "Authentication rate: 2.17%\n",
      "Agreement: 72.30%\n",
      "\n",
      "Distance Metrics for Authenticated ASVs:\n",
      "Mean distance ratio: 0.81\n",
      "Median distance ratio: 0.00\n",
      "\n",
      "Confusion Matrix:\n",
      "step2            authenticated  unauthenticated   All\n",
      "auth_status                                          \n",
      "authenticated             2120                0  2120\n",
      "unauthenticated            167             5369  5536\n",
      "All                       2287             5369  7656\n",
      "\n",
      "=== EXPORTING RESULTS ===\n",
      "\n",
      "=== EXPORTING ANALYSIS RESULTS ===\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysis Progress: 100%|██████████| 4/4 [00:16<00:00,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed report generated: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/result/asv_analysis_results_report.txt\n",
      "PDF report generated: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/result/asv_analysis_results_report.pdf\n",
      "\n",
      "Results exported successfully:\n",
      "- Excel file: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/result/asv_analysis_results.xlsx\n",
      "- JSON file: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/result/asv_analysis_results.json\n",
      "- Report file: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/result/asv_analysis_results_report.txt\n",
      "- PDF report: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/result/asv_analysis_results_report.pdf\n",
      "\n",
      "=== ANALYSIS COMPLETED SUCCESSFULLY ===\n",
      "\n",
      "Saving analysis state to /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/result/analysis_state.pkl...\n",
      "Analysis state saved successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation file exported to /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/result/validation_file.xlsx\n",
      "\n",
      "Validation Warnings:\n",
      "- High variation in group sizes detected\n",
      "\n",
      "Improvement Suggestions:\n",
      "\n",
      "Features:\n",
      "- Consider removing low-importance feature: group_size_quantile\n",
      "- Consider removing one feature from highly correlated pairs: (log_read, rank_read)\n",
      "\n",
      "General:\n",
      "- Consider normalizing features within groups due to high size variation\n",
      "\n",
      "Analysis pipeline completed successfully!\n",
      "Results directory: /Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/result\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metabarcoding Analysis Pipeline\n",
    "Clean version with visualization components removed, reporting functionality maintained\n",
    "\"\"\"\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    roc_curve, \n",
    "    auc, \n",
    "    precision_recall_curve,\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    matthews_corrcoef,\n",
    "    balanced_accuracy_score\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# For report generation\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.pagesizes import letter, landscape\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class OutputManager:\n",
    "    \"\"\"Manages output directory structure and file saving\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str, create_dirs: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize OutputManager\n",
    "        \n",
    "        Args:\n",
    "            base_path (str): Base directory for outputs\n",
    "            create_dirs (bool): Whether to create directory structure\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.paths = self._setup_paths(create_dirs)\n",
    "        \n",
    "    def _setup_paths(self, create_dirs: bool) -> Dict[str, Path]:\n",
    "        \"\"\"\n",
    "        Setup output directory structure\n",
    "        \n",
    "        Args:\n",
    "            create_dirs (bool): Whether to create directories\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Path]: Dictionary of output paths\n",
    "        \"\"\"\n",
    "        paths = {\n",
    "            'reports': self.base_path / 'reports',\n",
    "            'data': self.base_path / 'data',\n",
    "            'temp': self.base_path / 'temp'\n",
    "        }\n",
    "        \n",
    "        if create_dirs:\n",
    "            for path in paths.values():\n",
    "                path.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "        return paths\n",
    "    \n",
    "    def get_path(self, category: str) -> Path:\n",
    "        \"\"\"\n",
    "        Get path for specific output category\n",
    "        \n",
    "        Args:\n",
    "            category (str): Output category\n",
    "            \n",
    "        Returns:\n",
    "            Path: Path object for requested category\n",
    "        \"\"\"\n",
    "        return self.paths.get(category, self.base_path)\n",
    "\n",
    "class MemoryMonitor:\n",
    "    \"\"\"Monitors system memory usage\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_memory() -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Check current memory usage\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[float, float]: Used memory (GB), Available memory (GB)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            memory = psutil.virtual_memory()\n",
    "            used_gb = memory.used / (1024 ** 3)\n",
    "            available_gb = memory.available / (1024 ** 3)\n",
    "            return used_gb, available_gb\n",
    "        except (ImportError, AttributeError):\n",
    "            # Fallback if psutil is not available\n",
    "            return 0.0, float('inf')\n",
    "    \n",
    "    @staticmethod\n",
    "    def memory_warning(threshold_gb: float = 1.0) -> bool:\n",
    "        \"\"\"\n",
    "        Check if available memory is below threshold\n",
    "        \n",
    "        Args:\n",
    "            threshold_gb (float): Memory threshold in GB\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if memory is below threshold\n",
    "        \"\"\"\n",
    "        try:\n",
    "            _, available_gb = MemoryMonitor.check_memory()\n",
    "            return available_gb < threshold_gb\n",
    "        except:\n",
    "            # If we can't check memory, assume we're ok\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def get_memory_info() -> dict:\n",
    "        \"\"\"\n",
    "        Get detailed memory information\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing memory statistics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            memory = psutil.virtual_memory()\n",
    "            return {\n",
    "                'total_gb': memory.total / (1024 ** 3),\n",
    "                'available_gb': memory.available / (1024 ** 3),\n",
    "                'used_gb': memory.used / (1024 ** 3),\n",
    "                'percent_used': memory.percent\n",
    "            }\n",
    "        except:\n",
    "            return {\n",
    "                'total_gb': 0.0,\n",
    "                'available_gb': 0.0,\n",
    "                'used_gb': 0.0,\n",
    "                'percent_used': 0.0\n",
    "            }\n",
    "\n",
    "# Main ASV Analyzer class\n",
    "class IntegratedASVAnalyzer:\n",
    "    \"\"\"Integrated system for ASV analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"asv_analysis_output\"):\n",
    "        \"\"\"Initialize ASV analyzer with improved feature selection\"\"\"\n",
    "        # Validate environment\n",
    "        self._validate_environment()\n",
    "        \n",
    "        # Analysis parameters\n",
    "        self.params = {\n",
    "            'random_state': 42,\n",
    "            'test_size': 0.2,\n",
    "            'cv_folds': 5,\n",
    "            'chunk_size': 1000,\n",
    "            'n_bootstrap': 1000,\n",
    "            'threshold': {\n",
    "                'memory_gb': 0.5,\n",
    "                'correlation': 0.8,\n",
    "                'feature_importance': 0.01\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Color scheme (keeping for data categorization only)\n",
    "        self.color_scheme = {\n",
    "            'main': '#00008B',           # Dark Blue\n",
    "            'candidate': '#FF8C00',      # Dark Orange\n",
    "            'authenticated': '#007000',   # Green\n",
    "            'unauthenticated': '#D2222D', # Red\n",
    "            'main-authenticated': '#06d6a0',     # Blue-Green\n",
    "            'main-unauthenticated': '#ffd166',   # Yellow\n",
    "            'candidate-authenticated': '#118ab2', # Blue\n",
    "            'candidate-unauthenticated': '#ef476f' # Pink\n",
    "        }\n",
    "        \n",
    "        # Data storage\n",
    "        self.df = None\n",
    "        self.sequences = None\n",
    "        \n",
    "        # ML components\n",
    "        self.models = {\n",
    "            'main_candidate': None,\n",
    "            'authentication': None,\n",
    "            'scaler': StandardScaler()\n",
    "        }\n",
    "        \n",
    "        # Feature sets - Implementing all improvement suggestions\n",
    "        self.features = {\n",
    "            'main_candidate': [\n",
    "                'log_read',\n",
    "                'rank_read',  \n",
    "                # Removed 'percentage' as suggested\n",
    "                # Removed 'normalized_read_count' due to correlation with log_read\n",
    "                'log_group_size',  # Use log-transformed group size\n",
    "                'group_size_quantile'  # Use quantile-based group size\n",
    "            ],\n",
    "            'authentication': [\n",
    "                'read_count',            \n",
    "                'total_read',            \n",
    "                # Removed 'percentage' as suggested\n",
    "                'nearest_main_dist',     \n",
    "                'nearest_cand_dist',     \n",
    "                'dist_ratio',\n",
    "                'log_group_size'  # Use log-transformed group size\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Thresholds\n",
    "        self.thresholds = {\n",
    "            'main_candidate': None,\n",
    "            'authentication': {\n",
    "                'main': None,\n",
    "                'candidate': None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Performance metrics\n",
    "        self.metrics = {\n",
    "            'main_candidate': {},\n",
    "            'authentication': {},\n",
    "            'advanced': {\n",
    "                'mcc': None,\n",
    "                'stability': None,\n",
    "                'confidence_intervals': None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Output directory\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def _validate_environment(self) -> None:\n",
    "        \"\"\"Validate execution environment and dependencies\"\"\"\n",
    "        required_libs = ['pandas', 'numpy', 'Bio', 'sklearn']\n",
    "        missing_libs = []\n",
    "        for lib in required_libs:\n",
    "            try:\n",
    "                __import__(lib)\n",
    "            except ImportError:\n",
    "                missing_libs.append(lib)\n",
    "        if missing_libs:\n",
    "            raise ImportError(f\"Missing required libraries: {', '.join(missing_libs)}\")\n",
    "\n",
    "    def run_complete_analysis(self, metadata_path: str, fasta_path: str, \n",
    "                            output_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Run complete ASV analysis pipeline with progress tracking\n",
    "        \n",
    "        Args:\n",
    "            metadata_path (str): Path to metadata Excel file\n",
    "            fasta_path (str): Path to FASTA sequence file\n",
    "            output_path (str): Path for output files\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status of analysis\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"\\n=== STARTING COMPLETE ASV ANALYSIS ===\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # Validate input files\n",
    "            self._validate_input_files(metadata_path, fasta_path)\n",
    "            \n",
    "            # Track progress with tqdm\n",
    "            steps = [\n",
    "                (\"Loading and preprocessing data\", self._load_and_preprocess_data),\n",
    "                (\"Main/Candidate classification\", self.analyze_main_candidate),\n",
    "                (\"Authentication analysis\", self.analyze_authentication),\n",
    "                (\"Exporting results\", self.export_results)\n",
    "            ]\n",
    "            \n",
    "            for step_name, step_func in tqdm(steps, desc=\"Analysis Progress\"):\n",
    "                print(f\"\\n=== {step_name.upper()} ===\")\n",
    "                if step_name == \"Loading and preprocessing data\":\n",
    "                    success = step_func(metadata_path, fasta_path)\n",
    "                elif step_name == \"Exporting results\":\n",
    "                    success = step_func(output_path)\n",
    "                else:\n",
    "                    success = step_func()\n",
    "                    \n",
    "                if not success:\n",
    "                    print(f\"Error in {step_name}\")\n",
    "                    return False\n",
    "            \n",
    "            print(\"\\n=== ANALYSIS COMPLETED SUCCESSFULLY ===\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in analysis pipeline: {str(e)}\")\n",
    "            print(\"\\nFull error details:\")\n",
    "            print(traceback.format_exc())\n",
    "            return False\n",
    "\n",
    "    def _validate_input_files(self, metadata_path: str, fasta_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Validate input file existence and format\n",
    "        \n",
    "        Args:\n",
    "            metadata_path (str): Path to metadata file\n",
    "            fasta_path (str): Path to FASTA file\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: If files don't exist\n",
    "            ValueError: If file formats are invalid\n",
    "        \"\"\"\n",
    "        # Check file existence\n",
    "        for path, name in [(metadata_path, \"Metadata\"), (fasta_path, \"FASTA\")]:\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"{name} file not found: {path}\")\n",
    "        \n",
    "        # Validate metadata file format\n",
    "        if not metadata_path.endswith(('.xlsx', '.xls', '.csv')):\n",
    "            raise ValueError(\"Metadata file must be Excel or CSV format\")\n",
    "            \n",
    "        # Validate FASTA file\n",
    "        try:\n",
    "            with open(fasta_path) as f:\n",
    "                first_line = f.readline()\n",
    "                if not first_line.startswith('>'):\n",
    "                    raise ValueError(\"Invalid FASTA file format\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading FASTA file: {str(e)}\")\n",
    "\n",
    "    def _load_and_preprocess_data(self, metadata_path: str, fasta_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Load and preprocess data with improved error handling and memory management\n",
    "        \n",
    "        Args:\n",
    "            metadata_path (str): Path to metadata file\n",
    "            fasta_path (str): Path to FASTA file\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"\\n=== LOADING AND PREPROCESSING DATA ===\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Load metadata with progress bar\n",
    "            print(\"\\nLoading metadata...\")\n",
    "            \n",
    "            # Check if the file is space-delimited or standard format\n",
    "            if metadata_path.endswith('.xlsx'):\n",
    "                self.df = pd.read_excel(metadata_path)\n",
    "            elif metadata_path.endswith('.csv'):\n",
    "                self.df = pd.read_csv(metadata_path)\n",
    "            else:\n",
    "                # Try to load as space-delimited text file\n",
    "                try:\n",
    "                    self.df = pd.read_csv(metadata_path, sep=r'\\s+', engine='python')\n",
    "                    print(\"Loaded file as space-delimited text.\")\n",
    "                except:\n",
    "                    # If that fails, try standard CSV loading\n",
    "                    self.df = pd.read_csv(metadata_path)\n",
    "            \n",
    "            # Print columns to verify\n",
    "            print(\"\\nAvailable columns in your data:\")\n",
    "            print(self.df.columns.tolist())\n",
    "                \n",
    "            initial_count = len(self.df)\n",
    "            \n",
    "            # Memory check\n",
    "            if MemoryMonitor.memory_warning():\n",
    "                print(\"Warning: High memory usage detected\")\n",
    "            \n",
    "            # Filter outgroups if we have the required columns\n",
    "            if 'step1' in self.df.columns and 'step2' in self.df.columns:\n",
    "                # Remove outgroups with progress tracking\n",
    "                print(\"\\nFiltering outgroups...\")\n",
    "                original_shape = self.df.shape\n",
    "                self.df = self.df[\n",
    "                    (self.df['step1'] != 'outgroup') & \n",
    "                    (self.df['step2'] != 'outgroup')\n",
    "                ].reset_index(drop=True)\n",
    "                \n",
    "                filtered_count = original_shape[0] - len(self.df)\n",
    "                print(f\"Removed {filtered_count} outgroup entries\")\n",
    "            else:\n",
    "                print(\"\\nSkipping outgroup filtering - required columns not found.\")\n",
    "            \n",
    "            # Load sequences with progress tracking\n",
    "            print(\"\\nLoading sequences...\")\n",
    "            with tqdm(total=os.path.getsize(fasta_path), \n",
    "                    desc=\"Reading FASTA\", unit='B', unit_scale=True) as pbar:\n",
    "                self.sequences = {}\n",
    "                for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "                    self.sequences[record.id] = record\n",
    "                    pbar.update(len(str(record).encode('utf-8')))\n",
    "            \n",
    "            # Create the basic features\n",
    "            print(\"\\nCreating basic features...\")\n",
    "            \n",
    "            # Basic features that need to be created before further processing\n",
    "            # Log transformations\n",
    "            self.df['log_read'] = np.log1p(self.df['read_count'])\n",
    "            \n",
    "            # Proportions\n",
    "            self.df['read_proportion'] = self.df['read_count'] / self.df['total_read']\n",
    "            self.df['sample_proportion'] = self.df['count_read'] / self.df['sample_count']\n",
    "            \n",
    "            # Normalized counts\n",
    "            self.df['normalized_read_count'] = stats.zscore(self.df['read_count'])\n",
    "            \n",
    "            # Basic ratios\n",
    "            self.df['dist_ratio'] = self.df['nearest_main_dist'] / self.df['nearest_cand_dist']\n",
    "            \n",
    "            # Check for infinite values\n",
    "            inf_mask = np.isinf(self.df['dist_ratio'])\n",
    "            if inf_mask.any():\n",
    "                print(f\"Warning: {inf_mask.sum()} infinite values in distance ratio\")\n",
    "                self.df.loc[inf_mask, 'dist_ratio'] = np.nan\n",
    "                \n",
    "            # Now proceed with full feature creation\n",
    "            try:\n",
    "                print(\"\\nCreating advanced features...\")\n",
    "                self._create_features()\n",
    "                \n",
    "                # Validate data\n",
    "                self._validate_data()\n",
    "                \n",
    "                # Print summary\n",
    "                self._print_data_summary(initial_count)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning in feature creation: {str(e)}\")\n",
    "                print(\"Will proceed with basic features only.\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in data loading: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "            return False\n",
    "\n",
    "    def _validate_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Validate loaded data integrity\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If data validation fails\n",
    "        \"\"\"\n",
    "        # Check for required columns\n",
    "        required_columns = [\n",
    "            'asv_id', 'project_readfile_id', 'read_count', \n",
    "            'total_read', 'count_read', 'percentage',\n",
    "            'nearest_main_dist', 'nearest_cand_dist',\n",
    "            'family_tree', 'subfamily_tree', 'step1', 'step2'\n",
    "        ]\n",
    "        \n",
    "        missing_columns = [col for col in required_columns if col not in self.df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "            \n",
    "        # Check for null values in critical columns\n",
    "        null_counts = self.df[required_columns].isnull().sum()\n",
    "        if null_counts.any():\n",
    "            print(\"\\nWarning: Null values found in columns:\")\n",
    "            print(null_counts[null_counts > 0])\n",
    "            \n",
    "        # Validate sequence IDs\n",
    "        missing_sequences = set(self.df['asv_id']) - set(self.sequences.keys())\n",
    "        if missing_sequences:\n",
    "            raise ValueError(f\"Missing sequences for {len(missing_sequences)} ASVs\")\n",
    "            \n",
    "        # Validate numeric columns\n",
    "        numeric_columns = ['read_count', 'total_read', 'count_read', 'percentage']\n",
    "        for col in numeric_columns:\n",
    "            if not pd.to_numeric(self.df[col], errors='coerce').notnull().all():\n",
    "                raise ValueError(f\"Non-numeric values found in {col}\")\n",
    "\n",
    "    def _create_feature_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create summary statistics for features\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Feature summary statistics\n",
    "        \"\"\"\n",
    "        all_features = (\n",
    "            self.features['main_candidate'] + \n",
    "            self.features['authentication']\n",
    "        )\n",
    "        \n",
    "        summary_stats = self.df[all_features].describe()\n",
    "        \n",
    "        # Add additional statistics\n",
    "        summary_stats.loc['skew'] = self.df[all_features].skew()\n",
    "        summary_stats.loc['kurtosis'] = self.df[all_features].kurtosis()\n",
    "        summary_stats.loc['missing'] = self.df[all_features].isnull().sum()\n",
    "        \n",
    "        return summary_stats\n",
    "\n",
    "    def _print_data_summary(self, initial_count: int) -> None:\n",
    "        \"\"\"\n",
    "        Print comprehensive data summary\n",
    "        \n",
    "        Args:\n",
    "            initial_count (int): Initial number of ASVs\n",
    "        \"\"\"\n",
    "        print(\"\\nData Summary:\")\n",
    "        print(f\"Initial ASVs: {initial_count}\")\n",
    "        print(f\"ASVs after filtering: {len(self.df)}\")\n",
    "        \n",
    "        # Print distributions\n",
    "        print(\"\\nStep1 Distribution:\")\n",
    "        step1_dist = self.df['step1'].value_counts()\n",
    "        print(step1_dist.to_string())\n",
    "        \n",
    "        print(\"\\nStep2 Distribution:\")\n",
    "        step2_dist = self.df['step2'].value_counts()\n",
    "        print(step2_dist.to_string())\n",
    "        \n",
    "        # Print feature summary\n",
    "        print(\"\\nFeature Summary:\")\n",
    "        feature_summary = self._create_feature_summary()\n",
    "        print(feature_summary.round(2).to_string())\n",
    "\n",
    "    def _create_features(self) -> None:\n",
    "        \"\"\"Create all features with validation and additional metrics\"\"\"\n",
    "        try:\n",
    "            print(\"\\nCreating and validating features...\")\n",
    "            \n",
    "            steps = [\n",
    "                (\"Creating basic features\", self._create_basic_features),\n",
    "                (\"Creating taxonomic features\", self._create_taxonomic_features),\n",
    "                (\"Creating abundance features\", self._create_abundance_features),\n",
    "                (\"Creating distance features\", self._create_distance_features),\n",
    "                (\"Creating quality metrics\", self._create_quality_metrics),\n",
    "                (\"Cleaning features\", self._clean_features),\n",
    "                (\"Validating features\", self._validate_features)\n",
    "            ]\n",
    "            \n",
    "            for step_name, step_func in tqdm(steps, desc=\"Feature Creation\"):\n",
    "                try:\n",
    "                    print(f\"\\n{step_name}...\")\n",
    "                    step_func()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {step_name.lower()}: {str(e)}\")\n",
    "                    print(\"Attempting to continue with analysis...\")\n",
    "            \n",
    "            # Print feature summary even if some steps failed\n",
    "            try:\n",
    "                self._print_feature_summary()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in feature summary: {str(e)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature creation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _create_basic_features(self) -> None:\n",
    "        \"\"\"Create basic transformation features with improved robust transformations\"\"\"\n",
    "        print(\"Creating basic features...\")\n",
    "        \n",
    "        # Log transformations\n",
    "        self.df['log_read'] = np.log1p(self.df['read_count'])\n",
    "        \n",
    "        # Rank-based transformation (extremely robust to outliers)\n",
    "        self.df['rank_read'] = self.df['read_count'].rank(pct=True)\n",
    "        \n",
    "        # Proportions\n",
    "        self.df['read_proportion'] = self.df['read_count'] / self.df['total_read']\n",
    "        self.df['sample_proportion'] = self.df['count_read'] / self.df['sample_count']\n",
    "        \n",
    "        # Normalized counts\n",
    "        self.df['normalized_read_count'] = stats.zscore(self.df['read_count'])\n",
    "        \n",
    "        # Basic ratios\n",
    "        self.df['dist_ratio'] = self.df['nearest_main_dist'] / self.df['nearest_cand_dist']\n",
    "        \n",
    "        # Check for infinite values\n",
    "        inf_mask = np.isinf(self.df['dist_ratio'])\n",
    "        if inf_mask.any():\n",
    "            print(f\"Warning: {inf_mask.sum()} infinite values in distance ratio\")\n",
    "            self.df.loc[inf_mask, 'dist_ratio'] = np.nan\n",
    "            \n",
    "        # Add group size as a feature to account for variation\n",
    "        group_sizes = self.df.groupby('project_readfile_id').size()\n",
    "        \n",
    "        # Use a more robust approach for group size normalization\n",
    "        self.df['group_size'] = self.df['project_readfile_id'].map(group_sizes)\n",
    "        self.df['log_group_size'] = np.log1p(self.df['group_size'])\n",
    "        \n",
    "        # Use quantile-based normalization instead of simple division\n",
    "        quantiles = pd.qcut(group_sizes, 4, labels=False, duplicates='drop')\n",
    "        self.df['group_size_quantile'] = self.df['project_readfile_id'].map(\n",
    "            dict(zip(group_sizes.index, quantiles))\n",
    "        )\n",
    "        \n",
    "        # Create a more robust group size factor\n",
    "        mean_group_size = group_sizes.mean()\n",
    "        self.df['group_size_factor'] = np.log1p(self.df['group_size'] / mean_group_size)\n",
    "\n",
    "    def _create_taxonomic_features(self) -> None:\n",
    "        \"\"\"Create taxonomy-based features\"\"\"\n",
    "        try:\n",
    "            print(\"Creating taxonomic features...\")\n",
    "            \n",
    "            # Calculate taxonomic depth\n",
    "            self.df['taxonomic_depth'] = self.df.apply(\n",
    "                lambda row: len(str(row['family_tree']).split(';')) + \n",
    "                        len(str(row['subfamily_tree']).split(';')),\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Calculate family-level statistics - use groupby.agg for better performance\n",
    "            family_stats = self.df.groupby('family_tree', as_index=False).agg({\n",
    "                'read_count': ['mean', 'std', 'count'],\n",
    "                'count_read': ['mean', 'std']\n",
    "            })\n",
    "            \n",
    "            # Flatten column names\n",
    "            family_stats.columns = ['family_tree', 'read_count_mean', 'read_count_std', \n",
    "                                'read_count_count', 'count_read_mean', 'count_read_std']\n",
    "            \n",
    "            # Merge stats back to main dataframe\n",
    "            self.df = self.df.merge(family_stats, on='family_tree', how='left')\n",
    "            \n",
    "            # Rename columns for clarity\n",
    "            self.df.rename(columns={\n",
    "                'read_count_mean': 'family_abundance_mean',\n",
    "                'read_count_std': 'family_abundance_std',\n",
    "                'count_read_mean': 'family_prevalence'\n",
    "            }, inplace=True)\n",
    "            \n",
    "            # Calculate taxonomic similarity\n",
    "            self._calculate_taxonomic_similarity()\n",
    "            \n",
    "            # Fill any missing values\n",
    "            numeric_columns = self.df.select_dtypes(include=[np.number]).columns\n",
    "            self.df[numeric_columns] = self.df[numeric_columns].fillna(0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature creation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calculate_taxonomic_similarity(self) -> None:\n",
    "        \"\"\"Calculate taxonomic similarity scores between ASVs\"\"\"\n",
    "        try:\n",
    "            # Get family and subfamily info for main ASVs\n",
    "            main_tax_info = (\n",
    "                self.df[self.df['step1'] == 'main']\n",
    "                [['asv_id', 'family_tree', 'subfamily_tree']]\n",
    "                .drop_duplicates('asv_id')  # Remove duplicates\n",
    "                .set_index('asv_id')\n",
    "            )\n",
    "            \n",
    "            # Create mapping dictionaries\n",
    "            main_family_map = main_tax_info['family_tree'].to_dict()\n",
    "            main_subfamily_map = main_tax_info['subfamily_tree'].to_dict()\n",
    "            \n",
    "            # Map taxonomic info using dictionaries\n",
    "            self.df['nearest_main_family'] = self.df['nearest_main_ASV'].map(main_family_map)\n",
    "            self.df['nearest_main_subfamily'] = self.df['nearest_main_ASV'].map(main_subfamily_map)\n",
    "            \n",
    "            # Calculate similarity score\n",
    "            self.df['taxonomic_similarity'] = self.df.apply(\n",
    "                lambda row: (\n",
    "                    (int(row['family_tree'] == row['nearest_main_family']) +\n",
    "                    int(row['subfamily_tree'] == row['nearest_main_subfamily'])) / 2\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Fill missing values\n",
    "            self.df['taxonomic_similarity'].fillna(0, inplace=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in taxonomic similarity calculation: {str(e)}\")\n",
    "            # Set default values if calculation fails\n",
    "            self.df['nearest_main_family'] = self.df['family_tree']\n",
    "            self.df['nearest_main_subfamily'] = self.df['subfamily_tree']\n",
    "            self.df['taxonomic_similarity'] = 0\n",
    "\n",
    "    def _create_abundance_features(self) -> None:\n",
    "        \"\"\"Create abundance-based features with improved handling of normalization\"\"\"\n",
    "        try:\n",
    "            print(\"Creating abundance features...\")\n",
    "            \n",
    "            # Group-level statistics\n",
    "            group_stats = self.df.groupby('project_readfile_id').agg({\n",
    "                'read_count': ['sum', 'mean', 'std', 'count'],  # Added count for normalization\n",
    "                'count_read': ['mean', 'std']\n",
    "            })\n",
    "            \n",
    "            # Calculate relative abundances\n",
    "            def calculate_group_abundance(row):\n",
    "                group_sum = group_stats.loc[row['project_readfile_id'], ('read_count', 'sum')]\n",
    "                return row['read_count'] / group_sum\n",
    "            \n",
    "            self.df['relative_abundance'] = self.df.apply(calculate_group_abundance, axis=1)\n",
    "            \n",
    "            # Calculate abundance z-scores within groups with outlier handling\n",
    "            self.df['abundance_zscore'] = self.df.groupby('project_readfile_id')['read_count'].transform(\n",
    "                lambda x: (x - x.median()) / (x.quantile(0.75) - x.quantile(0.25)) if len(x) > 1 else 0\n",
    "            )\n",
    "            \n",
    "            # Add normalization by group size to address high group size variation\n",
    "            self.df['normalized_by_group_size'] = self.df.groupby('project_readfile_id')['read_count'].transform(\n",
    "                lambda x: x / x.sum() if x.sum() > 0 else 0\n",
    "            )\n",
    "            \n",
    "            # Add within-group rank for robust comparative analysis\n",
    "            self.df['within_group_rank'] = self.df.groupby('project_readfile_id')['read_count'].transform(\n",
    "                lambda x: x.rank(pct=True)\n",
    "            )\n",
    "            \n",
    "            # Add more robust within-group normalization\n",
    "            # For each group, normalize features relative to the group's own statistics\n",
    "            for group_id, group_df in self.df.groupby('project_readfile_id'):\n",
    "                # Calculate group-specific metrics\n",
    "                group_median = group_df['read_count'].median()\n",
    "                group_iqr = group_df['read_count'].quantile(0.75) - group_df['read_count'].quantile(0.25)\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if group_iqr == 0:\n",
    "                    group_iqr = 1\n",
    "                    \n",
    "                # Create robust normalized features within the group\n",
    "                self.df.loc[group_df.index, 'robust_group_norm'] = (\n",
    "                    (group_df['read_count'] - group_median) / group_iqr\n",
    "                )\n",
    "            \n",
    "            # Calculate quartiles with improved handling for duplicate values\n",
    "            def safe_qcut(x):\n",
    "                try:\n",
    "                    if len(x.unique()) >= 4:\n",
    "                        return pd.qcut(x, q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'], duplicates='drop')\n",
    "                    else:\n",
    "                        # If we can't create 4 quartiles, create fewer bins\n",
    "                        unique_values = len(x.unique())\n",
    "                        labels = [f'Q{i+1}' for i in range(unique_values)]\n",
    "                        return pd.qcut(x, q=unique_values, labels=labels, duplicates='drop')\n",
    "                except ValueError:\n",
    "                    # Fallback if qcut fails\n",
    "                    return pd.Series(['Q1'] * len(x), index=x.index)\n",
    "\n",
    "            self.df['abundance_quartile'] = self.df.groupby('project_readfile_id')['read_count'].transform(safe_qcut)\n",
    "            \n",
    "            # Calculate prevalence metrics\n",
    "            total_samples = self.df['sample_count'].max()\n",
    "            self.df['prevalence_ratio'] = self.df['count_read'] / total_samples\n",
    "            \n",
    "            # Calculate co-occurrence within groups\n",
    "            group_sizes = self.df.groupby('project_readfile_id').size()\n",
    "            self.df['group_presence_ratio'] = self.df.apply(\n",
    "                lambda row: row['count_read'] / group_sizes[row['project_readfile_id']],\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Fill any missing values\n",
    "            numeric_columns = ['relative_abundance', 'abundance_zscore', 'prevalence_ratio', \n",
    "                            'group_presence_ratio', 'normalized_by_group_size', 'within_group_rank',\n",
    "                            'robust_group_norm']\n",
    "            for col in numeric_columns:\n",
    "                if col in self.df.columns:\n",
    "                    self.df[col] = self.df[col].fillna(0)\n",
    "            \n",
    "            print(\"Abundance features created successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in abundance feature creation: {str(e)}\")\n",
    "            # Set default values if calculation fails\n",
    "            default_features = ['relative_abundance', 'abundance_zscore', 'abundance_quartile',\n",
    "                            'prevalence_ratio', 'group_presence_ratio', 'normalized_by_group_size',\n",
    "                            'within_group_rank', 'robust_group_norm']\n",
    "            for feature in default_features:\n",
    "                if feature not in self.df.columns:\n",
    "                    self.df[feature] = 0\n",
    "            print(\"Set default values for abundance features\")\n",
    "\n",
    "    def _create_distance_features(self) -> None:\n",
    "        \"\"\"Create distance-based features\"\"\"\n",
    "        print(\"Creating distance features...\")\n",
    "        \n",
    "        # Basic distance metrics\n",
    "        self.df['distance_sum'] = self.df['nearest_main_dist'] + self.df['nearest_cand_dist']\n",
    "        self.df['distance_product'] = self.df['nearest_main_dist'] * self.df['nearest_cand_dist']\n",
    "        \n",
    "        # Relative distances within groups\n",
    "        self.df['relative_main_dist'] = self.df.groupby('project_readfile_id')['nearest_main_dist'].transform(\n",
    "            lambda x: x / x.mean()\n",
    "        )\n",
    "        self.df['relative_cand_dist'] = self.df.groupby('project_readfile_id')['nearest_cand_dist'].transform(\n",
    "            lambda x: x / x.mean()\n",
    "        )\n",
    "        \n",
    "        # Distance ratios and normalized distances\n",
    "        self.df['log_distance_ratio'] = np.log1p(self.df['dist_ratio'])\n",
    "        self.df['normalized_main_dist'] = stats.zscore(self.df['nearest_main_dist'])\n",
    "        self.df['normalized_cand_dist'] = stats.zscore(self.df['nearest_cand_dist'])\n",
    "\n",
    "    def _create_quality_metrics(self) -> None:\n",
    "        \"\"\"Create quality and reliability metrics\"\"\"\n",
    "        print(\"Creating quality metrics...\")\n",
    "        \n",
    "        # Quality scores\n",
    "        self.df['quality_score'] = self._calculate_quality_scores()\n",
    "        self.df['reliability_score'] = self._calculate_reliability_scores()\n",
    "        \n",
    "        # Confidence metrics\n",
    "        self.df['classification_confidence'] = self._calculate_classification_confidence()\n",
    "        \n",
    "        # Sequence complexity\n",
    "        if hasattr(self, 'sequences'):\n",
    "            self.df['sequence_complexity'] = self.df['asv_id'].apply(\n",
    "                lambda x: self._calculate_sequence_complexity(x)\n",
    "            )\n",
    "\n",
    "    def _calculate_quality_scores(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Calculate comprehensive quality scores\n",
    "        \n",
    "        Returns:\n",
    "            pd.Series: Quality scores for each ASV\n",
    "        \"\"\"\n",
    "        # Component scores\n",
    "        read_score = stats.zscore(self.df['read_count'])\n",
    "        coverage_score = stats.zscore(self.df['count_read'] / self.df['sample_count'])\n",
    "        abundance_score = stats.zscore(self.df['percentage'])\n",
    "        distance_score = -stats.zscore(self.df['distance_sum'])\n",
    "        \n",
    "        # Combine scores with weights\n",
    "        quality_scores = (\n",
    "            0.3 * read_score +\n",
    "            0.3 * coverage_score +\n",
    "            0.2 * abundance_score +\n",
    "            0.2 * distance_score\n",
    "        )\n",
    "        \n",
    "        # Normalize to 0-1 range\n",
    "        quality_scores = (quality_scores - quality_scores.min()) / (\n",
    "            quality_scores.max() - quality_scores.min()\n",
    "        )\n",
    "        \n",
    "        return quality_scores\n",
    "\n",
    "    def _calculate_reliability_scores(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Calculate reliability scores\n",
    "        \n",
    "        Returns:\n",
    "            pd.Series: Reliability scores for each ASV\n",
    "        \"\"\"\n",
    "        # Component scores\n",
    "        abundance_reliability = self.df['read_count'] / self.df['total_read']\n",
    "        coverage_reliability = self.df['count_read'] / self.df['sample_count']\n",
    "        distance_reliability = 1 / (1 + self.df['distance_sum'])\n",
    "        taxonomic_reliability = self.df['taxonomic_depth'] / self.df['taxonomic_depth'].max()\n",
    "        \n",
    "        # Combine scores\n",
    "        reliability_scores = (\n",
    "            0.3 * abundance_reliability +\n",
    "            0.3 * coverage_reliability +\n",
    "            0.2 * distance_reliability +\n",
    "            0.2 * taxonomic_reliability\n",
    "        )\n",
    "        \n",
    "        # Normalize\n",
    "        reliability_scores = (reliability_scores - reliability_scores.min()) / (\n",
    "            reliability_scores.max() - reliability_scores.min()\n",
    "        )\n",
    "        \n",
    "        return reliability_scores\n",
    "\n",
    "    def _calculate_classification_confidence(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Calculate classification confidence scores\n",
    "        \n",
    "        Returns:\n",
    "            pd.Series: Confidence scores for each ASV\n",
    "        \"\"\"\n",
    "        # Component confidences\n",
    "        abundance_conf = stats.zscore(self.df['read_count'])\n",
    "        coverage_conf = stats.zscore(self.df['count_read'])\n",
    "        distance_conf = stats.zscore(1 / (1 + self.df['nearest_main_dist']))\n",
    "        \n",
    "        # Combine confidence scores\n",
    "        confidence_scores = (\n",
    "            0.4 * abundance_conf +\n",
    "            0.3 * coverage_conf +\n",
    "            0.3 * distance_conf\n",
    "        )\n",
    "        \n",
    "        return confidence_scores\n",
    "\n",
    "    def _calculate_sequence_complexity(self, asv_id: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate sequence complexity score\n",
    "        \n",
    "        Args:\n",
    "            asv_id (str): ASV identifier\n",
    "            \n",
    "        Returns:\n",
    "            float: Sequence complexity score\n",
    "        \"\"\"\n",
    "        if asv_id not in self.sequences:\n",
    "            return np.nan\n",
    "            \n",
    "        sequence = str(self.sequences[asv_id].seq)\n",
    "        \n",
    "        # Calculate basic complexity metrics\n",
    "        length = len(sequence)\n",
    "        unique_bases = len(set(sequence))\n",
    "        gc_content = sequence.count('G') + sequence.count('C')\n",
    "        \n",
    "        # Combine metrics\n",
    "        complexity_score = (\n",
    "            0.4 * (unique_bases / length) +\n",
    "            0.3 * (gc_content / length) +\n",
    "            0.3 * (1 - (sequence.count('N') / length))\n",
    "        )\n",
    "        \n",
    "        return complexity_score\n",
    "\n",
    "    def _clean_features(self) -> None:\n",
    "        \"\"\"Clean and validate all features with improved handling of outliers and missing values\"\"\"\n",
    "        print(\"Cleaning features...\")\n",
    "        \n",
    "        # Get all features\n",
    "        all_features = (\n",
    "            self.features['main_candidate'] + \n",
    "            self.features['authentication']\n",
    "        )\n",
    "        \n",
    "        # Handle reference ASV missing values\n",
    "        if 'nearest_main_ASV' in self.df.columns and self.df['nearest_main_ASV'].isna().any():\n",
    "            print(f\"Cleaning missing values in nearest_main_ASV\")\n",
    "            # Fill with a placeholder or the most common value\n",
    "            most_common = self.df['nearest_main_ASV'].mode()[0]\n",
    "            self.df['nearest_main_ASV'].fillna(most_common, inplace=True)\n",
    "\n",
    "        if 'nearest_cand_ASV' in self.df.columns and self.df['nearest_cand_ASV'].isna().any():\n",
    "            print(f\"Cleaning missing values in nearest_cand_ASV\")\n",
    "            most_common = self.df['nearest_cand_ASV'].mode()[0]\n",
    "            self.df['nearest_cand_ASV'].fillna(most_common, inplace=True)\n",
    "        \n",
    "        # Handle taxonomic missing values\n",
    "        if 'subfamily_tree' in self.df.columns and self.df['subfamily_tree'].isna().any():\n",
    "            print(f\"Cleaning missing values in subfamily_tree\")\n",
    "            self.df['subfamily_tree'].fillna(\"Unknown\", inplace=True)\n",
    "        \n",
    "        if 'nearest_main_family' in self.df.columns and self.df['nearest_main_family'].isna().any():\n",
    "            print(f\"Cleaning missing values in nearest_main_family\")\n",
    "            self.df['nearest_main_family'].fillna(self.df['family_tree'], inplace=True)\n",
    "        \n",
    "        if 'nearest_main_subfamily' in self.df.columns and self.df['nearest_main_subfamily'].isna().any():\n",
    "            print(f\"Cleaning missing values in nearest_main_subfamily\")\n",
    "            self.df['nearest_main_subfamily'].fillna(\"Unknown\", inplace=True)\n",
    "        \n",
    "        # Handle missing values in abundance features\n",
    "        if 'abundance_zscore' in self.df.columns and self.df['abundance_zscore'].isna().any():\n",
    "            print(f\"Cleaning missing values in abundance_zscore\")\n",
    "            self.df['abundance_zscore'].fillna(self.df['abundance_zscore'].median(), inplace=True)\n",
    "        \n",
    "        if 'relative_cand_dist' in self.df.columns and self.df['relative_cand_dist'].isna().any():\n",
    "            print(f\"Cleaning missing values in relative_cand_dist\")\n",
    "            self.df['relative_cand_dist'].fillna(0, inplace=True)\n",
    "        \n",
    "        # Special handling for log_read outliers\n",
    "        if 'log_read' in self.df.columns:\n",
    "            # Use a more aggressive approach for this specific feature\n",
    "            q1, q3 = np.percentile(self.df['log_read'], [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = max(q1 - 2.0 * iqr, self.df['log_read'].min())  # More strict lower bound\n",
    "            upper_bound = q3 + 2.0 * iqr  # More strict upper bound\n",
    "            \n",
    "            # Apply winsorization\n",
    "            outliers_before = ((self.df['log_read'] < lower_bound) | (self.df['log_read'] > upper_bound)).sum()\n",
    "            self.df['log_read'] = np.clip(self.df['log_read'], lower_bound, upper_bound)\n",
    "            print(f\"Applied stricter winsorization to log_read (capped {outliers_before} outliers)\")\n",
    "        \n",
    "        # Special handling for group_size_factor outliers\n",
    "        if 'group_size_factor' in self.df.columns:\n",
    "            # Already log-transformed in _create_basic_features\n",
    "            # Apply additional winsorization\n",
    "            q1, q3 = np.percentile(self.df['group_size_factor'], [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = max(q1 - 2.0 * iqr, self.df['group_size_factor'].min())\n",
    "            upper_bound = q3 + 2.0 * iqr\n",
    "            \n",
    "            outliers_before = ((self.df['group_size_factor'] < lower_bound) | \n",
    "                            (self.df['group_size_factor'] > upper_bound)).sum()\n",
    "            self.df['group_size_factor'] = np.clip(self.df['group_size_factor'], lower_bound, upper_bound)\n",
    "            print(f\"Applied winsorization to group_size_factor (capped {outliers_before} outliers)\")\n",
    "        \n",
    "        # Clean each feature\n",
    "        for feature in tqdm(all_features, desc=\"Cleaning features\"):\n",
    "            # Skip features we've already handled specially\n",
    "            if feature in ['log_read', 'group_size_factor']:\n",
    "                continue\n",
    "                \n",
    "            # Replace infinities\n",
    "            self.df[feature] = self.df[feature].replace([np.inf, -np.inf], np.nan)\n",
    "            \n",
    "            # Handle missing values\n",
    "            missing_count = self.df[feature].isna().sum()\n",
    "            if missing_count > 0:\n",
    "                print(f\"Cleaning {missing_count} missing values in {feature}\")\n",
    "                if 'dist' in feature:\n",
    "                    self.df[feature].fillna(self.df[feature].max(), inplace=True)\n",
    "                else:\n",
    "                    # Use median instead of 0 for better statistical properties\n",
    "                    self.df[feature].fillna(self.df[feature].median(), inplace=True)\n",
    "                    \n",
    "            # Handle outliers using more robust approach\n",
    "            z_scores = np.abs(stats.zscore(self.df[feature], nan_policy='omit'))\n",
    "            outliers = z_scores > 4  # Increased threshold from 3 to 4 for better robustness\n",
    "            if outliers.any():\n",
    "                print(f\"Found {outliers.sum()} outliers in {feature}\")\n",
    "                \n",
    "                # Use winsorization instead of simple replacement with median\n",
    "                q1, q3 = np.percentile(self.df[feature], [25, 75])\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = max(q1 - 1.5 * iqr, self.df[feature].min())  # Ensure non-negative\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                \n",
    "                # Apply winsorization: cap extreme values without losing them entirely\n",
    "                self.df.loc[self.df[feature] < lower_bound, feature] = lower_bound\n",
    "                self.df.loc[self.df[feature] > upper_bound, feature] = upper_bound\n",
    "                \n",
    "    def _validate_features(self) -> None:\n",
    "        \"\"\"Validate feature creation and cleaning\"\"\"\n",
    "        # Check for remaining missing values\n",
    "        missing_values = self.df[self.features['main_candidate'] + \n",
    "                            self.features['authentication']].isnull().sum()\n",
    "        if missing_values.any():\n",
    "            print(\"\\nWarning: Missing values remain in features:\")\n",
    "            print(missing_values[missing_values > 0])\n",
    "        \n",
    "        # Check for infinite values\n",
    "        inf_values = np.isinf(self.df[self.features['main_candidate'] + \n",
    "                                    self.features['authentication']]).sum()\n",
    "        if inf_values.any():\n",
    "            print(\"\\nWarning: Infinite values remain in features:\")\n",
    "            print(inf_values[inf_values > 0])\n",
    "            \n",
    "        # Check for constant features\n",
    "        for feature in self.features['main_candidate'] + self.features['authentication']:\n",
    "            if self.df[feature].nunique() == 1:\n",
    "                print(f\"\\nWarning: Feature '{feature}' has constant value\")\n",
    "        \n",
    "        # Check value ranges\n",
    "        for feature in self.features['main_candidate'] + self.features['authentication']:\n",
    "            feature_range = self.df[feature].agg(['min', 'max'])\n",
    "            if np.isinf(feature_range).any() or np.isnan(feature_range).any():\n",
    "                print(f\"\\nWarning: Feature '{feature}' has invalid range: {feature_range.to_dict()}\")\n",
    "                \n",
    "    def _print_feature_summary(self) -> None:\n",
    "        \"\"\"Print summary of created features\"\"\"\n",
    "        print(\"\\nFeature Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Get all numerical columns\n",
    "        numeric_columns = self.df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        try:\n",
    "            # Calculate and print basic statistics\n",
    "            stats_df = self.df[numeric_columns].describe()\n",
    "            print(\"\\nBasic Statistics:\")\n",
    "            print(stats_df.round(3))\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating basic statistics: {str(e)}\")\n",
    "        \n",
    "        try:\n",
    "            # Print missing values\n",
    "            missing_values = self.df[numeric_columns].isnull().sum()\n",
    "            if missing_values.any():\n",
    "                print(\"\\nMissing Values:\")\n",
    "                print(missing_values[missing_values > 0])\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking missing values: {str(e)}\")\n",
    "        \n",
    "        try:\n",
    "            # Print feature correlations with target variables\n",
    "            print(\"\\nFeature Correlations with Targets:\")\n",
    "            for feature in numeric_columns:\n",
    "                try:\n",
    "                    mc_corr = self.df[feature].corr(\n",
    "                        (self.df['step1'] == 'main').astype(int)\n",
    "                    )\n",
    "                    auth_corr = self.df[feature].corr(\n",
    "                        (self.df['step2'] == 'authenticated').astype(int)\n",
    "                    )\n",
    "                    print(f\"{feature}:\")\n",
    "                    print(f\"  Main/Candidate correlation: {mc_corr:.3f}\")\n",
    "                    print(f\"  Authentication correlation: {auth_corr:.3f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating correlations for {feature}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in correlation analysis: {str(e)}\")\n",
    "            \n",
    "    def analyze_main_candidate(self) -> bool:\n",
    "        \"\"\"\n",
    "        Perform main/candidate classification analysis\n",
    "        \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        print(\"\\n=== MAIN/CANDIDATE CLASSIFICATION ===\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            steps = [\n",
    "                (\"Preparing data\", self._prepare_mc_data),\n",
    "                (\"Selecting best model\", self._select_best_mc_model),\n",
    "                (\"Evaluating model\", self._evaluate_mc_model),\n",
    "                (\"Applying classification\", self._apply_mc_classification)\n",
    "            ]\n",
    "            \n",
    "            # Track progress\n",
    "            for step_name, step_func in tqdm(steps, desc=\"Classification Progress\"):\n",
    "                print(f\"\\nExecuting: {step_name}\")\n",
    "                if step_name == \"Evaluating model\":\n",
    "                    # These steps need the prepared data\n",
    "                    X_test, y_test = self.mc_data['test']\n",
    "                    success = step_func(X_test, y_test)\n",
    "                elif step_name == \"Selecting best model\":\n",
    "                    X_train, y_train = self.mc_data['train']\n",
    "                    success = step_func(X_train, y_train)\n",
    "                else:\n",
    "                    success = step_func()\n",
    "                    \n",
    "                if not success:\n",
    "                    print(f\"Error in {step_name}\")\n",
    "                    return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in main/candidate analysis: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "            return False\n",
    "        \n",
    "    def _prepare_mc_data(self):\n",
    "        \"\"\"Prepare data for main/candidate classification with improved scaling\"\"\"\n",
    "        try:\n",
    "            print(\"\\nPreparing classification data...\")\n",
    "            \n",
    "            # Store data splits in instance variable\n",
    "            self.mc_data = {}\n",
    "            \n",
    "            # Prepare features and target\n",
    "            X = self.df[self.features['main_candidate']].copy()\n",
    "            y = (self.df['step1'] == 'main').astype(int)\n",
    "            \n",
    "            # Split data with stratification by group to handle group size variation\n",
    "            if 'group_size' in self.df.columns:\n",
    "                # Create a stratification feature that considers both class and group size\n",
    "                strat = pd.qcut(self.df['group_size'], 4, duplicates='drop').astype(str) + '_' + y.astype(str)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.params['test_size'],\n",
    "                    random_state=self.params['random_state'],\n",
    "                    stratify=strat\n",
    "                )\n",
    "            else:\n",
    "                # Fall back to standard stratification\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.params['test_size'],\n",
    "                    random_state=self.params['random_state'],\n",
    "                    stratify=y\n",
    "                )\n",
    "            \n",
    "            # Use RobustScaler instead of StandardScaler for better outlier handling\n",
    "            from sklearn.preprocessing import RobustScaler\n",
    "            robust_scaler = RobustScaler()\n",
    "            X_train_scaled = robust_scaler.fit_transform(X_train)\n",
    "            X_test_scaled = robust_scaler.transform(X_test)\n",
    "            \n",
    "            # Store the scaler for later use\n",
    "            self.models['scaler'] = robust_scaler\n",
    "            \n",
    "            # Store splits\n",
    "            self.mc_data['train'] = (X_train_scaled, y_train)\n",
    "            self.mc_data['test'] = (X_test_scaled, y_test)\n",
    "            \n",
    "            # Print class distribution\n",
    "            print(\"\\nClass Distribution:\")\n",
    "            for label, count in zip(['Candidate', 'Main'], np.bincount(y)):\n",
    "                print(f\"{label}: {count} ({count/len(y)*100:.2f}%)\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in data preparation: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "    def _select_best_mc_model(self, X_train, y_train):\n",
    "        \"\"\"Select best model for classification with improved regularization\"\"\"\n",
    "        try:\n",
    "            print(\"\\nPerforming model selection...\")\n",
    "            \n",
    "            # Define models to evaluate with more robust configurations\n",
    "            models = {\n",
    "                'RandomForest': {\n",
    "                    'model': RandomForestClassifier(random_state=self.params['random_state']),\n",
    "                    'params': {\n",
    "                        'n_estimators': [100, 200],\n",
    "                        'max_depth': [10, None],\n",
    "                        'min_samples_split': [5, 10],\n",
    "                        'class_weight': ['balanced'],\n",
    "                        'min_samples_leaf': [2, 4]  # Added to prevent overfitting\n",
    "                    }\n",
    "                },\n",
    "                'GradientBoosting': {\n",
    "                    'model': GradientBoostingClassifier(random_state=self.params['random_state']),\n",
    "                    'params': {\n",
    "                        'n_estimators': [100, 200],\n",
    "                        'learning_rate': [0.05, 0.1],\n",
    "                        'max_depth': [4, 5],\n",
    "                        'subsample': [0.8, 1.0]  # Added to improve robustness\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Find best model\n",
    "            best_score = 0\n",
    "            best_model = None\n",
    "            cv_results = {}\n",
    "            \n",
    "            for name, config in tqdm(models.items(), desc=\"Evaluating models\"):\n",
    "                # Use nested cross-validation for more reliable model selection\n",
    "                from sklearn.model_selection import cross_validate\n",
    "                \n",
    "                # First, perform grid search\n",
    "                grid_search = GridSearchCV(\n",
    "                    config['model'],\n",
    "                    config['params'],\n",
    "                    cv=self.params['cv_folds'],\n",
    "                    scoring='f1',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                \n",
    "                # Store results\n",
    "                cv_results[name] = {\n",
    "                    'best_score': grid_search.best_score_,\n",
    "                    'best_params': grid_search.best_params_,\n",
    "                    'cv_results': grid_search.cv_results_\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n{name} Results:\")\n",
    "                print(f\"Best F1 score: {grid_search.best_score_:.4f}\")\n",
    "                print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "                \n",
    "                if grid_search.best_score_ > best_score:\n",
    "                    best_score = grid_search.best_score_\n",
    "                    best_model = grid_search.best_estimator_\n",
    "            \n",
    "            # Store results\n",
    "            self.models['main_candidate'] = best_model\n",
    "            self.metrics['model_selection'] = cv_results\n",
    "            \n",
    "            print(f\"\\nSelected model: {type(best_model).__name__}\")\n",
    "            print(f\"Best cross-validation F1 score: {best_score:.4f}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in model selection: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "    def _evaluate_mc_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate classification model performance\n",
    "        \n",
    "        Args:\n",
    "            X_test: Test features\n",
    "            y_test: Test labels\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"\\nEvaluating model performance...\")\n",
    "            \n",
    "            # Get predictions\n",
    "            y_pred = self.models['main_candidate'].predict(X_test)\n",
    "            y_prob = self.models['main_candidate'].predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Calculate ROC curve\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            # Calculate precision-recall curve\n",
    "            precision, recall, pr_thresholds = precision_recall_curve(y_test, y_prob)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            \n",
    "            # Find optimal threshold\n",
    "            optimal_idx = np.argmax(tpr - fpr)\n",
    "            self.thresholds['main_candidate'] = thresholds[optimal_idx]\n",
    "            \n",
    "            # Store performance metrics\n",
    "            self.metrics['main_candidate'] = {\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1': f1_score(y_test, y_pred),\n",
    "                'roc_auc': roc_auc,\n",
    "                'pr_auc': pr_auc,\n",
    "                'optimal_threshold': self.thresholds['main_candidate']\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\nOptimal threshold: {self.thresholds['main_candidate']:.4f}\")\n",
    "            print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "            print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "            \n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_test, y_pred, target_names=['Candidate', 'Main']))\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in model evaluation: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "    def _apply_mc_classification(self) -> bool:\n",
    "        \"\"\"\n",
    "        Apply classification to all data and analyze results\n",
    "        \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"\\nApplying classification to all data...\")\n",
    "            \n",
    "            # Scale features\n",
    "            X_all = self.models['scaler'].transform(self.df[self.features['main_candidate']])\n",
    "            \n",
    "            # Get predictions\n",
    "            probabilities = self.models['main_candidate'].predict_proba(X_all)[:, 1]\n",
    "            predictions = (probabilities >= self.thresholds['main_candidate']).astype(int)\n",
    "            \n",
    "            # Store results\n",
    "            self.df['predicted_status'] = np.where(predictions == 1, 'main', 'candidate')\n",
    "            self.df['mc_probability'] = probabilities\n",
    "            \n",
    "            # Calculate agreement metrics\n",
    "            results = pd.crosstab(\n",
    "                self.df['predicted_status'],\n",
    "                self.df['step1'],\n",
    "                margins=True\n",
    "            )\n",
    "            \n",
    "            # Calculate per-class metrics\n",
    "            class_metrics = {}\n",
    "            for status in ['main', 'candidate']:\n",
    "                true_mask = self.df['step1'] == status\n",
    "                pred_mask = self.df['predicted_status'] == status\n",
    "                \n",
    "                class_metrics[status] = {\n",
    "                    'precision': precision_score(true_mask, pred_mask),\n",
    "                    'recall': recall_score(true_mask, pred_mask),\n",
    "                    'f1': f1_score(true_mask, pred_mask)\n",
    "                }\n",
    "            \n",
    "            # Store results\n",
    "            self.metrics['classification_results'] = {\n",
    "                'confusion_matrix': results,\n",
    "                'agreement': (self.df['predicted_status'] == self.df['step1']).mean() * 100,\n",
    "                'class_metrics': class_metrics\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            self._print_classification_application_results()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in classification application: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "    def _print_classification_application_results(self) -> None:\n",
    "        \"\"\"Print detailed classification application results\"\"\"\n",
    "        results = self.metrics['classification_results']\n",
    "        \n",
    "        print(\"\\nClassification Results:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(results['confusion_matrix'])\n",
    "        \n",
    "        print(f\"\\nOverall Agreement: {results['agreement']:.2f}%\")\n",
    "        \n",
    "        print(\"\\nPer-Class Metrics:\")\n",
    "        for status, metrics in results['class_metrics'].items():\n",
    "            print(f\"\\n{status.title()}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"  {metric.title()}: {value:.4f}\")\n",
    "        \n",
    "        # Additional insights\n",
    "        print(\"\\nAdditional Insights:\")\n",
    "        \n",
    "        # Analyze high confidence predictions\n",
    "        high_conf_mask = (self.df['mc_probability'] >= 0.9) | (self.df['mc_probability'] <= 0.1)\n",
    "        high_conf_agreement = (\n",
    "            self.df.loc[high_conf_mask, 'predicted_status'] == \n",
    "            self.df.loc[high_conf_mask, 'step1']\n",
    "        ).mean() * 100\n",
    "        \n",
    "        print(f\"High confidence predictions: {high_conf_mask.sum()} \"\n",
    "              f\"({high_conf_mask.mean()*100:.1f}% of total)\")\n",
    "        print(f\"Agreement for high confidence predictions: {high_conf_agreement:.2f}%\")\n",
    "        \n",
    "        # Analyze disagreements\n",
    "        disagreement_mask = self.df['predicted_status'] != self.df['step1']\n",
    "        if disagreement_mask.any():\n",
    "            print(\"\\nDisagreement Analysis:\")\n",
    "            disagreements = self.df[disagreement_mask].groupby(\n",
    "                ['step1', 'predicted_status']\n",
    "            ).size().unstack(fill_value=0)\n",
    "            print(disagreements)\n",
    "\n",
    "    def analyze_authentication(self):\n",
    "        \"\"\"Perform authentication analysis with comprehensive evaluation\"\"\"\n",
    "        print(\"\\n=== AUTHENTICATION ANALYSIS ===\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            steps = [\n",
    "                (\"Preparing authentication data\", self._prepare_auth_data),\n",
    "                (\"Training authentication model\", self._train_auth_model),\n",
    "                (\"Evaluating authentication model\", self._evaluate_auth_model),\n",
    "                (\"Applying authentication\", self._apply_authentication)\n",
    "            ]\n",
    "            \n",
    "            for step_name, step_func in tqdm(steps, desc=\"Authentication Analysis\"):\n",
    "                print(f\"\\nExecuting: {step_name}\")\n",
    "                \n",
    "                if step_name == \"Evaluating authentication model\":\n",
    "                    X_test, y_test = self.auth_data['test']\n",
    "                    success = step_func(X_test, y_test)\n",
    "                elif step_name == \"Training authentication model\":\n",
    "                    X_train, y_train = self.auth_data['train']\n",
    "                    success = step_func(X_train, y_train)\n",
    "                else:\n",
    "                    success = step_func()\n",
    "                    \n",
    "                if not success:\n",
    "                    print(f\"Error in {step_name}\")\n",
    "                    return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in authentication analysis: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _prepare_auth_data(self) -> bool:\n",
    "        \"\"\"\n",
    "        Prepare data for authentication analysis with improved scaling and stratification\n",
    "        \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"\\nPreparing authentication data...\")\n",
    "            \n",
    "            # Store data splits\n",
    "            self.auth_data = {}\n",
    "            \n",
    "            # Prepare features\n",
    "            X = self.df[self.features['authentication']].copy()\n",
    "            \n",
    "            # Prepare target (authenticated = 1, unauthenticated = 0)\n",
    "            y = (self.df['step2'] == 'authenticated').astype(int)\n",
    "            \n",
    "            # Split data with stratification by group to handle group size variation\n",
    "            if 'group_size' in self.df.columns:\n",
    "                # Create a stratification feature that considers both class and group size\n",
    "                strat = pd.qcut(self.df['group_size'], 4, duplicates='drop').astype(str) + '_' + y.astype(str)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.params['test_size'],\n",
    "                    random_state=self.params['random_state'],\n",
    "                    stratify=strat\n",
    "                )\n",
    "            else:\n",
    "                # Fall back to standard stratification\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y,\n",
    "                    test_size=self.params['test_size'],\n",
    "                    random_state=self.params['random_state'],\n",
    "                    stratify=y\n",
    "                )\n",
    "            \n",
    "            # Use QuantileTransformer for even more robust handling of outliers\n",
    "            from sklearn.preprocessing import QuantileTransformer\n",
    "            quantile_scaler = QuantileTransformer(output_distribution='normal')\n",
    "            X_train_scaled = quantile_scaler.fit_transform(X_train)\n",
    "            X_test_scaled = quantile_scaler.transform(X_test)\n",
    "            \n",
    "            # Store splits\n",
    "            self.auth_data['train'] = (X_train_scaled, y_train)\n",
    "            self.auth_data['test'] = (X_test_scaled, y_test)\n",
    "            \n",
    "            # Store scaler\n",
    "            self.models['auth_scaler'] = quantile_scaler\n",
    "            \n",
    "            # Print class distribution\n",
    "            print(\"\\nAuthentication Class Distribution:\")\n",
    "            for label, count in zip(['Unauthenticated', 'Authenticated'], np.bincount(y)):\n",
    "                print(f\"{label}: {count} ({count/len(y)*100:.2f}%)\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in authentication data preparation: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _train_auth_model(self, X_train: np.ndarray, y_train: np.ndarray) -> bool:\n",
    "        \"\"\"\n",
    "        Train authentication model with advanced features and optimization\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training labels\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"\\nTraining authentication model...\")\n",
    "            \n",
    "            # Initialize model with optimized parameters\n",
    "            self.models['authentication'] = RandomForestClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=None,\n",
    "                min_samples_split=5,\n",
    "                class_weight='balanced',\n",
    "                random_state=self.params['random_state']\n",
    "            )\n",
    "            \n",
    "            # Perform cross-validation\n",
    "            cv_scores = cross_val_score(\n",
    "                self.models['authentication'],\n",
    "                X_train,\n",
    "                y_train,\n",
    "                cv=self.params['cv_folds'],\n",
    "                scoring='f1'\n",
    "            )\n",
    "            \n",
    "            print(\"\\nCross-validation scores:\")\n",
    "            print(f\"Mean F1: {cv_scores.mean():.4f} (±{cv_scores.std()*2:.4f})\")\n",
    "            \n",
    "            # Train final model\n",
    "            self.models['authentication'].fit(X_train, y_train)\n",
    "            \n",
    "            # Calculate and store feature importance\n",
    "            self.metrics['auth_feature_importance'] = pd.DataFrame({\n",
    "                'feature': self.features['authentication'],\n",
    "                'importance': self.models['authentication'].feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(\"\\nFeature Importance:\")\n",
    "            print(self.metrics['auth_feature_importance'])\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in authentication model training: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _evaluate_auth_model(self, X_test: np.ndarray, y_test: np.ndarray) -> bool:\n",
    "        \"\"\"\n",
    "        Evaluate authentication model with comprehensive metrics\n",
    "        \n",
    "        Args:\n",
    "            X_test: Test features\n",
    "            y_test: Test labels\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"\\nEvaluating authentication model performance...\")\n",
    "            \n",
    "            # Get predictions and probabilities\n",
    "            y_pred = self.models['authentication'].predict(X_test)\n",
    "            y_prob = self.models['authentication'].predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1': f1_score(y_test, y_pred),\n",
    "                'roc_auc': roc_auc_score(y_test, y_prob),\n",
    "                'matthews_corrcoef': matthews_corrcoef(y_test, y_pred)\n",
    "            }\n",
    "            \n",
    "            # Calculate ROC and PR curves\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "            precision, recall, pr_thresholds = precision_recall_curve(y_test, y_prob)\n",
    "            \n",
    "            # Find optimal thresholds\n",
    "            optimal_threshold_metrics = self._find_optimal_thresholds(\n",
    "                y_test, y_prob, fpr, tpr, precision, recall\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            self.metrics['authentication'] = {\n",
    "                **metrics,\n",
    "                **optimal_threshold_metrics\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(\"\\nAuthentication Model Performance:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"{metric.title()}: {value:.4f}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in authentication model evaluation: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _find_optimal_thresholds(self, y_test: np.ndarray, y_prob: np.ndarray,\n",
    "                               fpr: np.ndarray, tpr: np.ndarray,\n",
    "                               precision: np.ndarray, recall: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Find optimal thresholds for authentication\n",
    "        \n",
    "        Args:\n",
    "            y_test: Test labels\n",
    "            y_prob: Prediction probabilities\n",
    "            fpr: False positive rates\n",
    "            tpr: True positive rates\n",
    "            precision: Precision values\n",
    "            recall: Recall values\n",
    "            \n",
    "        Returns:\n",
    "            dict: Threshold metrics\n",
    "        \"\"\"\n",
    "        # Find threshold maximizing F1 score\n",
    "        thresholds = np.linspace(0, 1, 100)\n",
    "        f1_scores = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_prob >= threshold).astype(int)\n",
    "            f1_scores.append(f1_score(y_test, y_pred))\n",
    "        \n",
    "        optimal_f1_idx = np.argmax(f1_scores)\n",
    "        f1_threshold = thresholds[optimal_f1_idx]\n",
    "        \n",
    "        # Find threshold maximizing balanced accuracy\n",
    "        balanced_accuracy_scores = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_prob >= threshold).astype(int)\n",
    "            balanced_accuracy_scores.append(\n",
    "                balanced_accuracy_score(y_test, y_pred)\n",
    "            )\n",
    "        \n",
    "        optimal_ba_idx = np.argmax(balanced_accuracy_scores)\n",
    "        ba_threshold = thresholds[optimal_ba_idx]\n",
    "        \n",
    "        # Store thresholds\n",
    "        self.thresholds['authentication'] = {\n",
    "            'main': f1_threshold,\n",
    "            'candidate': ba_threshold  # Use more conservative threshold for candidates\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'f1_threshold': f1_threshold,\n",
    "            'f1_threshold_score': max(f1_scores),\n",
    "            'ba_threshold': ba_threshold,\n",
    "            'ba_threshold_score': max(balanced_accuracy_scores)\n",
    "        }\n",
    "    \n",
    "    def _apply_authentication(self) -> bool:\n",
    "        \"\"\"\n",
    "        Apply authentication rules with separate thresholds\n",
    "        \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"\\nApplying authentication rules...\")\n",
    "            \n",
    "            # Scale features\n",
    "            X = self.models['auth_scaler'].transform(self.df[self.features['authentication']])\n",
    "            \n",
    "            # Get probabilities\n",
    "            probabilities = self.models['authentication'].predict_proba(X)[:, 1]\n",
    "            \n",
    "            # Initialize authentication status and store probabilities\n",
    "            self.df['auth_status'] = 'unauthenticated'\n",
    "            self.df['auth_probability'] = probabilities\n",
    "            \n",
    "            # Authenticate ASVs based on their status\n",
    "            for status in ['main', 'candidate']:\n",
    "                mask = self.df['predicted_status'] == status\n",
    "                if mask.any():\n",
    "                    threshold = self.thresholds['authentication'][status]\n",
    "                    \n",
    "                    # Additional criteria for candidates\n",
    "                    if status == 'candidate':\n",
    "                        self.df.loc[\n",
    "                            mask & \n",
    "                            (probabilities >= threshold) &\n",
    "                            (self.df['dist_ratio'] >= 1.5),  # Distance check\n",
    "                            'auth_status'\n",
    "                        ] = 'authenticated'\n",
    "                    else:\n",
    "                        self.df.loc[\n",
    "                            mask & \n",
    "                            (probabilities >= threshold),\n",
    "                            'auth_status'\n",
    "                        ] = 'authenticated'\n",
    "            \n",
    "            # Calculate and store authentication metrics\n",
    "            self._calculate_authentication_metrics()\n",
    "            \n",
    "            # Print results\n",
    "            self._print_auth_results()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in authentication application: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _calculate_authentication_metrics(self) -> None:\n",
    "        \"\"\"Calculate comprehensive authentication metrics\"\"\"\n",
    "        # Overall metrics\n",
    "        self.metrics['authentication_results'] = {\n",
    "            'total_authenticated': (self.df['auth_status'] == 'authenticated').sum(),\n",
    "            'authentication_rate': (self.df['auth_status'] == 'authenticated').mean() * 100,\n",
    "            'agreement': (self.df['auth_status'] == self.df['step2']).mean() * 100\n",
    "        }\n",
    "        \n",
    "        # Metrics by ASV type\n",
    "        for status in ['main', 'candidate']:\n",
    "            mask = self.df['predicted_status'] == status\n",
    "            if mask.any():\n",
    "                self.metrics['authentication_results'][f'{status}_metrics'] = {\n",
    "                    'total': mask.sum(),\n",
    "                    'authenticated': (\n",
    "                        (self.df['auth_status'] == 'authenticated') & mask\n",
    "                    ).sum(),\n",
    "                    'authentication_rate': (\n",
    "                        (self.df['auth_status'] == 'authenticated') & mask\n",
    "                    ).mean() * 100,\n",
    "                    'agreement': (\n",
    "                        (self.df['auth_status'] == self.df['step2']) & mask\n",
    "                    ).mean() * 100\n",
    "                }\n",
    "        \n",
    "        # Distance-based metrics\n",
    "        self.metrics['authentication_results']['distance_metrics'] = {\n",
    "            'authenticated_mean_dist_ratio': self.df.loc[\n",
    "                self.df['auth_status'] == 'authenticated',\n",
    "                'dist_ratio'\n",
    "            ].mean(),\n",
    "            'authenticated_med_dist_ratio': self.df.loc[\n",
    "                self.df['auth_status'] == 'authenticated',\n",
    "                'dist_ratio'\n",
    "            ].median()\n",
    "        }\n",
    "\n",
    "    def _print_auth_results(self) -> None:\n",
    "        \"\"\"Print comprehensive authentication results\"\"\"\n",
    "        results = self.metrics['authentication_results']\n",
    "        \n",
    "        print(\"\\nAuthentication Results:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        print(f\"\\nOverall Results:\")\n",
    "        print(f\"Total ASVs authenticated: {results['total_authenticated']}\")\n",
    "        print(f\"Overall authentication rate: {results['authentication_rate']:.2f}%\")\n",
    "        print(f\"Agreement with original labels: {results['agreement']:.2f}%\")\n",
    "        \n",
    "        print(\"\\nResults by ASV Type:\")\n",
    "        for status in ['main', 'candidate']:\n",
    "            if f'{status}_metrics' in results:\n",
    "                metrics = results[f'{status}_metrics']\n",
    "                print(f\"\\n{status.title()} ASVs:\")\n",
    "                print(f\"Total: {metrics['total']}\")\n",
    "                print(f\"Authenticated: {metrics['authenticated']}\")\n",
    "                print(f\"Authentication rate: {metrics['authentication_rate']:.2f}%\")\n",
    "                print(f\"Agreement: {metrics['agreement']:.2f}%\")\n",
    "        \n",
    "        print(\"\\nDistance Metrics for Authenticated ASVs:\")\n",
    "        dist_metrics = results['distance_metrics']\n",
    "        print(f\"Mean distance ratio: {dist_metrics['authenticated_mean_dist_ratio']:.2f}\")\n",
    "        print(f\"Median distance ratio: {dist_metrics['authenticated_med_dist_ratio']:.2f}\")\n",
    "        \n",
    "        # Print confusion matrix\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        cm = pd.crosstab(\n",
    "            self.df['auth_status'],\n",
    "            self.df['step2'],\n",
    "            margins=True\n",
    "        )\n",
    "        print(cm)\n",
    "\n",
    "    def export_results(self, output_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Export comprehensive analysis results\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path for output files\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"\\n=== EXPORTING ANALYSIS RESULTS ===\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Create output directory if needed\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            # Export to different formats\n",
    "            self._export_excel_results(output_path)\n",
    "            \n",
    "            # Export JSON results\n",
    "            json_path = output_path.replace('.xlsx', '.json')\n",
    "            self._export_json_results(json_path)\n",
    "            \n",
    "            # Generate detailed report\n",
    "            report_path = output_path.replace('.xlsx', '_report.txt')\n",
    "            self._generate_detailed_report(report_path)\n",
    "            \n",
    "            # Generate PDF report\n",
    "            pdf_path = output_path.replace('.xlsx', '_report.pdf')\n",
    "            self._generate_pdf_report(pdf_path)\n",
    "            \n",
    "            print(\"\\nResults exported successfully:\")\n",
    "            print(f\"- Excel file: {output_path}\")\n",
    "            print(f\"- JSON file: {json_path}\")\n",
    "            print(f\"- Report file: {report_path}\")\n",
    "            print(f\"- PDF report: {pdf_path}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in results export: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _create_feature_importance_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create feature importance DataFrame\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Feature importance for all models\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Main/Candidate feature importance\n",
    "            mc_importance = pd.DataFrame({\n",
    "                'feature': self.features['main_candidate'],\n",
    "                'importance': self.models['main_candidate'].feature_importances_,\n",
    "                'analysis_type': 'Main/Candidate'\n",
    "            })\n",
    "            \n",
    "            # Authentication feature importance\n",
    "            auth_importance = pd.DataFrame({\n",
    "                'feature': self.features['authentication'],\n",
    "                'importance': self.models['authentication'].feature_importances_,\n",
    "                'analysis_type': 'Authentication'\n",
    "            })\n",
    "            \n",
    "            # Combine and sort\n",
    "            importance_df = pd.concat([mc_importance, auth_importance], ignore_index=True)\n",
    "            importance_df = importance_df.sort_values(['analysis_type', 'importance'], \n",
    "                                                    ascending=[True, False])\n",
    "            \n",
    "            return importance_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error creating feature importance DataFrame: {str(e)}\")\n",
    "            # Return empty DataFrame with correct columns if error occurs\n",
    "            return pd.DataFrame(columns=['feature', 'importance', 'analysis_type'])\n",
    "\n",
    "    def _export_excel_results(self, output_path: str) -> None:\n",
    "        \"\"\"Export results to Excel with multiple sheets\"\"\"\n",
    "        try:\n",
    "            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                # Main results\n",
    "                self._create_main_results().to_excel(\n",
    "                    writer, sheet_name='Main_Results', index=False\n",
    "                )\n",
    "                \n",
    "                # Performance metrics\n",
    "                self._create_metrics_df().to_excel(\n",
    "                    writer, sheet_name='Performance_Metrics', index=True\n",
    "                )\n",
    "                \n",
    "                # Feature importance\n",
    "                try:\n",
    "                    self._create_feature_importance_df().to_excel(\n",
    "                        writer, sheet_name='Feature_Importance', index=False\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not export feature importance: {str(e)}\")\n",
    "                \n",
    "                # Group statistics\n",
    "                try:\n",
    "                    self._create_group_stats_df().to_excel(\n",
    "                        writer, sheet_name='Group_Statistics', index=True\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not export group statistics: {str(e)}\")\n",
    "                \n",
    "                # Statistical summary\n",
    "                try:\n",
    "                    self._create_stats_summary_df().to_excel(\n",
    "                        writer, sheet_name='Statistical_Summary', index=True\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not export statistical summary: {str(e)}\")\n",
    "                \n",
    "                # Authentication analysis\n",
    "                try:\n",
    "                    self._create_authentication_analysis_df().to_excel(\n",
    "                        writer, sheet_name='Authentication_Analysis', index=True\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not export authentication analysis: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in Excel export: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _create_authentication_analysis_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create authentication analysis DataFrame\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Authentication analysis results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create authentication analysis summary\n",
    "            auth_df = pd.DataFrame({\n",
    "                'Metric': [\n",
    "                    'Total ASVs',\n",
    "                    'Authenticated ASVs',\n",
    "                    'Authentication Rate (%)',\n",
    "                    'Agreement with Original (%)',\n",
    "                    'Mean Distance Ratio',\n",
    "                    'Median Distance Ratio'\n",
    "                ],\n",
    "                'All': [\n",
    "                    len(self.df),\n",
    "                    (self.df['auth_status'] == 'authenticated').sum(),\n",
    "                    (self.df['auth_status'] == 'authenticated').mean() * 100,\n",
    "                    (self.df['auth_status'] == self.df['step2']).mean() * 100,\n",
    "                    self.df['dist_ratio'].mean(),\n",
    "                    self.df['dist_ratio'].median()\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            # Add metrics by predicted status\n",
    "            for status in ['main', 'candidate']:\n",
    "                mask = self.df['predicted_status'] == status\n",
    "                if mask.any():\n",
    "                    auth_df[status.title()] = [\n",
    "                        mask.sum(),\n",
    "                        (self.df.loc[mask, 'auth_status'] == 'authenticated').sum(),\n",
    "                        (self.df.loc[mask, 'auth_status'] == 'authenticated').mean() * 100,\n",
    "                        (self.df.loc[mask, 'auth_status'] == \n",
    "                        self.df.loc[mask, 'step2']).mean() * 100,\n",
    "                        self.df.loc[mask, 'dist_ratio'].mean(),\n",
    "                        self.df.loc[mask, 'dist_ratio'].median()\n",
    "                    ]\n",
    "            \n",
    "            return auth_df.round(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error creating authentication analysis DataFrame: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _create_main_results(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create main results DataFrame\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Main results\n",
    "        \"\"\"\n",
    "        return pd.DataFrame({\n",
    "            # Basic information\n",
    "            'asv_id': self.df['asv_id'],\n",
    "            'project_readfile_id': self.df['project_readfile_id'],\n",
    "            'family_tree': self.df['family_tree'],\n",
    "            'subfamily_tree': self.df['subfamily_tree'],\n",
    "            \n",
    "            # Original and predicted status\n",
    "            'original_step1': self.df['step1'],\n",
    "            'predicted_status': self.df['predicted_status'],\n",
    "            'mc_probability': self.df['mc_probability'],\n",
    "            'original_step2': self.df['step2'],\n",
    "            'auth_status': self.df['auth_status'],\n",
    "            'auth_probability': self.df['auth_probability'],\n",
    "            \n",
    "            # Feature values\n",
    "            'read_count': self.df['read_count'],\n",
    "            'total_read': self.df['total_read'],\n",
    "            'count_read': self.df['count_read'],\n",
    "            'percentage': self.df['percentage'],\n",
    "            'nearest_main_dist': self.df['nearest_main_dist'],\n",
    "            'nearest_cand_dist': self.df['nearest_cand_dist'],\n",
    "            'dist_ratio': self.df['dist_ratio'],\n",
    "            \n",
    "            # Additional metrics\n",
    "            'quality_score': self.df['quality_score'],\n",
    "            'reliability_score': self.df['reliability_score']\n",
    "        })\n",
    "\n",
    "    def _create_metrics_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create comprehensive metrics DataFrame\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Performance metrics\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'Main/Candidate Classification': {\n",
    "                key: self.metrics['main_candidate'].get(key, None)\n",
    "                for key in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "            },\n",
    "            'Authentication': {\n",
    "                key: self.metrics['authentication'].get(key, None)\n",
    "                for key in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add thresholds\n",
    "        metrics['Main/Candidate Classification']['threshold'] = self.thresholds['main_candidate']\n",
    "        metrics['Authentication'].update({\n",
    "            'main_threshold': self.thresholds['authentication']['main'],\n",
    "            'candidate_threshold': self.thresholds['authentication']['candidate']\n",
    "        })\n",
    "        \n",
    "        return pd.DataFrame.from_dict(metrics, orient='columns')\n",
    "\n",
    "    def _create_group_stats_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create group statistics DataFrame\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Group-wise statistics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate group statistics\n",
    "            group_stats = self.df.groupby('project_readfile_id').agg({\n",
    "                'asv_id': 'count',\n",
    "                'predicted_status': lambda x: (x == 'main').mean() * 100,\n",
    "                'auth_status': lambda x: (x == 'authenticated').mean() * 100,\n",
    "                'read_count': ['sum', 'mean', 'std'],\n",
    "                'total_read': 'first',\n",
    "                'count_read': 'mean'\n",
    "            }).round(2)\n",
    "            \n",
    "            # Rename columns for clarity\n",
    "            group_stats.columns = [\n",
    "                'ASV_Count',\n",
    "                'Main_ASV_Percentage',\n",
    "                'Authentication_Rate',\n",
    "                'Total_Reads',\n",
    "                'Mean_Reads',\n",
    "                'Std_Reads',\n",
    "                'Sequencing_Depth',\n",
    "                'Average_Coverage'\n",
    "            ]\n",
    "            \n",
    "            return group_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating group statistics: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _create_stats_summary_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create statistical summary DataFrame\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Statistical summary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            summary_stats = pd.DataFrame({\n",
    "                'Metric': [\n",
    "                    'Total ASVs',\n",
    "                    'Main ASVs (%)',\n",
    "                    'Candidate ASVs (%)',\n",
    "                    'Authenticated ASVs (%)',\n",
    "                    'Unauthenticated ASVs (%)',\n",
    "                    'Average Read Count',\n",
    "                    'Median Read Count',\n",
    "                    'Average Sample Coverage',\n",
    "                    'Average Distance Ratio',\n",
    "                    'Total Groups',\n",
    "                    'Average ASVs per Group'\n",
    "                ],\n",
    "                'Value': [\n",
    "                    len(self.df),\n",
    "                    (self.df['predicted_status'] == 'main').mean() * 100,\n",
    "                    (self.df['predicted_status'] == 'candidate').mean() * 100,\n",
    "                    (self.df['auth_status'] == 'authenticated').mean() * 100,\n",
    "                    (self.df['auth_status'] == 'unauthenticated').mean() * 100,\n",
    "                    self.df['read_count'].mean(),\n",
    "                    self.df['read_count'].median(),\n",
    "                    (self.df['count_read'] / self.df['sample_count']).mean() * 100,\n",
    "                    self.df['dist_ratio'].mean(),\n",
    "                    self.df['project_readfile_id'].nunique(),\n",
    "                    len(self.df) / self.df['project_readfile_id'].nunique()\n",
    "                ]\n",
    "            }).round(2)\n",
    "            \n",
    "            return summary_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating statistical summary: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _export_json_results(self, json_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Export results in JSON format\n",
    "        \n",
    "        Args:\n",
    "            json_path: Path for JSON file\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'total_asvs': len(self.df),\n",
    "                'features': self.features,\n",
    "                'thresholds': self.thresholds\n",
    "            },\n",
    "            'results': self.df[\n",
    "                ['asv_id', 'predicted_status', 'auth_status', \n",
    "                 'mc_probability', 'auth_probability']\n",
    "            ].to_dict('records'),\n",
    "            'metrics': {\n",
    "                'main_candidate': self.metrics['main_candidate'],\n",
    "                'authentication': self.metrics['authentication'],\n",
    "                'authentication_results': self.metrics['authentication_results']\n",
    "            },\n",
    "            'feature_importance': {\n",
    "                'main_candidate': dict(zip(\n",
    "                    self.features['main_candidate'],\n",
    "                    self.models['main_candidate'].feature_importances_\n",
    "                )),\n",
    "                'authentication': dict(zip(\n",
    "                    self.features['authentication'],\n",
    "                    self.models['authentication'].feature_importances_\n",
    "                ))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Convert numpy/pandas types to native Python types\n",
    "        results = self._convert_to_serializable(results)\n",
    "        \n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "    def _convert_to_serializable(self, obj: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Convert object to JSON serializable format\n",
    "        \n",
    "        Args:\n",
    "            obj: Input object\n",
    "            \n",
    "        Returns:\n",
    "            JSON serializable object\n",
    "        \"\"\"\n",
    "        if isinstance(obj, (np.integer, np.floating, np.int64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, pd.Series):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self._convert_to_serializable(value) \n",
    "                for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_to_serializable(item) for item in obj]\n",
    "        return obj\n",
    "\n",
    "    def _generate_detailed_report(self, report_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Generate comprehensive analysis report\n",
    "        \n",
    "        Args:\n",
    "            report_path: Path for report file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(report_path, 'w') as f:\n",
    "                # Header\n",
    "                f.write(\"ASV ANALYSIS DETAILED REPORT\\n\")\n",
    "                f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "                \n",
    "                # 1. Executive Summary\n",
    "                f.write(\"1. EXECUTIVE SUMMARY\\n\")\n",
    "                f.write(\"-\" * 20 + \"\\n\")\n",
    "                total_asvs = len(self.df)\n",
    "                main_count = (self.df['predicted_status'] == 'main').sum()\n",
    "                auth_count = (self.df['auth_status'] == 'authenticated').sum()\n",
    "                \n",
    "                f.write(f\"Total ASVs analyzed: {total_asvs:,}\\n\")\n",
    "                f.write(f\"Main vs Candidate Ratio: {main_count}:{total_asvs-main_count} \"\n",
    "                    f\"({main_count/total_asvs*100:.1f}%)\\n\")\n",
    "                f.write(f\"Authentication Rate: {auth_count/total_asvs*100:.1f}%\\n\\n\")\n",
    "                \n",
    "                # 2. Classification Analysis\n",
    "                f.write(\"\\n2. CLASSIFICATION PERFORMANCE\\n\")\n",
    "                f.write(\"-\" * 20 + \"\\n\")\n",
    "                for metric, value in self.metrics['main_candidate'].items():\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "                \n",
    "                # 3. Authentication Analysis\n",
    "                f.write(\"\\n3. AUTHENTICATION PERFORMANCE\\n\")\n",
    "                f.write(\"-\" * 20 + \"\\n\")\n",
    "                for metric, value in self.metrics['authentication'].items():\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "                \n",
    "                # 4. Feature Analysis\n",
    "                f.write(\"\\n4. FEATURE IMPORTANCE\\n\")\n",
    "                f.write(\"-\" * 20 + \"\\n\")\n",
    "                \n",
    "                # Main/Candidate features\n",
    "                f.write(\"\\nMain/Candidate Classification Features:\\n\")\n",
    "                importance = pd.DataFrame({\n",
    "                    'feature': self.features['main_candidate'],\n",
    "                    'importance': self.models['main_candidate'].feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                for _, row in importance.iterrows():\n",
    "                    f.write(f\"{row['feature']}: {row['importance']:.4f}\\n\")\n",
    "                \n",
    "                # Authentication features\n",
    "                f.write(\"\\nAuthentication Features:\\n\")\n",
    "                importance = pd.DataFrame({\n",
    "                    'feature': self.features['authentication'],\n",
    "                    'importance': self.models['authentication'].feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                for _, row in importance.iterrows():\n",
    "                    f.write(f\"{row['feature']}: {row['importance']:.4f}\\n\")\n",
    "                \n",
    "                # 5. Group Analysis\n",
    "                f.write(\"\\n5. GROUP ANALYSIS\\n\")\n",
    "                f.write(\"-\" * 20 + \"\\n\")\n",
    "                \n",
    "                group_stats = self.df.groupby('project_readfile_id').agg({\n",
    "                    'asv_id': 'count',\n",
    "                    'predicted_status': lambda x: (x == 'main').mean() * 100,\n",
    "                    'auth_status': lambda x: (x == 'authenticated').mean() * 100\n",
    "                })\n",
    "                \n",
    "                f.write(f\"\\nTotal Groups: {len(group_stats)}\\n\")\n",
    "                f.write(f\"Average ASVs per Group: {group_stats['asv_id'].mean():.1f}\\n\")\n",
    "                f.write(f\"Average Main ASV Percentage: {group_stats['predicted_status'].mean():.1f}%\\n\")\n",
    "                f.write(f\"Average Authentication Rate: {group_stats['auth_status'].mean():.1f}%\\n\")\n",
    "                \n",
    "                # 6. Threshold Information\n",
    "                f.write(\"\\n6. THRESHOLD INFORMATION\\n\")\n",
    "                f.write(\"-\" * 20 + \"\\n\")\n",
    "                f.write(f\"\\nMain/Candidate Classification Threshold: {self.thresholds['main_candidate']:.4f}\\n\")\n",
    "                f.write(\"\\nAuthentication Thresholds:\\n\")\n",
    "                f.write(f\"Main ASVs: {self.thresholds['authentication']['main']:.4f}\\n\")\n",
    "                f.write(f\"Candidate ASVs: {self.thresholds['authentication']['candidate']:.4f}\\n\")\n",
    "                \n",
    "            print(f\"Detailed report generated: {report_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating detailed report: {str(e)}\")\n",
    "\n",
    "    def _generate_pdf_report(self, pdf_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Generate PDF report with ReportLab\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path for PDF report\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create document\n",
    "            doc = SimpleDocTemplate(\n",
    "                pdf_path,\n",
    "                pagesize=letter,\n",
    "                rightMargin=72,\n",
    "                leftMargin=72,\n",
    "                topMargin=72,\n",
    "                bottomMargin=72\n",
    "            )\n",
    "            \n",
    "            # Initialize styles\n",
    "            styles = getSampleStyleSheet()\n",
    "            title_style = ParagraphStyle(\n",
    "                'CustomTitle',\n",
    "                parent=styles['Heading1'],\n",
    "                fontSize=24,\n",
    "                spaceAfter=30\n",
    "            )\n",
    "            heading_style = ParagraphStyle(\n",
    "                'CustomHeading',\n",
    "                parent=styles['Heading2'],\n",
    "                fontSize=18,\n",
    "                spaceAfter=20\n",
    "            )\n",
    "            subheading_style = ParagraphStyle(\n",
    "                'CustomSubHeading',\n",
    "                parent=styles['Heading3'],\n",
    "                fontSize=14,\n",
    "                spaceAfter=10\n",
    "            )\n",
    "            body_style = ParagraphStyle(\n",
    "                'CustomBody',\n",
    "                parent=styles['Normal'],\n",
    "                fontSize=12,\n",
    "                spaceAfter=12\n",
    "            )\n",
    "            \n",
    "            # Build content\n",
    "            content = []\n",
    "            \n",
    "            # Title\n",
    "            content.append(Paragraph(\"ASV Analysis Report\", title_style))\n",
    "            content.append(Spacer(1, 20))\n",
    "            \n",
    "            # Executive Summary\n",
    "            self._add_executive_summary(content, heading_style, body_style)\n",
    "            \n",
    "            # Classification Analysis\n",
    "            self._add_classification_analysis(content, heading_style, subheading_style, body_style)\n",
    "            \n",
    "            # Authentication Analysis\n",
    "            self._add_authentication_analysis(content, heading_style, subheading_style, body_style)\n",
    "            \n",
    "            # Feature Analysis\n",
    "            self._add_feature_analysis(content, heading_style, subheading_style, body_style)\n",
    "            \n",
    "            # Group Analysis\n",
    "            self._add_group_analysis(content, heading_style, subheading_style, body_style)\n",
    "            \n",
    "            # Save document\n",
    "            doc.build(content)\n",
    "            print(f\"PDF report generated: {pdf_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating PDF report: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "    def _add_executive_summary(self, content: List, heading_style: ParagraphStyle, \n",
    "                             body_style: ParagraphStyle) -> None:\n",
    "        \"\"\"Add executive summary section to report\"\"\"\n",
    "        content.append(Paragraph(\"1. Executive Summary\", heading_style))\n",
    "        \n",
    "        # Calculate key metrics\n",
    "        total_asvs = len(self.df)\n",
    "        main_count = (self.df['predicted_status'] == 'main').sum()\n",
    "        auth_count = (self.df['auth_status'] == 'authenticated').sum()\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "        This analysis processed {total_asvs:,} ASVs, identifying {main_count:,} ({main_count/total_asvs*100:.1f}%) \n",
    "        as main ASVs and {auth_count:,} ({auth_count/total_asvs*100:.1f}%) as authenticated sequences.\n",
    "        \n",
    "        Key Findings:\n",
    "        • Main/Candidate Classification achieved {self.metrics['main_candidate']['f1']:.1%} F1 score\n",
    "        • Authentication Analysis achieved {self.metrics['authentication']['f1']:.1%} F1 score\n",
    "        • Overall agreement with original labels: {self.metrics['authentication_results']['agreement']:.1f}%\n",
    "        \"\"\"\n",
    "        \n",
    "        content.append(Paragraph(summary_text, body_style))\n",
    "        content.append(Spacer(1, 20))\n",
    "\n",
    "    def _add_classification_analysis(self, content: List, heading_style: ParagraphStyle, \n",
    "                                  subheading_style: ParagraphStyle, \n",
    "                                  body_style: ParagraphStyle) -> None:\n",
    "        \"\"\"Add classification analysis section to report\"\"\"\n",
    "        content.append(Paragraph(\"2. Main/Candidate Classification Analysis\", heading_style))\n",
    "        \n",
    "        # Performance Metrics\n",
    "        content.append(Paragraph(\"2.1 Performance Metrics\", subheading_style))\n",
    "        metrics_text = f\"\"\"\n",
    "        Classification Performance:\n",
    "        • Accuracy: {self.metrics['main_candidate']['accuracy']:.3f}\n",
    "        • Precision: {self.metrics['main_candidate']['precision']:.3f}\n",
    "        • Recall: {self.metrics['main_candidate']['recall']:.3f}\n",
    "        • F1 Score: {self.metrics['main_candidate']['f1']:.3f}\n",
    "        • ROC AUC: {self.metrics['main_candidate']['roc_auc']:.3f}\n",
    "        \n",
    "        Optimal Classification Threshold: {self.thresholds['main_candidate']:.3f}\n",
    "        \"\"\"\n",
    "        content.append(Paragraph(metrics_text, body_style))\n",
    "        \n",
    "        # Feature Importance\n",
    "        content.append(Paragraph(\"2.2 Feature Importance\", subheading_style))\n",
    "        if hasattr(self.models['main_candidate'], 'feature_importances_'):\n",
    "            importance = pd.DataFrame({\n",
    "                'Feature': self.features['main_candidate'],\n",
    "                'Importance': self.models['main_candidate'].feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            importance_text = \"Key Features:\\n\" + \"\\n\".join([\n",
    "                f\"• {row['Feature']}: {row['Importance']:.3f}\"\n",
    "                for _, row in importance.head().iterrows()\n",
    "            ])\n",
    "            content.append(Paragraph(importance_text, body_style))\n",
    "        \n",
    "        content.append(Spacer(1, 20))\n",
    "\n",
    "    def _add_authentication_analysis(self, content: List, heading_style: ParagraphStyle, \n",
    "                                  subheading_style: ParagraphStyle, \n",
    "                                  body_style: ParagraphStyle) -> None:\n",
    "        \"\"\"Add authentication analysis section to report\"\"\"\n",
    "        content.append(Paragraph(\"3. Authentication Analysis\", heading_style))\n",
    "        \n",
    "        # Performance Metrics\n",
    "        content.append(Paragraph(\"3.1 Authentication Performance\", subheading_style))\n",
    "        auth_text = f\"\"\"\n",
    "        Overall Performance:\n",
    "        • Accuracy: {self.metrics['authentication']['accuracy']:.3f}\n",
    "        • Precision: {self.metrics['authentication']['precision']:.3f}\n",
    "        • Recall: {self.metrics['authentication']['recall']:.3f}\n",
    "        • F1 Score: {self.metrics['authentication']['f1']:.3f}\n",
    "        • ROC AUC: {self.metrics['authentication']['roc_auc']:.3f}\n",
    "        \n",
    "        Authentication Thresholds:\n",
    "        • Main ASVs: {self.thresholds['authentication']['main']:.3f}\n",
    "        • Candidate ASVs: {self.thresholds['authentication']['candidate']:.3f}\n",
    "        \"\"\"\n",
    "        content.append(Paragraph(auth_text, body_style))\n",
    "        \n",
    "        # Results by ASV Type\n",
    "        content.append(Paragraph(\"3.2 Results by ASV Type\", subheading_style))\n",
    "        for status in ['main', 'candidate']:\n",
    "            metrics = self.metrics['authentication_results'][f'{status}_metrics']\n",
    "            type_text = f\"\"\"\n",
    "            {status.title()} ASVs:\n",
    "            • Total: {metrics['total']:,}\n",
    "            • Authenticated: {metrics['authenticated']:,}\n",
    "            • Authentication Rate: {metrics['authentication_rate']:.1f}%\n",
    "            • Agreement with Original Labels: {metrics['agreement']:.1f}%\n",
    "            \"\"\"\n",
    "            content.append(Paragraph(type_text, body_style))\n",
    "        \n",
    "        content.append(Spacer(1, 20))\n",
    "\n",
    "    def _add_feature_analysis(self, content: List, heading_style: ParagraphStyle, \n",
    "                            subheading_style: ParagraphStyle, \n",
    "                            body_style: ParagraphStyle) -> None:\n",
    "        \"\"\"Add feature analysis section to report\"\"\"\n",
    "        content.append(Paragraph(\"4. Feature Analysis\", heading_style))\n",
    "        \n",
    "        # Feature Statistics\n",
    "        content.append(Paragraph(\"4.1 Feature Statistics\", subheading_style))\n",
    "        for feature_set in ['main_candidate', 'authentication']:\n",
    "            stats_text = f\"\\n{feature_set.replace('_', ' ').title()} Features:\\n\"\n",
    "            for feature in self.features[feature_set]:\n",
    "                stats = self.df[feature].describe()\n",
    "                stats_text += f\"\"\"\n",
    "                {feature}:\n",
    "                • Mean: {stats['mean']:.3f}\n",
    "                • Std: {stats['std']:.3f}\n",
    "                • Min: {stats['min']:.3f}\n",
    "                • Max: {stats['max']:.3f}\n",
    "                \"\"\"\n",
    "            content.append(Paragraph(stats_text, body_style))\n",
    "        \n",
    "        # Feature Correlations\n",
    "        content.append(Paragraph(\"4.2 Feature Correlations\", subheading_style))\n",
    "        corr_matrix = self.df[self.features['main_candidate'] + \n",
    "                            self.features['authentication']].corr()\n",
    "        \n",
    "        # Find strong correlations\n",
    "        strong_corr = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if abs(corr_matrix.iloc[i,j]) > 0.7:\n",
    "                    strong_corr.append(\n",
    "                        f\"• {corr_matrix.columns[i]} - {corr_matrix.columns[j]}: \"\n",
    "                        f\"{corr_matrix.iloc[i,j]:.3f}\"\n",
    "                    )\n",
    "        \n",
    "        if strong_corr:\n",
    "            corr_text = \"Strong Feature Correlations:\\n\" + \"\\n\".join(strong_corr)\n",
    "            content.append(Paragraph(corr_text, body_style))\n",
    "        \n",
    "        content.append(Spacer(1, 20))\n",
    "\n",
    "    def _add_group_analysis(self, content: List, heading_style: ParagraphStyle, \n",
    "                          subheading_style: ParagraphStyle, \n",
    "                          body_style: ParagraphStyle) -> None:\n",
    "        \"\"\"Add group analysis section to report\"\"\"\n",
    "        content.append(Paragraph(\"5. Group Analysis\", heading_style))\n",
    "        \n",
    "        # Group Statistics\n",
    "        group_stats = self.df.groupby('project_readfile_id').agg({\n",
    "            'asv_id': 'count',\n",
    "            'predicted_status': lambda x: (x == 'main').mean() * 100,\n",
    "            'auth_status': lambda x: (x == 'authenticated').mean() * 100\n",
    "        })\n",
    "        \n",
    "        stats_text = f\"\"\"\n",
    "        Group Analysis Summary:\n",
    "        • Total Groups: {len(group_stats)}\n",
    "        • Average ASVs per Group: {group_stats['asv_id'].mean():.1f}\n",
    "        • Average Main ASV Percentage: {group_stats['predicted_status'].mean():.1f}%\n",
    "        • Average Authentication Rate: {group_stats['auth_status'].mean():.1f}%\n",
    "        \n",
    "        Group Size Distribution:\n",
    "        • Minimum: {group_stats['asv_id'].min():.0f}\n",
    "        • Maximum: {group_stats['asv_id'].max():.0f}\n",
    "        • Median: {group_stats['asv_id'].median():.0f}\n",
    "        \"\"\"\n",
    "        \n",
    "        content.append(Paragraph(stats_text, body_style))\n",
    "        content.append(Spacer(1, 20))\n",
    "\n",
    "    def generate_summary_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate a text summary of the analysis\n",
    "        \n",
    "        Returns:\n",
    "            str: Summary report\n",
    "        \"\"\"\n",
    "        summary = []\n",
    "        \n",
    "        # Header\n",
    "        summary.append(\"ASV ANALYSIS SUMMARY REPORT\")\n",
    "        summary.append(\"=\" * 50)\n",
    "        \n",
    "        # Basic Statistics\n",
    "        summary.append(\"\\n1. BASIC STATISTICS\")\n",
    "        summary.append(\"-\" * 20)\n",
    "        summary.append(f\"Total ASVs analyzed: {len(self.df):,}\")\n",
    "        summary.append(f\"Main ASVs: {(self.df['predicted_status'] == 'main').sum():,}\")\n",
    "        summary.append(f\"Authenticated ASVs: {(self.df['auth_status'] == 'authenticated').sum():,}\")\n",
    "        \n",
    "        # Classification Performance\n",
    "        summary.append(\"\\n2. CLASSIFICATION PERFORMANCE\")\n",
    "        summary.append(\"-\" * 20)\n",
    "        mc_metrics = self.metrics['main_candidate']\n",
    "        summary.append(f\"Accuracy: {mc_metrics['accuracy']:.3f}\")\n",
    "        summary.append(f\"F1 Score: {mc_metrics['f1']:.3f}\")\n",
    "        summary.append(f\"ROC AUC: {mc_metrics['roc_auc']:.3f}\")\n",
    "        \n",
    "        # Authentication Performance\n",
    "        summary.append(\"\\n3. AUTHENTICATION PERFORMANCE\")\n",
    "        summary.append(\"-\" * 20)\n",
    "        auth_metrics = self.metrics['authentication']\n",
    "        summary.append(f\"Accuracy: {auth_metrics['accuracy']:.3f}\")\n",
    "        summary.append(f\"F1 Score: {auth_metrics['f1']:.3f}\")\n",
    "        summary.append(f\"ROC AUC: {auth_metrics['roc_auc']:.3f}\")\n",
    "        \n",
    "        # Group Analysis\n",
    "        summary.append(\"\\n4. GROUP ANALYSIS\")\n",
    "        summary.append(\"-\" * 20)\n",
    "        group_stats = self.df.groupby('project_readfile_id').agg({\n",
    "            'asv_id': 'count',\n",
    "            'predicted_status': lambda x: (x == 'main').mean() * 100,\n",
    "            'auth_status': lambda x: (x == 'authenticated').mean() * 100\n",
    "        })\n",
    "        summary.append(f\"Total Groups: {len(group_stats)}\")\n",
    "        summary.append(f\"Average ASVs per Group: {group_stats['asv_id'].mean():.1f}\")\n",
    "        summary.append(f\"Average Authentication Rate: {group_stats['auth_status'].mean():.1f}%\")\n",
    "        \n",
    "        # Recommendations\n",
    "        summary.append(\"\\n5. RECOMMENDATIONS\")\n",
    "        summary.append(\"-\" * 20)\n",
    "        \n",
    "        # Add recommendations based on analysis results\n",
    "        if mc_metrics['f1'] < 0.8:\n",
    "            summary.append(\"• Consider reviewing main/candidate classification criteria\")\n",
    "        if auth_metrics['f1'] < 0.8:\n",
    "            summary.append(\"• Authentication thresholds may need adjustment\")\n",
    "        \n",
    "        # Add any groups with unusual patterns\n",
    "        unusual_groups = group_stats[\n",
    "            (group_stats['auth_status'] < group_stats['auth_status'].mean() - \n",
    "             group_stats['auth_status'].std())\n",
    "        ].index\n",
    "        if len(unusual_groups) > 0:\n",
    "            summary.append(\"\\nGroups requiring review:\")\n",
    "            for group in unusual_groups:\n",
    "                summary.append(f\"• {group}\")\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "\n",
    "    def save_analysis_state(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Save analysis state to file for later use\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to save state file\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"\\nSaving analysis state to {filepath}...\")\n",
    "            \n",
    "            # Create state dictionary\n",
    "            state = {\n",
    "                'version': '1.0',\n",
    "                'timestamp': pd.Timestamp.now().isoformat(),\n",
    "                'data': {\n",
    "                    'df': self.df.to_dict(),\n",
    "                    'features': self.features,\n",
    "                    'thresholds': self.thresholds,\n",
    "                    'metrics': self.metrics,\n",
    "                    'parameters': self.params\n",
    "                },\n",
    "                'models': {\n",
    "                    'main_candidate': self._serialize_model(self.models['main_candidate']),\n",
    "                    'authentication': self._serialize_model(self.models['authentication']),\n",
    "                    'scaler': self._serialize_model(self.models['scaler'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save to file\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(state, f)\n",
    "            \n",
    "            print(\"Analysis state saved successfully\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving analysis state: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def load_analysis_state(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Load analysis state from file\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to state file\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"\\nLoading analysis state from {filepath}...\")\n",
    "            \n",
    "            # Load state dictionary\n",
    "            with open(filepath, 'rb') as f:\n",
    "                state = pickle.load(f)\n",
    "            \n",
    "            # Validate version\n",
    "            if state.get('version', '0.0') != '1.0':\n",
    "                print(\"Warning: State file version mismatch\")\n",
    "            \n",
    "            # Restore data\n",
    "            self.df = pd.DataFrame.from_dict(state['data']['df'])\n",
    "            self.features = state['data']['features']\n",
    "            self.thresholds = state['data']['thresholds']\n",
    "            self.metrics = state['data']['metrics']\n",
    "            self.params = state['data']['parameters']\n",
    "            \n",
    "            # Restore models\n",
    "            self.models = {\n",
    "                'main_candidate': self._deserialize_model(state['models']['main_candidate']),\n",
    "                'authentication': self._deserialize_model(state['models']['authentication']),\n",
    "                'scaler': self._deserialize_model(state['models']['scaler'])\n",
    "            }\n",
    "            \n",
    "            print(\"Analysis state loaded successfully\")\n",
    "            print(f\"State timestamp: {state['timestamp']}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading analysis state: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _serialize_model(self, model: Any) -> bytes:\n",
    "        \"\"\"\n",
    "        Serialize model object to bytes\n",
    "        \n",
    "        Args:\n",
    "            model: Model object to serialize\n",
    "            \n",
    "        Returns:\n",
    "            bytes: Serialized model\n",
    "        \"\"\"\n",
    "        return pickle.dumps(model)\n",
    "\n",
    "    def _deserialize_model(self, model_bytes: bytes) -> Any:\n",
    "        \"\"\"\n",
    "        Deserialize model from bytes\n",
    "        \n",
    "        Args:\n",
    "            model_bytes: Serialized model\n",
    "            \n",
    "        Returns:\n",
    "            Any: Deserialized model\n",
    "        \"\"\"\n",
    "        return pickle.loads(model_bytes)\n",
    "\n",
    "    def validate_results(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Perform comprehensive validation of analysis results\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of validation warnings\n",
    "        \"\"\"\n",
    "        warnings = []\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_values = self.df.isnull().sum()\n",
    "        if missing_values.any():\n",
    "            warnings.append(f\"Missing values found in columns: \"\n",
    "                          f\"{missing_values[missing_values > 0].index.tolist()}\")\n",
    "        \n",
    "        # Check model performance\n",
    "        if self.metrics['main_candidate']['f1'] < 0.7:\n",
    "            warnings.append(\"Main/Candidate classification F1 score below 0.7\")\n",
    "        if self.metrics['authentication']['f1'] < 0.7:\n",
    "            warnings.append(\"Authentication F1 score below 0.7\")\n",
    "        \n",
    "        # Check group distributions\n",
    "        group_sizes = self.df.groupby('project_readfile_id').size()\n",
    "        if group_sizes.std() / group_sizes.mean() > 0.5:\n",
    "            warnings.append(\"High variation in group sizes detected\")\n",
    "        \n",
    "        # Check authentication rates\n",
    "        auth_rates = self.df.groupby('project_readfile_id')['auth_status'].apply(\n",
    "            lambda x: (x == 'authenticated').mean()\n",
    "        )\n",
    "        if auth_rates.std() > 0.2:\n",
    "            warnings.append(\"High variation in authentication rates across groups\")\n",
    "        \n",
    "        # Check feature distributions\n",
    "        for feature in self.features['main_candidate'] + self.features['authentication']:\n",
    "            z_scores = np.abs(stats.zscore(self.df[feature]))\n",
    "            outliers = (z_scores > 3).sum()\n",
    "            if outliers > len(self.df) * 0.01:\n",
    "                warnings.append(f\"High number of outliers in feature {feature}\")\n",
    "        \n",
    "        return warnings\n",
    "\n",
    "    def suggest_improvements(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Suggest potential improvements based on analysis results\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, List[str]]: Dictionary of improvement suggestions\n",
    "        \"\"\"\n",
    "        suggestions = {\n",
    "            'classification': [],\n",
    "            'authentication': [],\n",
    "            'features': [],\n",
    "            'general': []\n",
    "        }\n",
    "        \n",
    "        # Classification suggestions\n",
    "        mc_metrics = self.metrics['main_candidate']\n",
    "        if mc_metrics['precision'] < mc_metrics['recall']:\n",
    "            suggestions['classification'].append(\n",
    "                \"Consider increasing classification threshold for higher precision\"\n",
    "            )\n",
    "        if mc_metrics['f1'] < 0.8:\n",
    "            suggestions['classification'].append(\n",
    "                \"Review feature importance and consider adding new features\"\n",
    "            )\n",
    "        \n",
    "        # Authentication suggestions\n",
    "        auth_metrics = self.metrics['authentication']\n",
    "        if auth_metrics['precision'] < 0.8:\n",
    "            suggestions['authentication'].append(\n",
    "                \"Consider stricter authentication criteria for candidates\"\n",
    "            )\n",
    "        \n",
    "        # Feature suggestions\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': self.features['main_candidate'],\n",
    "            'importance': self.models['main_candidate'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        if feature_imp['importance'].iloc[-1] < 0.01:\n",
    "            suggestions['features'].append(\n",
    "                f\"Consider removing low-importance feature: {feature_imp.iloc[-1]['feature']}\"\n",
    "            )\n",
    "        \n",
    "        # Check feature correlations\n",
    "        corr_matrix = self.df[self.features['main_candidate']].corr()\n",
    "        high_corr = np.where(np.abs(corr_matrix) > 0.9)\n",
    "        high_corr_pairs = set()\n",
    "        for i, j in zip(*high_corr):\n",
    "            if i != j:\n",
    "                pair = tuple(sorted([corr_matrix.index[i], corr_matrix.index[j]]))\n",
    "                high_corr_pairs.add(pair)\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            suggestions['features'].append(\n",
    "                \"Consider removing one feature from highly correlated pairs: \" +\n",
    "                \", \".join([f\"({f1}, {f2})\" for f1, f2 in high_corr_pairs])\n",
    "            )\n",
    "        \n",
    "        # General suggestions\n",
    "        group_sizes = self.df.groupby('project_readfile_id').size()\n",
    "        if group_sizes.std() / group_sizes.mean() > 0.5:\n",
    "            suggestions['general'].append(\n",
    "                \"Consider normalizing features within groups due to high size variation\"\n",
    "            )\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "    def export_for_validation(self, output_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Export results in format suitable for expert validation\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path for validation file\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create validation DataFrame\n",
    "            validation_df = pd.DataFrame({\n",
    "                'ASV_ID': self.df['asv_id'],\n",
    "                'Group_ID': self.df['project_readfile_id'],\n",
    "                'Original_Status': self.df['step1'],\n",
    "                'Predicted_Status': self.df['predicted_status'],\n",
    "                'Status_Confidence': self.df['mc_probability'],\n",
    "                'Original_Authentication': self.df['step2'],\n",
    "                'Predicted_Authentication': self.df['auth_status'],\n",
    "                'Auth_Confidence': self.df['auth_probability'],\n",
    "                'Read_Count': self.df['read_count'],\n",
    "                'Sample_Coverage': self.df['count_read'],\n",
    "                'Distance_Ratio': self.df['dist_ratio']\n",
    "            })\n",
    "            \n",
    "            # Add validation columns\n",
    "            validation_df['Status_Correct'] = ''\n",
    "            validation_df['Auth_Correct'] = ''\n",
    "            validation_df['Notes'] = ''\n",
    "            \n",
    "            # Export to Excel\n",
    "            validation_df.to_excel(output_path, index=False)\n",
    "            \n",
    "            print(f\"Validation file exported to {output_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting validation file: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "def run_complete_pipeline(metadata_path: str, fasta_path: str, output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Run complete ASV analysis pipeline with comprehensive outputs\n",
    "    \n",
    "    Args:\n",
    "        metadata_path: Path to metadata Excel file\n",
    "        fasta_path: Path to FASTA sequence file\n",
    "        output_dir: Directory for outputs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize analyzer\n",
    "        print(\"\\nInitializing ASV analyzer...\")\n",
    "        analyzer = IntegratedASVAnalyzer(output_dir)\n",
    "        \n",
    "        # Define output paths\n",
    "        results_path = os.path.join(output_dir, 'asv_analysis_results.xlsx')\n",
    "        report_path = os.path.join(output_dir, 'analysis_report.pdf')\n",
    "        state_path = os.path.join(output_dir, 'analysis_state.pkl')\n",
    "        validation_path = os.path.join(output_dir, 'validation_file.xlsx')\n",
    "        \n",
    "        # Run main analysis\n",
    "        success = analyzer.run_complete_analysis(\n",
    "            metadata_path=metadata_path,\n",
    "            fasta_path=fasta_path,\n",
    "            output_path=results_path\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            # Save analysis state\n",
    "            analyzer.save_analysis_state(state_path)\n",
    "            \n",
    "            # Export validation file\n",
    "            analyzer.export_for_validation(validation_path)\n",
    "            \n",
    "            # Validate results and get warnings\n",
    "            warnings = analyzer.validate_results()\n",
    "            if warnings:\n",
    "                print(\"\\nValidation Warnings:\")\n",
    "                for warning in warnings:\n",
    "                    print(f\"- {warning}\")\n",
    "                \n",
    "                # Save warnings to file\n",
    "                with open(os.path.join(output_dir, 'validation_warnings.txt'), 'w') as f:\n",
    "                    f.write(\"\\n\".join(warnings))\n",
    "            \n",
    "            # Get improvement suggestions\n",
    "            suggestions = analyzer.suggest_improvements()\n",
    "            print(\"\\nImprovement Suggestions:\")\n",
    "            for category, items in suggestions.items():\n",
    "                if items:\n",
    "                    print(f\"\\n{category.title()}:\")\n",
    "                    for item in items:\n",
    "                        print(f\"- {item}\")\n",
    "            \n",
    "            # Save suggestions to file\n",
    "            with open(os.path.join(output_dir, 'improvement_suggestions.txt'), 'w') as f:\n",
    "                for category, items in suggestions.items():\n",
    "                    if items:\n",
    "                        f.write(f\"\\n{category.title()}:\\n\")\n",
    "                        for item in items:\n",
    "                            f.write(f\"- {item}\\n\")\n",
    "            \n",
    "            print(\"\\nAnalysis pipeline completed successfully!\")\n",
    "            print(f\"Results directory: {output_dir}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nAnalysis pipeline failed!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in analysis pipeline: {str(e)}\")\n",
    "        print(\"\\nFull error details:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "# Main execution if run as script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nMetabarcoding ASV Analysis Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Example file paths\n",
    "    metadata_path = \"/Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/Metabarcoding_Machine_Learning.csv\"\n",
    "    fasta_path = \"/Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/MBCTH.fasta\"\n",
    "    output_dir = \"/Users/sarawut/Library/CloudStorage/OneDrive-ImperialCollegeLondon/2024_R/R_analysis/Chapter2_Data_generation/Metabarcoding_Machine_Learning/result\"\n",
    "    \n",
    "    # Run pipeline\n",
    "    run_complete_pipeline(metadata_path, fasta_path, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
